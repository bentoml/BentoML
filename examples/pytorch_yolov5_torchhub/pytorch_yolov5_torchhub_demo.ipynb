{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BentoML PyTorch YoloV5 Tutorial\n",
    "\n",
    "Link to source code: https://github.com/bentoml/BentoML/tree/main/examples/pytorch_yolov5_torchhub/\n",
    "\n",
    "Here is a quick example of how to use BentoML to serve a TorchHub model. In this example, we will be using the YoloV5 model from TorchHub. We will be using the `torch.hub.load` API to load the model from TorchHub. \n",
    "We will then use BentoML to create a REST API server for the model. We will also create a simple web app to interact with the model.\n",
    "\n",
    "Take `ultralytics/yolov5` as the example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Copy the source repository to your project directory\n",
    "\n",
    "We need two things from the source repository:\n",
    "    * the dependencies, typically in a `requirements.txt` file\n",
    "    * the source code of the PyTorch Module\n",
    "\n",
    "Due to the limitation of PyTorch serialization, the source code of the model is not saved in the saved bundle. \n",
    "Therefore, we always need to the source code when loading the model from the saved bundle.\n",
    "\n",
    "Let's clone the source code from the original repo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!git clone 'https://github.com/ultralytics/yolov5.git'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we copy the requirements.txt file to the project directory, and append the `pydantic` package to the file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!cp yolov5/requirements.txt ./requirements.txt\n",
    "# !echo \"pydantic\" >> requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Load the pre-trained model and do some fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "original_model = torch.hub.load(\"./yolov5\", \"yolov5s\", pretrained=True, source=\"local\")\n",
    "# Do your custom fine-tune...\n",
    "# For more details, see https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Normalize the input and output of the model\n",
    "\n",
    "The output of the yolov5 model is a tensor that mixed the detected objects, position box and probability like:\n",
    "\n",
    "category_id, x, y, width, height, probability\n",
    "\n",
    "We need to extract the outputs to a list of dict, each dict contains the information of a detected object before saving the model with bentoml."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class WrapperModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        outputs = self.model(imgs)\n",
    "\n",
    "        # convert outputs to a json serializable list\n",
    "        results = []\n",
    "        for det in outputs.pred:\n",
    "            detections = []\n",
    "            for i in det:\n",
    "                d = {}\n",
    "                d[\"obj\"] = outputs.names[int(i[5])]\n",
    "                d[\"position\"] = i[:4].tolist()\n",
    "                d[\"prob\"] = i[4].tolist()\n",
    "                detections.append(d)\n",
    "            results.append(detections)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "model = WrapperModel(original_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the model with bentoml."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import bentoml\n",
    "\n",
    "bentoml.pytorch.save_model(\n",
    "    \"pytorch_yolov5\",\n",
    "    model,\n",
    "    signatures={\"__call__\": {\"batchable\": True, \"batchdim\": 0}},\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Create a BentoML Service for serving the model\n",
    "\n",
    "Tips:\n",
    "* using `%%writefile` here because `bentoml.Service` instance must be created in a separate `.py` file\n",
    "* Even though we have only one model, we can create as many api endpoints as we want."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile service.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "import bentoml\n",
    "from bentoml.io import JSON\n",
    "from bentoml.io import Image\n",
    "\n",
    "yolo_runner = bentoml.pytorch.get(\"pytorch_yolov5\").to_runner()\n",
    "\n",
    "svc = bentoml.Service(\n",
    "    name=\"pytorch_yolo_demo\",\n",
    "    runners=[yolo_runner],\n",
    ")\n",
    "\n",
    "\n",
    "sys.path.append('yolov5')\n",
    "\n",
    "@svc.api(input=Image(), output=JSON())\n",
    "def predict_image(img: PIL.Image.Image) -> list:\n",
    "    assert isinstance(img, PIL.Image.Image)\n",
    "    return yolo_runner.run([np.array(img)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a dev model server to test out the service defined above"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!bentoml serve service.py:svc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use something like:\n",
    "\n",
    "`curl -H \"Content-Type: multipart/form-data\" -F 'fileobj=@yolov5/data/images/bus.jpg;type=image/png' http://127.0.0.1:3000/predict_image`\n",
    "    \n",
    "to send an image to the digit recognition service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Build a Bento for distribution and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting a dev server with the Bento build:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!bentoml build"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Deploy the Bento to production\n",
    "\n",
    "BentoML supports deploying the BentoML service to AWS Lambda, AWS SageMaker, GCP Cloud Run, Azure Functions, Kubernetes, any docker-compatible environment or to a local server."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# to deploy to local server:\n",
    "!bentoml serve pytorch_yolo_demo:latest --production\n",
    "\n",
    "# deploy to docker:\n",
    "!bentoml containerize pytorch_yolo_demo -t pytorch_yolo_demo:latest\n",
    "!docker run -p 5000:5000 pytorch_yolo_demo:latest\n",
    "\n",
    "# to deploy to AWS Lambda:\n",
    "!bentoctl deploy aws-lambda pytorch_yolo_demo:latest"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}