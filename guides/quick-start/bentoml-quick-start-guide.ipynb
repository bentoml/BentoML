{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open-source framework for high-performance machine learning model serving. It makes it easy to build production API endpoints for trained ML models and supports all major machine learning frameworks, including Tensorflow, Keras, PyTorch, XGBoost, scikit-learn, fastai, etc.\n",
    "\n",
    "BentoML comes with a high-performance API model server with adaptive micro-batching support, bringing the advantage of batch processing to online serving workloads. It also provides batch serving, model management and model deployment functionality, which gives ML teams an end-to-end model serving solution with baked-in DevOps best practices.\n",
    "\n",
    "This is a quick tutorial on how to use BentoML to serve a sklearn modeld via a REST API server, containerize the API model server with Docker, and deploy it to [AWS Lambda](https://aws.amazon.com/lambda/) as a serverless endpoint.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=guides&ea=bentoml-quick-start-guide&dt=bentoml-quick-start-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML requires python 3.6 or above, install dependencies via `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyPI packages required in this guide, including BentoML\n",
    "!pip install -q bentoml pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Prediction Service with BentoML\n",
    "\n",
    "\n",
    "A minimal prediction service in BentoML looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier.py\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.artifact import SklearnModelArtifact\n",
    "\n",
    "@env(auto_pip_dependencies=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(input=DataframeInput())\n",
    "    def predict(self, df):\n",
    "        # Optional pre-processing, post-processing code goes here\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a prediction service that bundles a scikit-learn model and provides an\n",
    "API that expects input data in the form of `pandas.Dataframe`. The user-defined API\n",
    "function `predict` defines how the input dataframe data will be processed and used for \n",
    "inference with the bundled scikit-learn model. BentoML also supports other API input \n",
    "types such as `ImageInput`, `JsonInput` and \n",
    "[more](https://docs.bentoml.org/en/latest/api/adapters.html).\n",
    "\n",
    "The following code trains a scikit-learn model and packages the trained model with the\n",
    "`IrisClassifier` class defined above. It then saves the IrisClassifier instance to disk \n",
    "in the BentoML SavedBundle format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-06 00:35:30,716] INFO - BentoService bundle 'IrisClassifier:20200506003514_CBCF1F' saved to: /Users/chaoyu/bentoml/repository/IrisClassifier/20200506003514_CBCF1F\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "# import the custom BentoService defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', clf)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, BentoML stores SavedBundle files under the `~/bentoml` directory. Users \n",
    "can also customize BentoML to use a different directory or cloud storage like\n",
    "[AWS S3](https://aws.amazon.com/s3/) and [MinIO](https://min.io/), via BentoML's\n",
    "model management component [YataiService](https://docs.bentoml.org/en/latest/concepts.html#customizing-model-repository),\n",
    "which provides advanced model management features including a dashboard web UI:\n",
    "\n",
    "![BentoML YataiService Bento Repository Page](https://raw.githubusercontent.com/bentoml/BentoML/master/docs/source/_static/img/yatai-service-web-ui-repository.png)\n",
    "\n",
    "![BentoML YataiService Bento Details Page](https://raw.githubusercontent.com/bentoml/BentoML/master/docs/source/_static/img/yatai-service-web-ui-repository-detail.png)\n",
    "\n",
    "\n",
    "Learn more about using YataiService for model management and try out the Web UI \n",
    "[here](https://docs.bentoml.org/en/latest/concepts.html#model-management)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_path: /Users/chaoyu/bentoml/repository/IrisClassifier/20200506003514_CBCF1F\n",
      "version: 20200506003514_CBCF1F\n"
     ]
    }
   ],
   "source": [
    "# Where the SavedBundle directory is saved to\n",
    "print(\"saved_path:\", saved_path)\n",
    "\n",
    "# Print the auto-generated service version\n",
    "print(\"version:\", iris_classifier_service.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "\n",
    "\n",
    "The BentoML SavedBundle directory contains all the code, data and configs required to \n",
    "deploy the model. \n",
    "\n",
    "To start a REST API model server with the `IrisClassifier` SavedBundle, use the `bentoml serve` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-06 00:35:32,943] INFO - Getting latest version IrisClassifier:20200506003514_CBCF1F\n",
      " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [06/May/2020 00:35:40] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Note that REST API serving **does not work in Google Colab** due to unable to access Colab's VM\n",
    "!bentoml serve IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IrisClassifier` model is now served at `localhost:5000`. Use `curl` command to send\n",
    "a prediction request:\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "Or with `python` and [request library](https://requests.readthedocs.io/):\n",
    "```python\n",
    "import requests\n",
    "response = requests.post(\"http://127.0.0.1:5000/predict\", json=[[5.1, 3.5, 1.4, 0.2]])\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "The BentoML API server also provides a web UI for accessing predictions and debugging \n",
    "the server. Visit http://localhost:5000 in the browser and use the Web UI to send\n",
    "prediction request:\n",
    "\n",
    "![BentoML API Server Web UI Screenshot](https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/bento-api-server-web-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containerize model server with Docker\n",
    "\n",
    "\n",
    "BentoML provides a convenient way to containerize the model API server with Docker. Simply run `docker build` with the SavedBundle directory which contains a generated Dockerfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:fc32db7fbc628ce9b5fd301b01d1f5aba808ae5863b449aa570f5c2c54f3bf96\r\n"
     ]
    }
   ],
   "source": [
    "!docker build -q -t  iris-classifier {saved_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `docker` is __note available in Google Colab__, download the notebook, ensure docker is installed and try it locally.\n",
    "\n",
    "Run the generated docker image to start a docker container serving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-06 07:37:07,693] INFO - get_gunicorn_num_of_workers: 3, calculated by cpu count\n",
      "[2020-05-06 07:37:07,704] INFO - Running micro batch service on :5000\n",
      "[2020-05-06 07:37:07 +0000] [9] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-05-06 07:37:07 +0000] [9] [INFO] Listening at: http://0.0.0.0:5000 (9)\n",
      "[2020-05-06 07:37:07 +0000] [9] [INFO] Using worker: aiohttp.worker.GunicornWebWorker\n",
      "[2020-05-06 07:37:07 +0000] [10] [INFO] Booting worker with pid: 10\n",
      "[2020-05-06 07:37:08,008] INFO - Micro batch enabled for API `predict`\n",
      "[2020-05-06 07:37:08,009] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "[2020-05-06 07:37:08 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-05-06 07:37:08 +0000] [1] [INFO] Listening at: http://0.0.0.0:53331 (1)\n",
      "[2020-05-06 07:37:08 +0000] [1] [INFO] Using worker: sync\n",
      "[2020-05-06 07:37:08 +0000] [11] [INFO] Booting worker with pid: 11\n",
      "[2020-05-06 07:37:08 +0000] [12] [INFO] Booting worker with pid: 12\n",
      "[2020-05-06 07:37:08 +0000] [13] [INFO] Booting worker with pid: 13\n",
      "^C\n",
      "[2020-05-06 07:37:16 +0000] [1] [INFO] Handling signal: int\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator SVC from version 0.22 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "[2020-05-06 07:37:16 +0000] [12] [INFO] Worker exiting (pid: 12)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator SVC from version 0.22 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "[2020-05-06 07:37:16 +0000] [13] [INFO] Worker exiting (pid: 13)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator SVC from version 0.22 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "[2020-05-06 07:37:16 +0000] [11] [INFO] Worker exiting (pid: 11)\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 5000:5000 -e BENTOML_ENABLE_MICROBATCH=True iris-classifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This made it possible to deploy BentoML bundled ML models with platforms such as\n",
    "[Kubeflow](https://www.kubeflow.org/docs/components/serving/bentoml/),\n",
    "[Knative](https://knative.dev/community/samples/serving/machinelearning-python-bentoml/),\n",
    "[Kubernetes](https://docs.bentoml.org/en/latest/deployment/kubernetes.html), which\n",
    "provides advanced model deployment features such as auto-scaling, A/B testing,\n",
    "scale-to-zero, canary rollout and multi-armed bandit.\n",
    "\n",
    "\n",
    "## Load saved BentoService\n",
    "\n",
    "`bentoml.load` is the enssential API for loading a Bento into your\n",
    "python application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-06 00:37:20,492] WARNING - Module `iris_classifier` already loaded, using existing imported module.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "memmap([0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "import pandas as pd\n",
    "\n",
    "bento_svc = bentoml.load(saved_path)\n",
    "\n",
    "# Test loaded bentoml service:\n",
    "bento_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful for building test pipeline for your prediction service or using the same predictions service for  offline batch serving.\n",
    "\n",
    "\n",
    "## Distribute BentoML SavedBundle as PyPI package\n",
    "\n",
    "\n",
    "The BentoML SavedBundle is pip-installable and can be directly distributed as a\n",
    "PyPI package for use in python applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The BentoService class name will become packaged name\n",
    "import IrisClassifier\n",
    "\n",
    "installed_svc = IrisClassifier.load()\n",
    "installed_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also allow users to upload their BentoService to pypi.org as public python package\n",
    "or to their organization's private PyPi index to share with other developers.\n",
    "\n",
    "`cd {saved_path} & python setup.py sdist upload`\n",
    "\n",
    "*You will have to configure \".pypirc\" file before uploading to pypi index.\n",
    "    You can find more information about distributing python package at:\n",
    "    https://docs.python.org/3.7/distributing/index.html#distributing-index*\n",
    "\n",
    "\n",
    "# Batch Offline Serving via CLI\n",
    "\n",
    "`pip install {saved_path}` also installs a CLI tool for accessing the BentoML service, print CLI help document with `--help`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: IrisClassifier [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  BentoML CLI tool\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version  Show the version and exit.\r\n",
      "  --help     Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  info                List APIs\r\n",
      "  install-completion  Install shell command completion\r\n",
      "  open-api-spec       Display OpenAPI/Swagger JSON specs\r\n",
      "  run                 Run API function\r\n",
      "  serve               Start local rest server\r\n",
      "  serve-gunicorn      Start local gunicorn server\r\n"
     ]
    }
   ],
   "source": [
    "!IrisClassifier --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the help manual for the `run` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: IrisClassifier run [OPTIONS] API_NAME [RUN_ARGS]...\r\n",
      "\r\n",
      "  Run a API defined in saved BentoService bundle from command line\r\n",
      "\r\n",
      "Options:\r\n",
      "  --with-conda        Run API server in a BentoML managed Conda environment\r\n",
      "  -q, --quiet         Hide all warnings and info logs\r\n",
      "  --verbose, --debug  Show debug logs when running the command\r\n",
      "  --help              Show this message and exit.\r\n"
     ]
    }
   ],
   "source": [
    "!IrisClassifier run predict --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction job from CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\r\n"
     ]
    }
   ],
   "source": [
    "!IrisClassifier run predict --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML cli also supports reading input data from `csv` or `json` files, in either local machine or remote HTTP/S3 location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\r\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\r\n",
      " 2 2]\r\n"
     ]
    }
   ],
   "source": [
    "!IrisClassifier run predict --input=\"https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/iris_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same CLI command is also available via `bentoml` cli, by specifying the BentoService name and version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-06 00:37:51,843] INFO - Getting latest version IrisClassifier:20200506003514_CBCF1F\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "!bentoml run IrisClassifier:latest predict --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy API model server to cloud services\n",
    "\n",
    "\n",
    "BentoML can deploy SavedBundle directly to cloud services such as AWS Lambda or \n",
    "AWS SageMaker, with the bentoml CLI command. Check out the deployment guides and \n",
    "other deployment options with BentoML [here](https://docs.bentoml.org/en/latest/deployment/index.html).\n",
    "\n",
    "\n",
    "The following part of the notebook, demonstrates how to deploy the IrisClassifier\n",
    "model server built in the previous steps, to [AWS Lambda](https://aws.amazon.com/lambda/)\n",
    "as a serverless endpoint.\n",
    "\n",
    "Before started, install the `aws-sam-cli` package, which is required by BentoML\n",
    "to create AWS Lambda deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U aws-sam-cli==0.33.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure an AWS account and credentials is configured either via\n",
    "[environment variables](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html)\n",
    "or the `aws configure` command. (Install `aws` cli command via `pip install awscli` and follow\n",
    "[instructions here](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration))\n",
    "\n",
    "To create a BentoML deployment on AWS Lambda, using the `bentoml lambda deploy` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying \"IrisClassifier:20200506003514_CBCF1F\" to AWS Lambda -[2020-05-06 00:44:50,537] INFO - Building lambda project\n",
      "\b|[2020-05-06 00:47:07,298] INFO - Packaging AWS Lambda project at /private/var/folders/7p/y_934t3s4yg8fx595vr28gym0000gn/T/bentoml-temp-truw0360 ...\n",
      "\b/[2020-05-06 00:49:06,140] INFO - Deploying lambda project\n",
      "\b\\[2020-05-06 00:49:56,374] INFO - ApplyDeployment (quick-start-guide-deployment, namespace dev) succeeded\n",
      "\u001b[32mSuccessfully created AWS Lambda deployment quick-start-guide-deployment\u001b[0m\n",
      "\u001b[39m{\n",
      "  \"namespace\": \"dev\",\n",
      "  \"name\": \"quick-start-guide-deployment\",\n",
      "  \"spec\": {\n",
      "    \"bentoName\": \"IrisClassifier\",\n",
      "    \"bentoVersion\": \"20200506003514_CBCF1F\",\n",
      "    \"operator\": \"AWS_LAMBDA\",\n",
      "    \"awsLambdaOperatorConfig\": {\n",
      "      \"region\": \"us-west-2\",\n",
      "      \"memorySize\": 1024,\n",
      "      \"timeout\": 3\n",
      "    }\n",
      "  },\n",
      "  \"state\": {\n",
      "    \"state\": \"RUNNING\",\n",
      "    \"infoJson\": {\n",
      "      \"endpoints\": [\n",
      "        \"https://y9jg9ljf21.execute-api.us-west-2.amazonaws.com/Prod/predict\"\n",
      "      ],\n",
      "      \"s3_bucket\": \"btml-dev-quick-start-guide-deployment-b15326\"\n",
      "    },\n",
      "    \"timestamp\": \"2020-05-06T07:49:56.600224Z\"\n",
      "  },\n",
      "  \"createdAt\": \"2020-05-06T07:44:45.627817Z\",\n",
      "  \"lastUpdatedAt\": \"2020-05-06T07:44:45.627842Z\"\n",
      "}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml lambda deploy quick-start-guide-deployment -b IrisClassifier:{iris_classifier_service.version} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'quick-starrt-guide-deployment' here is the deployment name, which can be used to query the current deployment status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m{\r\n",
      "  \"namespace\": \"dev\",\r\n",
      "  \"name\": \"quick-start-guide-deployment\",\r\n",
      "  \"spec\": {\r\n",
      "    \"bentoName\": \"IrisClassifier\",\r\n",
      "    \"bentoVersion\": \"20200506003514_CBCF1F\",\r\n",
      "    \"operator\": \"AWS_LAMBDA\",\r\n",
      "    \"awsLambdaOperatorConfig\": {\r\n",
      "      \"region\": \"us-west-2\",\r\n",
      "      \"memorySize\": 1024,\r\n",
      "      \"timeout\": 3\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"state\": {\r\n",
      "    \"state\": \"RUNNING\",\r\n",
      "    \"infoJson\": {\r\n",
      "      \"endpoints\": [\r\n",
      "        \"https://y9jg9ljf21.execute-api.us-west-2.amazonaws.com/Prod/predict\"\r\n",
      "      ],\r\n",
      "      \"s3_bucket\": \"btml-dev-quick-start-guide-deployment-b15326\"\r\n",
      "    },\r\n",
      "    \"timestamp\": \"2020-05-06T07:50:03.839368Z\"\r\n",
      "  },\r\n",
      "  \"createdAt\": \"2020-05-06T07:44:45.627817Z\",\r\n",
      "  \"lastUpdatedAt\": \"2020-05-06T07:44:45.627842Z\"\r\n",
      "}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml lambda get quick-start-guide-deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://y9jg9ljf21.execute-api.us-west-2.amazonaws.com/Prod/predict\r\n"
     ]
    }
   ],
   "source": [
    "!endpoint=$(bentoml lambda get quick-start-guide-deployment | jq -r \".state.infoJson.endpoints[0]\") && \\\n",
    "    echo $endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send request to your AWS Lambda deployment, grab the endpoint URL from the json output above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r",
      "\r\n",
      "\u001b[1mContent-Type\u001b[0m: application/json\r",
      "\r\n",
      "\u001b[1mContent-Length\u001b[0m: 3\r",
      "\r\n",
      "\u001b[1mConnection\u001b[0m: keep-alive\r",
      "\r\n",
      "\u001b[1mDate\u001b[0m: Wed, 06 May 2020 07:53:14 GMT\r",
      "\r\n",
      "\u001b[1mx-amzn-RequestId\u001b[0m: c8a3b0a9-5ce8-4ac0-bf5c-33c83f19cd8c\r",
      "\r\n",
      "\u001b[1mx-amz-apigw-id\u001b[0m: MGX0pGKQPHcFTJQ=\r",
      "\r\n",
      "\u001b[1mX-Amzn-Trace-Id\u001b[0m: Root=1-5eb26cea-0a367f6e0b0caad748cb0838;Sampled=0\r",
      "\r\n",
      "\u001b[1mX-Cache\u001b[0m: Miss from cloudfront\r",
      "\r\n",
      "\u001b[1mVia\u001b[0m: 1.1 5b060cb62fb310be9e41fb3218654679.cloudfront.net (CloudFront)\r",
      "\r\n",
      "\u001b[1mX-Amz-Cf-Pop\u001b[0m: SFO20-C1\r",
      "\r\n",
      "\u001b[1mX-Amz-Cf-Id\u001b[0m: YjYSmzncyE4eNlu6RbQ0Lne9DkLEevs2DEEwVjLeJo8mcd-Kf8ehKg==\r",
      "\r\n",
      "\r",
      "\r\n",
      "[0]"
     ]
    }
   ],
   "source": [
    "!curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "$(bentoml lambda get quick-start-guide-deployment | jq -r \".state.infoJson.endpoints[0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list all the deployments you've created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39mNAME                          NAMESPACE    PLATFORM    BENTO_SERVICE                         STATUS    AGE\r\n",
      "quick-start-guide-deployment  dev          aws-lambda  IrisClassifier:20200506003514_CBCF1F  running   8 minutes and 31.06 seconds\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml deployment list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to delete an active deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully deleted AWS Lambda deployment \"quick-start-guide-deployment\"\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml lambda delete quick-start-guide-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML by default stores the deployment metadata on the local machine. For team settings, we recommend hosting a shared BentoML YataiService for a data science team to track all their BentoML SavedBundles and model serving deployments created. See related documentation [here](https://docs.bentoml.org/en/latest/concepts.html#customizing-model-repository)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is what it looks like when using BentoML to serve and deploy a model in the cloud. BentoML also supports [many other Machine Learning frameworks](https://docs.bentoml.org/en/latest/examples.html), as well as [many other deployment platforms](https://docs.bentoml.org/en/latest/deployment/index.html). The [BentoML core concepts](https://docs.bentoml.org/en/latest/concepts.html) doc is also recommended for anyone looking to get a deeper understanding of BentoML.\n",
    "\n",
    "Join the [BentoML Slack](https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg) to follow the latest development updates and roadmap discussions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
