{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open-source framework for high-performance machine learning model serving. It makes it easy to build production API endpoints for trained ML models and supports all major machine learning frameworks, including Tensorflow, Keras, PyTorch, XGBoost, scikit-learn, fastai, etc.\n",
    "\n",
    "BentoML comes with a high-performance API model server with adaptive micro-batching support, bringing the advantage of batch processing to online serving workloads. It also provides batch serving, model management and model deployment functionality, which gives ML teams an end-to-end model serving solution with baked-in DevOps best practices.\n",
    "\n",
    "This is a quick tutorial on how to use BentoML to serve a sklearn modeld via a REST API server, containerize the API model server with Docker, and deploy it to [AWS Lambda](https://aws.amazon.com/lambda/) as a serverless endpoint.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=guides&ea=bentoml-quick-start-guide&dt=bentoml-quick-start-guide)\n",
    "\n",
    "BentoML requires python 3.6 or above, install dependencies via `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyPI packages required in this guide, including BentoML\n",
    "!pip install -q bentoml 'scikit-learn>=0.23.2' 'pandas>=1.1.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier model with the Iris flower data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Prediction Service with BentoML\n",
    "\n",
    "\n",
    "A minimal prediction service in BentoML looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier.py\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.artifact import SklearnModelArtifact\n",
    "\n",
    "@env(auto_pip_dependencies=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(input=DataframeInput())\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        # Optional pre-processing, post-processing code goes here\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a prediction service that bundles a scikit-learn model and provides an\n",
    "API that expects input data in the form of `pandas.Dataframe`. The user-defined API\n",
    "function `predict` defines how the input dataframe data will be processed and used for \n",
    "inference with the bundled scikit-learn model. BentoML also supports other API input \n",
    "types such as `ImageInput`, `JsonInput` and \n",
    "[more](https://docs.bentoml.org/en/latest/api/adapters.html).\n",
    "\n",
    "The following code packages the trained model with the\n",
    "`IrisClassifier` class defined above. It then saves the IrisClassifier instance to disk \n",
    "in the BentoML SavedBundle format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-27 22:25:52,733] INFO - BentoService bundle 'IrisClassifier:20200827222552_151AF3' saved to: /Users/chaoyu/bentoml/repository/IrisClassifier/20200827222552_151AF3\n"
     ]
    }
   ],
   "source": [
    "# import the custom BentoService defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', clf)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, BentoML stores SavedBundle files under the `~/bentoml` directory. Users \n",
    "can also customize BentoML to use a different directory or cloud storage like\n",
    "[AWS S3](https://aws.amazon.com/s3/) and [MinIO](https://min.io/), via BentoML's\n",
    "model management component [YataiService](https://docs.bentoml.org/en/latest/concepts.html#customizing-model-repository),\n",
    "which provides advanced model management features including a dashboard web UI:\n",
    "\n",
    "![BentoML YataiService Bento Repository Page](https://raw.githubusercontent.com/bentoml/BentoML/master/docs/source/_static/img/yatai-service-web-ui-repository.png)\n",
    "\n",
    "![BentoML YataiService Bento Details Page](https://raw.githubusercontent.com/bentoml/BentoML/master/docs/source/_static/img/yatai-service-web-ui-repository-detail.png)\n",
    "\n",
    "Start the YataiService web server on your local development machine with the CLI command `bentoml yatai-service-start` and visit http://127.0.0.1:3000 to view the web UI. More documentation about model management can be found [here](https://docs.bentoml.org/en/latest/concepts.html#model-management)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_path: /Users/chaoyu/bentoml/repository/IrisClassifier/20200827222552_151AF3\n",
      "version: 20200827222552_151AF3\n"
     ]
    }
   ],
   "source": [
    "# Where the SavedBundle directory is saved to\n",
    "print(\"saved_path:\", saved_path)\n",
    "\n",
    "# Print the auto-generated service version\n",
    "print(\"version:\", iris_classifier_service.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m/Users/chaoyu/bentoml/repository/IrisClassifier/20200827222552_151AF3\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Find the saved path from CLI:\n",
    "!bentoml get IrisClassifier:latest --print-location --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "\n",
    "\n",
    "The BentoML SavedBundle directory contains all the code, data and configs required to \n",
    "deploy the model. \n",
    "\n",
    "To start a local development REST API model server with the `IrisClassifier` SavedBundle, use the `bentoml serve` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-27 22:27:00,936] INFO - Getting latest version IrisClassifier:20200827222552_151AF3\n",
      "[2020-08-27 22:27:00,936] INFO - Starting BentoML API server in development mode..\n",
      " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Aug/2020 22:27:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [27/Aug/2020 22:27:05] \"\u001b[37mGET /swagger_static/swagger-ui.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [27/Aug/2020 22:27:05] \"\u001b[37mGET /swagger_static/swagger-ui-bundle.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [27/Aug/2020 22:27:05] \"\u001b[37mGET /docs.json HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [27/Aug/2020 22:27:05] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "[2020-08-27 22:27:15,393] INFO - {'request_id': 'ad4692bd-0dd0-450f-89d8-88cf8026a687', 'service_name': 'IrisClassifier', 'service_version': '20200827222552_151AF3', 'api': 'predict', 'request': [[5.1, 3.5, 1.4, 0.2]], 'response_code': 200, 'response': [b'[0]']}\n",
      "127.0.0.1 - - [27/Aug/2020 22:27:15] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook from Google Colab, you can start the dev server with `--run-with-ngrok` option, to gain acccess to the API endpoint via a public endpoint managed by [ngrok](https://ngrok.com/): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bentoml serve IrisClassifier:latest --run-with-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IrisClassifier` model is now served at `localhost:5000`. Use `curl` command to send\n",
    "a prediction request:\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "Or with `python` and [request library](https://requests.readthedocs.io/):\n",
    "```python\n",
    "import requests\n",
    "response = requests.post(\"http://127.0.0.1:5000/predict\", json=[[5.1, 3.5, 1.4, 0.2]])\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "The BentoML API server also provides a web UI for accessing predictions and debugging \n",
    "the server. Visit http://localhost:5000 in the browser and use the Web UI to send\n",
    "prediction request:\n",
    "\n",
    "![BentoML API Server Web UI Screenshot](https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/bento-api-server-web-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containerize model server with Docker\n",
    "\n",
    "\n",
    "BentoML provides a convenient way to containerize the model API server with Docker. Simply run `docker build` with the SavedBundle directory which contains a generated Dockerfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:8be009f63cdfd31231461885ad47aa871d90f0e44296d85d35c09a9818bd54c2\r\n"
     ]
    }
   ],
   "source": [
    "!saved_path=$(bentoml get IrisClassifier:latest --print-location --quiet)\n",
    "!docker build -q -t iris-classifier $saved_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML also provide an equivilant CLI command for building docker image via the Docker deamon configured in current environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-27 22:36:28,926] INFO - Getting latest version IrisClassifier:20200827222552_151AF3\n",
      "\u001b[39mFound Bento: /Users/chaoyu/bentoml/repository/IrisClassifier/20200827222552_151AF3\u001b[0m\n",
      "\u001b[33mImage version not specified, using version parsed from BentoService: '20200827222552_151AF3'\u001b[0m\n",
      "Building Docker image iris-classifier:20200827222552_151AF3 from IrisClassifier:latest \n",
      "|\u001b[39mStep 1/15 : FROM bentoml/model-server:0.8.6\u001b[0m\n",
      "\u001b[39m ---> 71644b758bed\u001b[0m\n",
      "\u001b[39mStep 2/15 : COPY . /bento\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 9b8fed7107d2\u001b[0m\n",
      "\u001b[39mStep 3/15 : WORKDIR /bento\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 4095858ad689\u001b[0m\n",
      "\u001b[39mStep 4/15 : ARG PIP_INDEX_URL=https://pypi.python.org/simple/\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 92b03d63dd6c\u001b[0m\n",
      "\u001b[39mStep 5/15 : ARG PIP_TRUSTED_HOST=pypi.python.org\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 648df1c702d4\u001b[0m\n",
      "\u001b[39mStep 6/15 : ENV PIP_INDEX_URL $PIP_INDEX_URL\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 8916e89508bd\u001b[0m\n",
      "\u001b[39mStep 7/15 : ENV PIP_TRUSTED_HOST $PIP_TRUSTED_HOST\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 1ba6fd6eb8ed\u001b[0m\n",
      "\u001b[39mStep 8/15 : RUN chmod +x /bento/bentoml-init.sh\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 321b4854d808\u001b[0m\n",
      "\u001b[39mStep 9/15 : RUN if [ -f /bento/bentoml-init.sh ]; then bash -c /bento/bentoml-init.sh; fi\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 77e33ee28405\u001b[0m\n",
      "\u001b[39mStep 10/15 : ENV PORT 5000\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 66ccbae87806\u001b[0m\n",
      "\u001b[39mStep 11/15 : EXPOSE $PORT\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 83b5293ffb11\u001b[0m\n",
      "\u001b[39mStep 12/15 : COPY docker-entrypoint.sh /usr/local/bin/\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> ed9d20790b3a\u001b[0m\n",
      "\u001b[39mStep 13/15 : RUN chmod +x /usr/local/bin/docker-entrypoint.sh\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> a95713bcc164\u001b[0m\n",
      "\u001b[39mStep 14/15 : ENTRYPOINT [ \"docker-entrypoint.sh\" ]\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 46453ff9f78f\u001b[0m\n",
      "\u001b[39mStep 15/15 : CMD [\"bentoml\", \"serve-gunicorn\", \"/bento\"]\u001b[0m\n",
      "\u001b[39m ---> Using cache\u001b[0m\n",
      "\u001b[39m ---> 33069f2aa268\u001b[0m\n",
      "\u001b[39mSuccessfully built 33069f2aa268\u001b[0m\n",
      "\u001b[39mSuccessfully tagged iris-classifier:20200827222552_151AF3\u001b[0m\n",
      "\u001b[32mFinished building iris-classifier:20200827222552_151AF3 from IrisClassifier:latest\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml containerize IrisClassifier:latest -t iris-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `docker` is __note available in Google Colab__, download the notebook, ensure docker is installed and try it locally.\n",
    "\n",
    "Run the generated docker image to start a docker container serving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-28 05:36:33,644] INFO - Starting BentoML API server in production mode..\n",
      "[2020-08-28 05:36:34 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-08-28 05:36:34 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n",
      "[2020-08-28 05:36:34 +0000] [1] [INFO] Using worker: sync\n",
      "[2020-08-28 05:36:34 +0000] [11] [INFO] Booting worker with pid: 11\n",
      "[2020-08-28 05:36:38,315] INFO - {'request_id': '9b85d80c-365a-4529-952a-746cfa2359ea', 'service_name': 'IrisClassifier', 'service_version': '20200827222552_151AF3', 'api': 'predict', 'request': [[5.1, 3.5, 1.4, 0.2]], 'response_code': 200, 'response': [b'[0]']}\n",
      "^C\n",
      "[2020-08-28 05:36:41 +0000] [1] [INFO] Handling signal: int\n",
      "[2020-08-28 05:36:41 +0000] [11] [INFO] Worker exiting (pid: 11)\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 5000:5000 iris-classifier:latest --workers=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This made it possible to deploy BentoML bundled ML models with platforms such as\n",
    "[Kubeflow](https://www.kubeflow.org/docs/components/serving/bentoml/),\n",
    "[Knative](https://knative.dev/community/samples/serving/machinelearning-python-bentoml/),\n",
    "[Kubernetes](https://docs.bentoml.org/en/latest/deployment/kubernetes.html), which\n",
    "provides advanced model deployment features such as auto-scaling, A/B testing,\n",
    "scale-to-zero, canary rollout and multi-armed bandit.\n",
    "\n",
    "\n",
    "## Load saved BentoService\n",
    "\n",
    "`bentoml.load` is the enssential API for loading a Bento into your\n",
    "python application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-27 22:36:46,861] WARNING - Module `iris_classifier` already loaded, using existing imported module.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "memmap([0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "import pandas as pd\n",
    "\n",
    "bento_svc = bentoml.load(saved_path)\n",
    "\n",
    "# Test loaded bentoml service:\n",
    "bento_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful for building test pipeline for your prediction service or using the same predictions service for  offline batch serving.\n",
    "\n",
    "\n",
    "## Distribute BentoML SavedBundle as PyPI package\n",
    "\n",
    "\n",
    "The BentoML SavedBundle is pip-installable and can be directly distributed as a\n",
    "PyPI package for use in python applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The BentoService class name will become packaged name\n",
    "import IrisClassifier\n",
    "\n",
    "installed_svc = IrisClassifier.load()\n",
    "installed_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also allow users to upload their BentoService to pypi.org as public python package\n",
    "or to their organization's private PyPi index to share with other developers.\n",
    "\n",
    "`cd {saved_path} & python setup.py sdist upload`\n",
    "\n",
    "*You will have to configure \".pypirc\" file before uploading to pypi index.\n",
    "    You can find more information about distributing python package at:\n",
    "    https://docs.python.org/3.7/distributing/index.html#distributing-index*\n",
    "\n",
    "\n",
    "# Batch Offline Serving via CLI\n",
    "\n",
    "`pip install {saved_path}` also installs a CLI tool for accessing the BentoML service, print CLI help document with `--help`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: IrisClassifier [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  BentoML CLI tool\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version  Show the version and exit.\r\n",
      "  --help     Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  containerize        Containerizes given Bento into a ready-to-use Docker\r\n",
      "                      image\r\n",
      "\r\n",
      "  info                List APIs\r\n",
      "  install-completion  Install shell command completion\r\n",
      "  open-api-spec       Display OpenAPI/Swagger JSON specs\r\n",
      "  run                 Run API function\r\n",
      "  serve               Start local dev API server\r\n",
      "  serve-gunicorn      Start production API server\r\n"
     ]
    }
   ],
   "source": [
    "!IrisClassifier --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the help manual for the `run` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!IrisClassifier run predict --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction job from CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\r\n"
     ]
    }
   ],
   "source": [
    "!IrisClassifier run predict --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML cli also supports reading input data from `csv` or `json` files, in either local machine or remote HTTP/S3 location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r\n",
      " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\r\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\r\n",
      " 2 2]\r\n"
     ]
    }
   ],
   "source": [
    "!IrisClassifier run predict --input=\"https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/iris_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same CLI command is also available via `bentoml` cli, by specifying the BentoService name and version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-08-27 22:37:09,964] INFO - Getting latest version IrisClassifier:20200827222552_151AF3\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "!bentoml run IrisClassifier:latest predict --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy API model server to cloud services\n",
    "\n",
    "\n",
    "BentoML can deploy SavedBundle directly to cloud services such as AWS Lambda or \n",
    "AWS SageMaker, with the bentoml CLI command. Check out the deployment guides and \n",
    "other deployment options with BentoML [here](https://docs.bentoml.org/en/latest/deployment/index.html).\n",
    "\n",
    "\n",
    "The following part of the notebook, demonstrates how to deploy the IrisClassifier\n",
    "model server built in the previous steps, to [AWS Lambda](https://aws.amazon.com/lambda/)\n",
    "as a serverless endpoint.\n",
    "\n",
    "Before started, install the `aws-sam-cli` package, which is required by BentoML\n",
    "to create AWS Lambda deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U aws-sam-cli==0.33.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure an AWS account and credentials is configured either via\n",
    "[environment variables](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html)\n",
    "or the `aws configure` command. (Install `aws` cli command via `pip install awscli` and follow\n",
    "[instructions here](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration))\n",
    "\n",
    "To create a BentoML deployment on AWS Lambda, using the `bentoml lambda deploy` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying \"IrisClassifier:20200827222552_151AF3\" to AWS Lambda -[2020-08-27 22:37:51,915] INFO - Building lambda project\n",
      "\b/[2020-08-27 22:39:07,324] INFO - Packaging AWS Lambda project at /private/var/folders/7p/y_934t3s4yg8fx595vr28gym0000gn/T/bentoml-temp-8iv7dufc ...\n",
      "\b|[2020-08-27 22:40:50,010] INFO - Deploying lambda project\n",
      "\b/[2020-08-27 22:41:41,781] INFO - ApplyDeployment (quick-start-guide-deployment, namespace dev) succeeded\n",
      "\u001b[32mSuccessfully created AWS Lambda deployment quick-start-guide-deployment\u001b[0m\n",
      "\u001b[39m{\n",
      "  \"namespace\": \"dev\",\n",
      "  \"name\": \"quick-start-guide-deployment\",\n",
      "  \"spec\": {\n",
      "    \"bentoName\": \"IrisClassifier\",\n",
      "    \"bentoVersion\": \"20200827222552_151AF3\",\n",
      "    \"operator\": \"AWS_LAMBDA\",\n",
      "    \"awsLambdaOperatorConfig\": {\n",
      "      \"region\": \"us-west-1\",\n",
      "      \"memorySize\": 1024,\n",
      "      \"timeout\": 3\n",
      "    }\n",
      "  },\n",
      "  \"state\": {\n",
      "    \"state\": \"RUNNING\",\n",
      "    \"infoJson\": {\n",
      "      \"endpoints\": [\n",
      "        \"https://qcc6weu1u3.execute-api.us-west-1.amazonaws.com/Prod/predict\"\n",
      "      ],\n",
      "      \"s3_bucket\": \"btml-dev-quick-start-guide-deployment-ea76c7\"\n",
      "    },\n",
      "    \"timestamp\": \"2020-08-28T05:41:41.948484Z\"\n",
      "  },\n",
      "  \"createdAt\": \"2020-08-28T05:37:45.380026Z\",\n",
      "  \"lastUpdatedAt\": \"2020-08-28T05:37:45.380053Z\"\n",
      "}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml lambda deploy quick-start-guide-deployment -b IrisClassifier:{iris_classifier_service.version} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'quick-starrt-guide-deployment' here is the deployment name, which can be used to query the current deployment status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m{\r\n",
      "  \"namespace\": \"dev\",\r\n",
      "  \"name\": \"quick-start-guide-deployment\",\r\n",
      "  \"spec\": {\r\n",
      "    \"bentoName\": \"IrisClassifier\",\r\n",
      "    \"bentoVersion\": \"20200827222552_151AF3\",\r\n",
      "    \"operator\": \"AWS_LAMBDA\",\r\n",
      "    \"awsLambdaOperatorConfig\": {\r\n",
      "      \"region\": \"us-west-1\",\r\n",
      "      \"memorySize\": 1024,\r\n",
      "      \"timeout\": 3\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"state\": {\r\n",
      "    \"state\": \"RUNNING\",\r\n",
      "    \"infoJson\": {\r\n",
      "      \"endpoints\": [\r\n",
      "        \"https://qcc6weu1u3.execute-api.us-west-1.amazonaws.com/Prod/predict\"\r\n",
      "      ],\r\n",
      "      \"s3_bucket\": \"btml-dev-quick-start-guide-deployment-ea76c7\"\r\n",
      "    },\r\n",
      "    \"timestamp\": \"2020-08-28T05:48:28.356241Z\"\r\n",
      "  },\r\n",
      "  \"createdAt\": \"2020-08-28T05:37:45.380026Z\",\r\n",
      "  \"lastUpdatedAt\": \"2020-08-28T05:37:45.380053Z\"\r\n",
      "}\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml lambda get quick-start-guide-deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://qcc6weu1u3.execute-api.us-west-1.amazonaws.com/Prod/predict\r\n"
     ]
    }
   ],
   "source": [
    "# Grab the endpoint URL from the command result above, this requires `jq` to be installed\n",
    "!endpoint=$(bentoml lambda get quick-start-guide-deployment | jq -r \".state.infoJson.endpoints[0]\") && \\\n",
    "    echo $endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send request to your AWS Lambda deployment, grab the endpoint URL from the json output above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r",
      "\r\n",
      "\u001b[1mContent-Type\u001b[0m: application/json\r",
      "\r\n",
      "\u001b[1mContent-Length\u001b[0m: 3\r",
      "\r\n",
      "\u001b[1mConnection\u001b[0m: keep-alive\r",
      "\r\n",
      "\u001b[1mDate\u001b[0m: Fri, 28 Aug 2020 05:51:40 GMT\r",
      "\r\n",
      "\u001b[1mx-amzn-RequestId\u001b[0m: aa9bed06-81e0-45c5-a482-b1282d6b994e\r",
      "\r\n",
      "\u001b[1mAccess-Control-Allow-Origin\u001b[0m: *\r",
      "\r\n",
      "\u001b[1mx-amz-apigw-id\u001b[0m: R904-HL4yK4Fnsw=\r",
      "\r\n",
      "\u001b[1mX-Amzn-Trace-Id\u001b[0m: Root=1-5f489b6c-1d5a18b950f9e017fbc54850;Sampled=0\r",
      "\r\n",
      "\u001b[1mX-Cache\u001b[0m: Miss from cloudfront\r",
      "\r\n",
      "\u001b[1mVia\u001b[0m: 1.1 2de9b6504a97ad8423645370927ef0cf.cloudfront.net (CloudFront)\r",
      "\r\n",
      "\u001b[1mX-Amz-Cf-Pop\u001b[0m: SFO20-C1\r",
      "\r\n",
      "\u001b[1mX-Amz-Cf-Id\u001b[0m: yB8HL9n-MrF_CzW3HDEzoU9z82Q-mWjc4M9WQQMAzARKksOtFmiNFg==\r",
      "\r\n",
      "\r",
      "\r\n",
      "[0]"
     ]
    }
   ],
   "source": [
    "! curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "$(bentoml lambda get quick-start-guide-deployment | jq -r \".state.infoJson.endpoints[0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list all the deployments you've created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39mNAME                          NAMESPACE    PLATFORM    BENTO_SERVICE                         STATUS    AGE\r\n",
      "quick-start-guide-deployment  dev          aws-lambda  IrisClassifier:20200827222552_151AF3  running   13 minutes and 57.26 seconds\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml deployment list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to delete an active deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml deployment delete quick-start-guide-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML by default stores the deployment metadata on the local machine. For team settings, we recommend hosting a shared BentoML YataiService for a data science team to track all their BentoML SavedBundles and model serving deployments created. See related documentation [here](https://docs.bentoml.org/en/latest/concepts.html#customizing-model-repository)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is what it looks like when using BentoML to serve and deploy a model in the cloud. BentoML also supports [many other Machine Learning frameworks](https://docs.bentoml.org/en/latest/examples.html), as well as [many other deployment platforms](https://docs.bentoml.org/en/latest/deployment/index.html). The [BentoML core concepts](https://docs.bentoml.org/en/latest/concepts.html) doc is also recommended for anyone looking to get a deeper understanding of BentoML.\n",
    "\n",
    "Join the [BentoML Slack](https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg) to follow the latest development updates and roadmap discussions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
