{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open source framework for serving and deploying machine learning models. It provides high-level APIs for defining a prediction service and packaging trained models, source code, dependencies, and configurations into a production-system-friendly format that is ready for production deployment.\n",
    "\n",
    "This is a quick tutorial on how to use BentoML to create a prediction service with a trained sklearn model, serving the model via a REST API server and deploy it to [AWS Lambda](https://aws.amazon.com/lambda/) as a serverless endpoint.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=guides&ea=bentoml-quick-start-guide&dt=bentoml-quick-start-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install BentoML\n",
    "!pip install bentoml\n",
    "\n",
    "# Install scikit-learn, we will use a sklean model as an example\n",
    "!pip install pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started with a simple scikit-learn model as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BentoService for model serving\n",
    "\n",
    "To package this trained model for model serving in production, you will need to create a custom BentoService class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_classifier.py\n",
    "from bentoml import BentoService, api, env, artifacts\n",
    "from bentoml.artifact import SklearnModelArtifact\n",
    "from bentoml.handlers import DataframeHandler\n",
    "\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "@env(pip_dependencies=[\"scikit-learn\"])\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(DataframeHandler)\n",
    "    def predict(self, df):\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@artifacts` decorator here tells BentoML what artifacts are required when \n",
    "packaging this BentoService. Besides `SklearnModelArtifact`, BentoML also provides\n",
    "`KerasModelArtifact`, `PytorchModelArtifact`, `FastaiModelArtifact` and \n",
    "`PickleArtifact` etc.\n",
    "\n",
    "`@env` is designed for specifying the desired system environment in order for this\n",
    "BentoService to load. If you already have a requirement.txt file listing all python \n",
    "libraries you need:\n",
    "```python\n",
    "@env(requirement_txt='../myproject/requirement.txt')\n",
    "```\n",
    "\n",
    "Lastly `@api` adds an entry point for accessing this BentoService. Each\n",
    "`api` will be translated into a REST endpoint when [deploying as API\n",
    "server](#serving-via-rest-api), or a CLI command when [running as a CLI\n",
    "tool](#use-as-cli-tool).\n",
    "\n",
    "Each API also requires a `Handler` for defining the expected input format. In\n",
    "this case, `DataframeHandler` will transform either an HTTP request or CLI\n",
    "command arguments into a pandas Dataframe and pass it down to the user defined\n",
    "API function. BentoML also supports `JsonHandler`, `ImageHandler` and\n",
    "`TensorHandler`.\n",
    "\n",
    "\n",
    "## Save BentoService to file archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) import the custom BentoService defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# 2) `pack` it with required artifacts\n",
    "svc = IrisClassifier.pack(model=clf)\n",
    "\n",
    "# 3) save BentoSerivce to a BentoML bundle\n",
    "saved_path = svc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_That's it._ You've just created your first BentoML Bundle. It's a versioned file archive, containing the BentoService you defined, including the trained model, dependencies and configurations etc, everything it needs to deploy the exact same service in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving via REST API\n",
    "\n",
    "Use the `bentoml serve` command to start a REST API server from a saved BentoML bundle. This allows application developers to easily intergrate with the ML model you are developing.\n",
    "\n",
    "Note that REST API serving **does not work in Google Colab**, due to unable to access Colab's VM. You may download the notebook and run it locally to play with the BentoML API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!bentoml serve {saved_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View documentations for REST APIs\n",
    "\n",
    "Open http://127.0.0.1:5000 to see more information about the REST APIs server in your\n",
    "browser.\n",
    "\n",
    "#### Send prediction request to REST API server\n",
    "\n",
    "*Run the following command in terminal to make a HTTP request to the API server*\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "Note you must ensure the pip and conda dependencies are available in your python\n",
    "environment when using `bentoml serve` command. More commonly we recommend using\n",
    "BentoML API server with Docker:\n",
    "\n",
    "## Run REST API server with Docker\n",
    "\n",
    "BentoML supports building Docker Image for your REST API model server.\n",
    "Simply use the BentoML bundle directory as the docker build context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {saved_path} && docker build -t iris-classifier ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `docker` is __note available in Google Colab__, download the notebook, ensure docker is installed and try it locally.\n",
    "\n",
    "Next, you can `docker push` the image to your choice of registry for deployment,\n",
    "or run it locally for development and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 5000:5000 iris-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved BentoService\n",
    "\n",
    "`bentoml.load` is the enssential API for loading a Bento into your\n",
    "python application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "import pandas as pd\n",
    "\n",
    "bento_svc = bentoml.load(saved_path)\n",
    "\n",
    "# Test loaded bentoml service:\n",
    "bento_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"pip install\" a BentoML bundle\n",
    "\n",
    "BentoML also supports distributing a BentoService as PyPI package, with the\n",
    "generated `setup.py` file. A Bento directory can be installed with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install {saved_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can import your ML service as a regular python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IrisClassifier\n",
    "\n",
    "installed_svc = IrisClassifier.load()\n",
    "installed_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bento PyPI package can also be uploaded to pypi.org\n",
    "as a public python package, or to your organization's private PyPI index for all\n",
    "developers in your organization to use:\n",
    "\n",
    "`cd {saved_path} & python setup.py sdist upload`\n",
    "\n",
    "*You will need a \".pypirc\" config file before doing this: https://docs.python.org/2/distutils/packageindex.html*\n",
    "\n",
    "\n",
    "# CLI access\n",
    "\n",
    "`pip install {saved_path}` also installs a CLI tool for accessing the BentoML service, print CLI help document with `--help`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!IrisClassifier --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing more information about this ML service with `info` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!IrisClassifier info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also print help and docs on individual commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!IrisClassifier predict --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each service API you defined in the BentoService will be exposed as a CLI command with the same name as the API function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!IrisClassifier predict --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML cli also supports reading input data from `csv` or `json` files, in either local machine or remote HTTP/S3 location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing test data to a csv file\n",
    "pd.DataFrame(iris.data).to_csv('iris_data.csv', index=False)\n",
    "\n",
    "# Invoke predict from command lien\n",
    "!IrisClassifier predict --input='./iris_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also use the `bentoml` cli to load and run a BentoML service archive without installing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml info {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml predict {saved_path} --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying to AWS Lambda\n",
    "\n",
    "AWS Lambda is a serverless computing platform provided by Amazon Web Services. BentoML service archive can be easily deployed to AWS Lambda as a REST API endpoint.\n",
    "\n",
    "In order to run this demo, make sure to configure your AWS credentials via either `aws configure` command or setting the environment variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AWS_ACCESS_KEY_ID=\n",
    "%env AWS_SECRET_ACCESS_KEY="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have [nodejs](https://nodejs.org) installed on your machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!node --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can deploy the BentML bundle you just created to AWS Lambda with one command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bentoml deployment create quick-start-guide-deployment \\\n",
    "    --bento=IrisClassifier:{svc.version} \\\n",
    "    --api-name=predict \\\n",
    "    --platform=aws-lambda \\\n",
    "    --region=us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the 'quick-starrt-guide-deployment' is the deployment name, you can reference the deployment by this name and query its status. For example, to get current deployment status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bentoml deployment describe quick-start-guide-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view your deployment configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml deployment get quick-start-guide-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to delete an active deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml deployment delete quick-start-guide-deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML by default stores the deployment metadata on the local machine. For team settings, we recommend hosting a shared BentoML Yatai server for your entire team to track all the BentoML bundle and deployments they've created in a central place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is what it looks like when using BentoML to create and deploy a machine learning service, all the way from training notebook to deployment in production. BentoML also supports many other Machine Learning frameworks, as well as many other deployment platforms. Take a look at other BentoML examples [here](https://github.com/bentoml/BentoML#examples)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
