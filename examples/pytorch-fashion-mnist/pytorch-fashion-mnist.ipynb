{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local bentoml repository\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "import bentoml\n",
    "\n",
    "print(bentoml.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional autoencoder\n",
    "\n",
    "source: https://github.com/baldassarreFe/zalando-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load a custom subclass of torchvision.datasets.MNIST that instead downloads the FashionMNIST dataset \n",
    "\n",
    "(waiting for [this commit](https://github.com/pytorch/vision/commit/eec5ba4405c8815bd1797619d9cc9276f81b76f4) be available in the pip version of PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "class FashionMNIST(MNIST):\n",
    "    \"\"\"`Fashion MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n",
    "    \"\"\"\n",
    "    urls = [\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz',\n",
    "    ]\n",
    "\n",
    "    input_shape = (28, 28)\n",
    "    num_classes = 10\n",
    "\n",
    "    labels = [\n",
    "        'T-shirt/top',\n",
    "        'Trouser',\n",
    "        'Pullover',\n",
    "        'Dress',\n",
    "        'Coat',\n",
    "        'Sandal',\n",
    "        'Shirt',\n",
    "        'Sneaker',\n",
    "        'Bag',\n",
    "        'Ankle boot'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, 'src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test set in batches of 1000.\n",
    "\n",
    "The `28x28` images are scaled up to `29x29` so that combining convolutions and transposed convolutions would not chop off pixels from the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "train_dataset = FashionMNIST(\n",
    "    './data', train=True, download=True, \n",
    "    transform=transforms.Compose([transforms.CenterCrop((29, 29)), transforms.ToTensor()]))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = FashionMNIST(\n",
    "    './data', train=False, download=True, \n",
    "    transform=transforms.Compose([transforms.CenterCrop((29, 29)), transforms.ToTensor()]))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised reconstruction\n",
    "\n",
    "Note that in this section we'll never use the image labels, the whole training is unsupervised.\n",
    "\n",
    "### Autoencoder\n",
    "The two components of the autoencoder are defined subclassing `nn.Module`, that gives more flexibility than `nn.Sequential`.\n",
    "\n",
    "#### Encoder\n",
    "A series of convolutions with `kernel_size=5` and `stride=2` is used to squeeze the images into a volume of 40x1x1, then a fully connected layer turns this vector in a vector of size `embedding_size`, that can be specified externally.\n",
    "\n",
    "#### Decoder\n",
    "The decoder takes up from where the encoder left, first transforming back the embedding of size `embedding_size` into a volume of size 40x1x1, then applying a series of Transposed Convolutions to yield an image of the same size of the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(20, 40, kernel_size=5, stride=2)\n",
    "        self.fully = nn.Linear(40, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1x29x29\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # 10x13x13\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # 20x5x5\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # 40x1x1\n",
    "        x = x.view(x.data.shape[0], 40)\n",
    "        # 40\n",
    "        x = self.fully(x)\n",
    "        # output_size\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fully = nn.Linear(input_size, 40)\n",
    "        self.conv1 = nn.ConvTranspose2d(40, 20, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.ConvTranspose2d(20, 10, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.ConvTranspose2d(10, 1, kernel_size=5, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fully(x)\n",
    "        x = x.view(x.data.shape[0], 40, 1, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.sigmoid(self.conv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "encoder = Encoder(embedding_size)\n",
    "decoder = Decoder(embedding_size)\n",
    "\n",
    "autoencoder = nn.Sequential(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised classification\n",
    "\n",
    "Once trained in an unsupervised fashion, the encoder module can be used to generate _fashion_ embeddings (see what I did here?), that can then be used to train a simple classifier on the original labels.\n",
    "\n",
    "### Model\n",
    "The weights of the encoder are freezed, so only the classifier will be trained.\n",
    "\n",
    "(later on, when the classifier starts performing decently, we could unfreeze them and do some fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    encoder, \n",
    "    nn.Linear(embedding_size, 15),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(15, len(FashionMNIST.labels)),\n",
    "    nn.LogSoftmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train()\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam([p for p in classifier.parameters() if p.requires_grad])\n",
    "epoch_loss = []\n",
    "\n",
    "for epoch in range(2):\n",
    "    batch_loss = []\n",
    "    for batch_num, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = Variable(data), Variable(targets)\n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(data)\n",
    "        loss = loss_fn(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.data.item() )\n",
    "    epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "    accuracy = accuracy_score(targets.data.numpy(), output.data.numpy().argmax(axis=1))\n",
    "    print('Epoch {}:\\tloss {:.4f}\\taccuracy {:.2%}'.format(epoch, epoch_loss[-1], accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "data, targets = next(test_loader.__iter__())\n",
    "outputs = classifier(Variable(data))\n",
    "log_probs, output_classes = outputs.max(dim=1)\n",
    "\n",
    "accuracy = accuracy_score(targets.numpy(), output_classes.data.numpy())\n",
    "print('Accuracy: {:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axex = plt.subplots(8, 8, figsize=(16, 16))\n",
    "\n",
    "zip_these = axex.ravel(), log_probs.data.exp(), output_classes.data, targets, data.numpy().squeeze()\n",
    "for ax, prob, output_class, target, img in zip(*zip_these):\n",
    "    ax.imshow(img, cmap='gray' if output_class == target else 'autumn')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('{} {:.1%}'.format(FashionMNIST.labels[output_class], prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BentoML  example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Fashion Model class and save it to the file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model class file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytorch_fashion_mnist.py\n",
    "\n",
    "from bentoml import BentoModel\n",
    "from bentoml.artifacts import PytorchModelArtifact\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class FashionMNISTModel(BentoModel):\n",
    "    \"\"\"\n",
    "    documentation strip\n",
    "    \"\"\"\n",
    "    def config(self, artifacts, env):\n",
    "        self.artifacts.add(PytorchModelArtifact('classifier'))\n",
    "        self.env.add_pip_dependencies(['torch', 'numpy', 'torchvision', 'scikit-learn'])\n",
    "    \n",
    "    def predict(self, tensor):\n",
    "        output_tensor = self.artifacts.classifier(Variable(tensor))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export versioned model to file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fashion_mnist import FashionMNISTModel\n",
    "new_model = FashionMNISTModel(classifier=classifier)\n",
    "saved_path = new_model.save('/tmp/bento')\n",
    "# saved_path = new_model.save('/tmp/bento', version=\"My_custom_version\")\n",
    "print(saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__* For demo purpurse, copy generated model to ./model folder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "shutil.rmtree('./model', ignore_errors=True)\n",
    "shutil.copytree(saved_path, './model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model use scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model in the current notebook and start a local REST API server for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "\n",
    "saved_model = bentoml.load('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=5, shuffle=True)\n",
    "testing_data, targets = next(data_loader.__iter__())\n",
    "\n",
    "saved_model.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bentoml.server import BentoModelApiServer\n",
    "\n",
    "server = BentoModelApiServer('fashion_mnist_server', saved_model, 5000) \n",
    "server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model as rest API server via CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "bentoml serve --model-path ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
