import os
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
import torch
from torch import Tensor, device, nn
from .configuration_utils import PretrainedConfig
from .file_utils import ModelOutput, PushToHubMixin, replace_return_docstrings
from .generation_utils import GenerationMixin

logger = ...
_init_weights = ...

@contextmanager
def no_init_weights(_enable=...): ...
def find_pruneable_heads_and_indices(
    heads: List[int], n_heads: int, head_size: int, already_pruned_heads: Set[int]
) -> Tuple[Set[int], torch.LongTensor]: ...
def get_parameter_device(
    parameter: Union[nn.Module, GenerationMixin, ModuleUtilsMixin]
): ...
def get_parameter_dtype(
    parameter: Union[nn.Module, GenerationMixin, ModuleUtilsMixin]
): ...

class ModuleUtilsMixin:
    def add_memory_hooks(self): ...
    def reset_memory_hooks_state(self): ...
    @property
    def device(self) -> device: ...
    @property
    def dtype(self) -> torch.dtype: ...
    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor: ...
    def get_extended_attention_mask(
        self, attention_mask: Tensor, input_shape: Tuple[int], device: device
    ) -> Tensor: ...
    def get_head_mask(
        self,
        head_mask: Optional[Tensor],
        num_hidden_layers: int,
        is_attention_chunked: bool = ...,
    ) -> Tensor: ...
    def num_parameters(
        self, only_trainable: bool = ..., exclude_embeddings: bool = ...
    ) -> int: ...
    def estimate_tokens(
        self, input_dict: Dict[str, Union[torch.Tensor, Any]]
    ) -> int: ...
    def floating_point_ops(
        self,
        input_dict: Dict[str, Union[torch.Tensor, Any]],
        exclude_embeddings: bool = ...,
    ) -> int: ...

class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin):
    config_class = ...
    base_model_prefix = ...
    _keys_to_ignore_on_load_missing = ...
    _keys_to_ignore_on_load_unexpected = ...
    _keys_to_ignore_on_save = ...
    is_parallelizable = ...
    @property
    def dummy_inputs(self) -> Dict[str, torch.Tensor]: ...
    def __init__(self, config: PretrainedConfig, *inputs, **kwargs) -> None: ...
    @property
    def base_model(self) -> nn.Module: ...
    def get_input_embeddings(self) -> nn.Module: ...
    def set_input_embeddings(self, value: nn.Module): ...
    def get_output_embeddings(self) -> nn.Module: ...
    def tie_weights(self): ...
    def resize_token_embeddings(
        self, new_num_tokens: Optional[int] = ...
    ) -> nn.Embedding: ...
    def init_weights(self): ...
    def prune_heads(self, heads_to_prune: Dict[int, List[int]]): ...
    def save_pretrained(
        self,
        save_directory: Union[str, os.PathLike[str]],
        save_config: bool = ...,
        state_dict: Optional[Dict[Any, Any]] = ...,
        save_function: Callable[..., Any] = ...,
        push_to_hub: bool = ...,
        **kwargs: Any
    ) -> None: ...
    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        **kwargs
    ): ...
    def retrieve_modules_from_names(self, names, add_prefix=..., remove_prefix=...): ...

class Conv1D(nn.Module):
    def __init__(self, nf, nx) -> None: ...
    def forward(self, x): ...

class PoolerStartLogits(nn.Module):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        p_mask: Optional[torch.FloatTensor] = ...,
    ) -> torch.FloatTensor: ...

class PoolerEndLogits(nn.Module):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        start_states: Optional[torch.FloatTensor] = ...,
        start_positions: Optional[torch.LongTensor] = ...,
        p_mask: Optional[torch.FloatTensor] = ...,
    ) -> torch.FloatTensor: ...

class PoolerAnswerClass(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        start_states: Optional[torch.FloatTensor] = ...,
        start_positions: Optional[torch.LongTensor] = ...,
        cls_index: Optional[torch.LongTensor] = ...,
    ) -> torch.FloatTensor: ...

@dataclass
class SquadHeadOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    start_top_log_probs: Optional[torch.FloatTensor] = ...
    start_top_index: Optional[torch.LongTensor] = ...
    end_top_log_probs: Optional[torch.FloatTensor] = ...
    end_top_index: Optional[torch.LongTensor] = ...
    cls_logits: Optional[torch.FloatTensor] = ...

class SQuADHead(nn.Module):
    def __init__(self, config) -> None: ...
    @replace_return_docstrings(
        output_type=SquadHeadOutput, config_class=PretrainedConfig
    )
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        start_positions: Optional[torch.LongTensor] = ...,
        end_positions: Optional[torch.LongTensor] = ...,
        cls_index: Optional[torch.LongTensor] = ...,
        is_impossible: Optional[torch.LongTensor] = ...,
        p_mask: Optional[torch.FloatTensor] = ...,
        return_dict: bool = ...,
    ) -> Union[SquadHeadOutput, Tuple[torch.FloatTensor]]: ...

class SequenceSummary(nn.Module):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        cls_index: Optional[torch.LongTensor] = ...,
    ) -> torch.FloatTensor: ...

def unwrap_model(model: nn.Module) -> nn.Module: ...
def prune_linear_layer(
    layer: nn.Linear, index: torch.LongTensor, dim: int = ...
) -> nn.Linear: ...
def prune_conv1d_layer(
    layer: Conv1D, index: torch.LongTensor, dim: int = ...
) -> Conv1D: ...
def prune_layer(
    layer: Union[nn.Linear, Conv1D], index: torch.LongTensor, dim: Optional[int] = ...
) -> Union[nn.Linear, Conv1D]: ...
def apply_chunking_to_forward(
    forward_fn: Callable[..., torch.Tensor],
    chunk_size: int,
    chunk_dim: int,
    *input_tensors
) -> torch.Tensor: ...
