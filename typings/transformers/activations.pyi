

import torch
from packaging import version

logger = ...
def gelu_new(x):
    """
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see
    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415
    """
    ...

if version.parse(torch.__version__) < version.parse("1.4"):
    gelu = ...
else:
    gelu = ...
def gelu_fast(x):
    ...

def quick_gelu(x):
    ...

if version.parse(torch.__version__) < version.parse("1.7"):
    silu = ...
else:
    silu = ...
if version.parse(torch.__version__) < version.parse("1.9"):
    mish = ...
else:
    mish = ...
def linear_act(x):
    ...

ACT2FN = ...
def get_activation(activation_string):
    ...

