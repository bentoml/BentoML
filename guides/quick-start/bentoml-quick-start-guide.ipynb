{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open-source framework for machine learning **model serving**, aiming to **bridge the gap between Data Science and DevOps**.\n",
    "\n",
    "Data Scientists can easily package their models trained with any ML framework using BentoMl and reproduce the model for serving in production. BentoML helps with managing packaged models in the BentoML format, and allows DevOps to deploy them as online API serving endpoints or offline batch inference jobs, on any cloud platform.\n",
    "\n",
    "This getting started guide demonstrates how to use BentoML to serve a sklearn modeld via a REST API server, and then containerize the model server for production deployment.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=guides&ea=bentoml-quick-start-guide&dt=bentoml-quick-start-guide)\n",
    "\n",
    "BentoML requires python 3.6 or above, install dependencies via `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyPI packages required in this guide, including BentoML\n",
    "!pip install -q bentoml 'scikit-learn>=0.23.2' 'pandas>=1.1.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's prepare a trained model for serving with BentoML. Train a classifier model on the [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Prediction Service with BentoML\n",
    "\n",
    "Model serving with BentoML comes after a model is trained. The first step is creating a\n",
    "prediction service class, which defines the models required and the inference APIs which\n",
    "contains the serving logic. Here is a minimal prediction service created for serving\n",
    "the iris classifier model trained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting iris_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iris_classifier.py\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "\n",
    "@env(auto_pip_dependencies=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(input=DataframeInput())\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        # Optional pre-processing, post-processing code goes here\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a prediction service that packages a scikit-learn model and provides\n",
    "an inference API that expects a `pandas.Dataframe` object as its input. BentoML also supports other API input \n",
    "data types including `JsonInput`, `ImageInput`, `FileInput` and \n",
    "[more](https://docs.bentoml.org/en/latest/api/adapters.html).\n",
    "\n",
    "\n",
    "In BentoML, **all inference APIs are suppose to accept a list of inputs and return a \n",
    "list of results**. In the case of `DataframeInput`, each row of the dataframe is mapping\n",
    "to one prediction request received from the client. BentoML will convert HTTP JSON \n",
    "requests into :code:`pandas.DataFrame` object before passing it to the user-defined \n",
    "inference API function.\n",
    " \n",
    "This design allows BentoML to group API requests into small batches while serving online\n",
    "traffic. Comparing to a regular flask or FastAPI based model server, this can increases\n",
    "the overall throughput of the API server by 10-100x depending on the workload.\n",
    "\n",
    "The following code packages the trained model with the prediction service class\n",
    "`IrisClassifier` defined above, and then saves the IrisClassifier instance to disk \n",
    "in the BentoML format for distribution and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-05 15:29:49,251] INFO - BentoService bundle 'IrisClassifier:20200905152948_51B712' saved to: /Users/chaoyu/bentoml/repository/IrisClassifier/20200905152948_51B712\n"
     ]
    }
   ],
   "source": [
    "# import the IrisClassifier class defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', clf)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML stores all packaged model files under the\n",
    "`~/bentoml/{service_name}/{service_version}` directory by default.\n",
    "The BentoML file format contains all the code, files, and configs required to \n",
    "deploy the model for serving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "\n",
    "\n",
    "To start a REST API model server with the `IrisClassifier` saved above, use \n",
    "the `bentoml serve` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-05 15:29:51,019] INFO - Getting latest version IrisClassifier:20200905152948_51B712\n",
      "[2020-09-05 15:29:51,019] INFO - Starting BentoML API server in development mode..\n",
      "[2020-09-05 15:29:51,466] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.6, but loading from BentoML version 0.8.6+26.g06d34096.dirty\n",
      " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [05/Sep/2020 15:29:53] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [05/Sep/2020 15:29:53] \"\u001b[37mGET /swagger_static/swagger-ui.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [05/Sep/2020 15:29:53] \"\u001b[37mGET /swagger_static/swagger-ui-bundle.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [05/Sep/2020 15:29:54] \"\u001b[37mGET /docs.json HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [05/Sep/2020 15:29:55] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [05/Sep/2020 15:30:08] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook from Google Colab, you can start the dev server with `--run-with-ngrok` option, to gain acccess to the API endpoint via a public endpoint managed by [ngrok](https://ngrok.com/): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-05 15:30:29,007] INFO - Getting latest version IrisClassifier:20200905152948_51B712\n",
      "[2020-09-05 15:30:29,008] INFO - Starting BentoML API server in development mode..\n",
      "[2020-09-05 15:30:29,498] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.6, but loading from BentoML version 0.8.6+26.g06d34096.dirty\n",
      " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "\u001b7\u001b[?47h\u001b[?1h\u001b=\u0002\u0007\u001b[H\u001b[2J\u001b[m\u001b[38;5;6m\u001b[48;5;16m\u001b[1m\u001b[1;1Hngrok\u001b[m\u001b[38;5;16m\u001b[48;5;16m \u001b[m\u001b[38;5;7m\u001b[48;5;16mby\u001b[m\u001b[38;5;16m\u001b[48;5;16m \u001b[m\u001b[38;5;6m\u001b[48;5;16m\u001b[1m@inconshreveable\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                       \u001b[m\u001b[38;5;7m\u001b[48;5;16m(Ctrl+C to quit)\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[2;1H                                                                                \u001b[m\u001b[38;5;6m\u001b[48;5;16m\u001b[3;1HSession Status                connecting\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                        \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[4;1HVersion                       2.3.35\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                            \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[5;1HRegion                        United States (us)\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[6;1HWeb Interface                 http://127.0.0.1:4040\u001b[m\u001b[38;5;16m\u001b[48;5;16m                             \u001b[7;1H                                                                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[8;1HConnections                   ttl     opn     rt1     rt5     p50     p90     \u001b[m\u001b[38;5;16m\u001b[48;5;16m  \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[9;1H                              0       0       0.00    0.00    0.00    0.00    \u001b[m\u001b[38;5;16m\u001b[48;5;16m  \u001b[10;1H                                                                                \u001b[11;1H                                                                                \u001b[12;1H                                                                                \u001b[13;1H                                                                                \u001b[14;1H                                                                                \u001b[15;1H                                                                                \u001b[16;1H                                                                                \u001b[17;1H                                                                                \u001b[18;1H                                                                                \u001b[19;1H                                                                                \u001b[20;1H                                                                                \u001b[21;1H                                                                                \u001b[22;1H                                                                                \u001b[23;1H                                                                                \u001b[24;1H                                                                                \u001b[m\u001b[38;5;2m\u001b[48;5;16m\u001b[3;1HSession Status                online\u001b[m\u001b[38;5;16m\u001b[48;5;16m    \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[4;1HS\u001b[4;3Hs\u001b[4;9HExpires\u001b[4;31H7 hours, 59 minutes\u001b[5;1HV\u001b[5;3Hrsion\u001b[5;31H2.3.35\u001b[m\u001b[38;5;16m\u001b[48;5;16m            \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[6;1HR\u001b[6;3Hgio\u001b[6;7H       \u001b[6;31HUnited States (us)\u001b[m\u001b[38;5;16m\u001b[48;5;16m   \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[7;1HWeb Interface                 http://127.0.0.1:4040\u001b[8;1HF\u001b[8;3Hrward\u001b[8;9Hng \u001b[8;31Hh\u001b[8;33Htps://3039a39ee95d.ngrok.io\u001b[8;61H-> http://localhost:\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[9;1H                                                                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[10;1HConnections                   ttl     opn     rt1     rt5     p50     p90     \u001b[11;1H                              0       0       0.00    0.00    0.00    0.00    \u001b[8;35H:/\u001b[8;38H3039a39e\u001b[8;47H95d.ngrok.io -> ht\u001b[8;66Hp:/\u001b[8;70Hlocalhost:5\u001b[9;1HForwarding                    https://3039a39ee95d.ngrok.io -> http://localhost:\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[10;1H                                                                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[11;1HConnections\u001b[11;31Httl\u001b[11;39Hopn\u001b[11;47Hrt1 \u001b[11;55Hrt5 \u001b[11;63Hp5\u001b[11;66H \u001b[11;71Hp9\u001b[11;74H \u001b[12;1H                              0       0       0.00    0.00    0.00    0.00    [2020-09-05 15:30:40,444] INFO -  * Running on http://3039a39ee95d.ngrok.io\n",
      "[2020-09-05 15:30:40,445] INFO -  * Traffic stats available on http://127.0.0.1:4040\n",
      "\u0002\u0007\u001b[m\u001b[H\u001b[2J\u001b[2J\u001b[?47l\u001b8\u001b[?1l\u001b>\u001b[?1006l\u001b[?1015l\u001b[?1002l\u001b[?1000l"
     ]
    }
   ],
   "source": [
    "!bentoml serve IrisClassifier:latest --run-with-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IrisClassifier` model is now served at `localhost:5000`. Use `curl` command to send\n",
    "a prediction request:\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "Or with `python` and [request library](https://requests.readthedocs.io/):\n",
    "```python\n",
    "import requests\n",
    "response = requests.post(\"http://127.0.0.1:5000/predict\", json=[[5.1, 3.5, 1.4, 0.2]])\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "Note that BentoML API server automatically converts the Dataframe JSON format into a\n",
    "`pandas.DataFrame` object before sending it to the user-defined inference API function.\n",
    "\n",
    "The BentoML API server also provides a simple web UI dashboard.\n",
    "Go to http://localhost:5000 in the browser and use the Web UI to send\n",
    "prediction request:\n",
    "\n",
    "![BentoML API Server Web UI Screenshot](https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/bento-api-server-web-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containerize model server with Docker\n",
    "\n",
    "\n",
    "\n",
    "One common way of distributing this model API server for production deployment, is via\n",
    "Docker containers. And BentoML provides a convenient way to do that.\n",
    "\n",
    "Note that `docker` is __note available in Google Colab__. You will need to download and run this notebook locally to try out this containerization with docker feature.\n",
    "\n",
    "If you already have docker configured, simply run the follow command to product a \n",
    "docker container serving the `IrisClassifier` prediction service created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-05 15:31:03,171] INFO - Getting latest version IrisClassifier:20200905152948_51B712\n",
      "\u001b[39mFound Bento: /Users/chaoyu/bentoml/repository/IrisClassifier/20200905152948_51B712\u001b[0m\n",
      "[2020-09-05 15:31:03,184] WARNING - Saved BentoService bundle version mismatch: loading BentoService bundle create with BentoML version 0.8.6, but loading from BentoML version 0.8.6+26.g06d34096.dirty\n",
      "\u001b[33mImage version not specified, using version parsed from BentoService: '20200905152948_51B712'\u001b[0m\n",
      "Building Docker image iris-classifier:20200905152948_51B712 from IrisClassifier:latest \n",
      "/\u001b[39mStep 1/15 : FROM bentoml/model-server:0.8.6\u001b[0m\n",
      "\u001b[39m ---> 71644b758bed\u001b[0m\n",
      "\u001b[39mStep 2/15 : COPY . /bento\u001b[0m\n",
      "\b\\\u001b[39m ---> b47a02c45188\u001b[0m\n",
      "\u001b[39mStep 3/15 : WORKDIR /bento\u001b[0m\n",
      "\u001b[39m ---> Running in dc614ee72f30\u001b[0m\n",
      "\b-\u001b[39m ---> 8164ff1227bc\u001b[0m\n",
      "\u001b[39mStep 4/15 : ARG PIP_INDEX_URL=https://pypi.python.org/simple/\u001b[0m\n",
      "\b/\u001b[39m ---> Running in a9319b0629fd\u001b[0m\n",
      "\b|\u001b[39m ---> 17d8d90cb745\u001b[0m\n",
      "\u001b[39mStep 5/15 : ARG PIP_TRUSTED_HOST=pypi.python.org\u001b[0m\n",
      "\u001b[39m ---> Running in 409ba339c63b\u001b[0m\n",
      "\b\\\u001b[39m ---> b07e7955a48f\u001b[0m\n",
      "\u001b[39mStep 6/15 : ENV PIP_INDEX_URL $PIP_INDEX_URL\u001b[0m\n",
      "\u001b[39m ---> Running in 188f144d231e\u001b[0m\n",
      "\b-\u001b[39m ---> 0002ded8f112\u001b[0m\n",
      "\u001b[39mStep 7/15 : ENV PIP_TRUSTED_HOST $PIP_TRUSTED_HOST\u001b[0m\n",
      "\b/\u001b[39m ---> Running in 1f33a8dcbb85\u001b[0m\n",
      "\b|\u001b[39m ---> e9eceedba3cc\u001b[0m\n",
      "\u001b[39mStep 8/15 : RUN chmod +x /bento/bentoml-init.sh\u001b[0m\n",
      "\u001b[39m ---> Running in 0fa04a6ab151\u001b[0m\n",
      "\b-\u001b[39m ---> b685151d2cb3\u001b[0m\n",
      "\u001b[39mStep 9/15 : RUN if [ -f /bento/bentoml-init.sh ]; then bash -c /bento/bentoml-init.sh; fi\u001b[0m\n",
      "\u001b[39m ---> Running in c7c7d3dd4505\u001b[0m\n",
      "\b\\\u001b[39m\u001b[91m+++ dirname /bento/bentoml-init.sh\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[39mUpdating conda base environment with environment.yml\u001b[0m\n",
      "\u001b[39m\u001b[91m++ cd /bento\n",
      "++ pwd -P\n",
      "+ SAVED_BUNDLE_PATH=/bento\n",
      "+ cd /bento\n",
      "+ '[' -f ./setup.sh ']'\n",
      "+ command -v conda\n",
      "+ echo 'Updating conda base environment with environment.yml'\n",
      "+ conda env update -n base -f ./environment.yml\n",
      "\u001b[0m\u001b[0m\n",
      "\b/\u001b[39mCollecting package metadata (repodata.json): ...working... \u001b[0m\n",
      "\b|\u001b[39mdone\n",
      "Solving environment: ...working... \u001b[0m\n",
      "\b-\u001b[39mdone\u001b[0m\n",
      "\u001b[39m\n",
      "Downloading and Extracting Packages\n",
      "readline-8.0         | 281 KB    |            |   0% \u001b[0m\n",
      "readline-8.0         | 281 KB    | 5          |   6% \u001b[0m\n",
      "readline-8.0         | 281 KB    | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "ca-certificates-2020 | 145 KB    |            |   0% \u001b[0m\n",
      "ca-certificates-2020 | 145 KB    | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "pip-20.2.2           | 1.1 MB    |            |   0% \u001b[0m\n",
      "pip-20.2.2           | 1.1 MB    | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "sqlite-3.33.0        | 1.4 MB    |            |   0% \u001b[0m\n",
      "sqlite-3.33.0        | 1.4 MB    | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "xz-5.2.5             | 343 KB    |            |   0% \u001b[0m\n",
      "xz-5.2.5             | 343 KB    | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "openssl-1.1.1g       | 2.1 MB    |            |   0% \u001b[0m\n",
      "openssl-1.1.1g       | 2.1 MB    | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "certifi-2020.6.20    | 151 KB    |            |   0% \u001b[0m\n",
      "certifi-2020.6.20    | 151 KB    | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "python_abi-3.7       | 4 KB      |            |   0% \u001b[0m\n",
      "python_abi-3.7       | 4 KB      | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "python-3.7.8         | 53.1 MB   |            |   0% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | 5          |   6% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | #5         |  15% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | ##5        |  25% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | ###4       |  35% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | ####4      |  44% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | #####1     |  52% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | ######1    |  61% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | #######    |  70% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | #######8   |  79% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | ########7  |  87% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | #########6 |  96% \u001b[0m\n",
      "python-3.7.8         | 53.1 MB   | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "tk-8.6.10            | 3.2 MB    |            |   0% \u001b[0m\n",
      "tk-8.6.10            | 3.2 MB    | #######6   |  76% \u001b[0m\n",
      "tk-8.6.10            | 3.2 MB    | ########## | 100% \u001b[0m\n",
      "\u001b[39m\n",
      "Preparing transaction: ...working... \u001b[0m\n",
      "\b\\\u001b[39mdone\u001b[0m\n",
      "\u001b[39mVerifying transaction: ...working... \u001b[0m\n",
      "\b-\u001b[39mdone\u001b[0m\n",
      "\u001b[39mExecuting transaction: ...working... \u001b[0m\n",
      "\b/\u001b[39mdone\u001b[0m\n",
      "\b|\u001b[39m\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 4.8.4\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[39m#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate base\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\u001b[0m\n",
      "\b\\\u001b[39m\u001b[91m+ pip install -r ./requirements.txt --no-cache-dir\n",
      "\u001b[0m\u001b[0m\n",
      "\b|\u001b[39mLooking in indexes: https://pypi.python.org/simple/\u001b[0m\n",
      "\b|\u001b[39mCollecting scikit-learn\u001b[0m\n",
      "\b/\u001b[39m  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\u001b[0m\n",
      "\b\\\u001b[39mCollecting pandas==1.1.1\u001b[0m\n",
      "\u001b[39m  Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\u001b[0m\n",
      "\b|\u001b[39mRequirement already satisfied: bentoml==0.8.6 in /opt/conda/lib/python3.7/site-packages (from -r ./requirements.txt (line 3)) (0.8.6)\u001b[0m\n",
      "\b\\\u001b[39mRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->-r ./requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\b-\u001b[39mCollecting joblib>=0.11\u001b[0m\n",
      "\b/\u001b[39m  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\u001b[0m\n",
      "\b-\u001b[39mCollecting scipy>=0.19.1\u001b[0m\n",
      "\u001b[39m  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\u001b[0m\n",
      "\b\\\u001b[39mCollecting threadpoolctl>=2.0.0\u001b[0m\n",
      "\u001b[39m  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\b/\u001b[39mCollecting pytz>=2017.2\u001b[0m\n",
      "\u001b[39m  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\u001b[0m\n",
      "\b|\u001b[39mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.1->-r ./requirements.txt (line 2)) (2.8.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.3.19)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: ruamel.yaml>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.15.87)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: python-json-logger in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.1.11)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: multidict in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (4.7.6)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (3.6.2)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (3.13.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (5.7.2)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (7.1.2)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.4.2)\u001b[0m\n",
      "\b\\\u001b[39mRequirement already satisfied: py-zipkin in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.20.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: gunicorn in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (20.0.4)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.8.7)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: configparser in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (5.0.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: cerberus in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.3.2)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: humanfriendly in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (8.2)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: flask in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.1.2)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39mRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (20.4)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: grpcio<=1.27.2 in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.27.2)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (2020.6.20)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: sqlalchemy-utils in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.36.8)\u001b[0m\n",
      "\b-\u001b[39mRequirement already satisfied: docker in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (4.3.1)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.14.48)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.8.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.1->-r ./requirements.txt (line 2)) (1.14.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.5.1)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->bentoml==0.8.6->-r ./requirements.txt (line 3)) (3.0.1)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->bentoml==0.8.6->-r ./requirements.txt (line 3)) (20.1.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->bentoml==0.8.6->-r ./requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.0->bentoml==0.8.6->-r ./requirements.txt (line 3)) (45.2.0.post20200210)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.25.8)\u001b[0m\n",
      "\b/\u001b[39mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->bentoml==0.8.6->-r ./requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic->bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.0.4)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.1.3)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: thriftpy2>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from py-zipkin->bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.4.11)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: Jinja2>=2.10.1 in /opt/conda/lib/python3.7/site-packages (from flask->bentoml==0.8.6->-r ./requirements.txt (line 3)) (2.11.2)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: Werkzeug>=0.15 in /opt/conda/lib/python3.7/site-packages (from flask->bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.0.1)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.7/site-packages (from flask->bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.1.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->bentoml==0.8.6->-r ./requirements.txt (line 3)) (2.4.7)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker->bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.57.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: botocore<1.18.0,>=1.17.48 in /opt/conda/lib/python3.7/site-packages (from boto3->bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.17.48)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3->bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.3.3)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.10.0)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->bentoml==0.8.6->-r ./requirements.txt (line 3)) (3.7.4.3)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->bentoml==0.8.6->-r ./requirements.txt (line 3)) (1.1.1)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: ply<4.0,>=3.4 in /opt/conda/lib/python3.7/site-packages (from thriftpy2>=0.4.0->py-zipkin->bentoml==0.8.6->-r ./requirements.txt (line 3)) (3.11)\u001b[0m\n",
      "\u001b[39mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.48->boto3->bentoml==0.8.6->-r ./requirements.txt (line 3)) (0.15.2)\u001b[0m\n",
      "\b|\u001b[39mInstalling collected packages: joblib, scipy, threadpoolctl, scikit-learn, pytz, pandas\u001b[0m\n",
      "\b/\u001b[39mSuccessfully installed joblib-0.16.0 pandas-1.1.1 pytz-2020.1 scikit-learn-0.23.2 scipy-1.5.2 threadpoolctl-2.1.0\u001b[0m\n",
      "\b-\u001b[39m\u001b[91m+ for filename in ./bundled_pip_dependencies/*.tar.gz\n",
      "+ '[' -e './bundled_pip_dependencies/*.tar.gz' ']'\n",
      "+ continue\n",
      "\u001b[0m\u001b[0m\n",
      "\b/\u001b[39m ---> 451aca54f5dd\u001b[0m\n",
      "\u001b[39mStep 10/15 : ENV PORT 5000\u001b[0m\n",
      "\b|\u001b[39m ---> Running in d28f253ac5fb\u001b[0m\n",
      "\b\\\u001b[39m ---> cf67c4916fae\u001b[0m\n",
      "\u001b[39mStep 11/15 : EXPOSE $PORT\u001b[0m\n",
      "\b-\u001b[39m ---> Running in 6beb7139bb0f\u001b[0m\n",
      "\u001b[39m ---> 71feabc403d0\u001b[0m\n",
      "\u001b[39mStep 12/15 : COPY docker-entrypoint.sh /usr/local/bin/\u001b[0m\n",
      "\b|\u001b[39m ---> 0f8b135e806f\u001b[0m\n",
      "\u001b[39mStep 13/15 : RUN chmod +x /usr/local/bin/docker-entrypoint.sh\u001b[0m\n",
      "\u001b[39m ---> Running in c4760d38cbfe\u001b[0m\n",
      "\b-\u001b[39m ---> d168022e0d88\u001b[0m\n",
      "\u001b[39mStep 14/15 : ENTRYPOINT [ \"docker-entrypoint.sh\" ]\u001b[0m\n",
      "\u001b[39m ---> Running in 1f94d2e56a15\u001b[0m\n",
      "\b/\u001b[39m ---> 83fd678db894\u001b[0m\n",
      "\u001b[39mStep 15/15 : CMD [\"bentoml\", \"serve-gunicorn\", \"/bento\"]\u001b[0m\n",
      "\b|\u001b[39m ---> Running in abff83fb5926\u001b[0m\n",
      "\u001b[39m ---> b984dff06f48\u001b[0m\n",
      "\u001b[39mSuccessfully built b984dff06f48\u001b[0m\n",
      "\b\\\u001b[39mSuccessfully tagged iris-classifier:20200905152948_51B712\u001b[0m\n",
      "\u001b[32mFinished building iris-classifier:20200905152948_51B712 from IrisClassifier:latest\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml containerize IrisClassifier:latest -t iris-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a container with the docker image built in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-05 22:33:28,601] INFO - Starting BentoML API server in production mode..\n",
      "[2020-09-05 22:33:28,998] INFO - Running micro batch service on :5000\n",
      "[2020-09-05 22:33:29 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-09-05 22:33:29 +0000] [11] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-09-05 22:33:29 +0000] [1] [INFO] Listening at: http://0.0.0.0:59059 (1)\n",
      "[2020-09-05 22:33:29 +0000] [11] [INFO] Listening at: http://0.0.0.0:5000 (11)\n",
      "[2020-09-05 22:33:29 +0000] [11] [INFO] Using worker: aiohttp.worker.GunicornWebWorker\n",
      "[2020-09-05 22:33:29 +0000] [1] [INFO] Using worker: sync\n",
      "[2020-09-05 22:33:29 +0000] [12] [INFO] Booting worker with pid: 12\n",
      "[2020-09-05 22:33:29 +0000] [13] [INFO] Booting worker with pid: 13\n",
      "[2020-09-05 22:33:29,061] INFO - Micro batch enabled for API `predict`\n",
      "[2020-09-05 22:33:29,062] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
      "^C\n",
      "[2020-09-05 22:33:31 +0000] [1] [INFO] Handling signal: int\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator SVC from version 0.23.0 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "[2020-09-05 22:33:31 +0000] [12] [INFO] Worker exiting (pid: 12)\n"
     ]
    }
   ],
   "source": [
    "!docker run -p 5000:5000 iris-classifier:latest --workers=1 --enable-microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This made it possible to deploy BentoML bundled ML models with platforms such as\n",
    "[Kubeflow](https://www.kubeflow.org/docs/components/serving/bentoml/),\n",
    "[Knative](https://knative.dev/community/samples/serving/machinelearning-python-bentoml/),\n",
    "[Kubernetes](https://docs.bentoml.org/en/latest/deployment/kubernetes.html), which\n",
    "provides advanced model deployment features such as auto-scaling, A/B testing,\n",
    "scale-to-zero, canary rollout and multi-armed bandit.\n",
    "\n",
    "\n",
    "## Load saved BentoService\n",
    "\n",
    "`bentoml.load` is the API for loading a BentoML packaged model in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "import pandas as pd\n",
    "\n",
    "bento_svc = bentoml.load(saved_path)\n",
    "\n",
    "# Test loaded bentoml service:\n",
    "bento_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BentoML format is pip-installable and can be directly distributed as a\n",
    "PyPI package for using in python applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The BentoService class name will become packaged name\n",
    "import IrisClassifier\n",
    "\n",
    "installed_svc = IrisClassifier.load()\n",
    "installed_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also allow users to upload their BentoService to pypi.org as public python package\n",
    "or to their organization's private PyPi index to share with other developers.\n",
    "\n",
    "`cd {saved_path} & python setup.py sdist upload`\n",
    "\n",
    "*You will have to configure \".pypirc\" file before uploading to pypi index.\n",
    "    You can find more information about distributing python package at:\n",
    "    https://docs.python.org/3.7/distributing/index.html#distributing-index*\n",
    "\n",
    "\n",
    "# Launch inference job from CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML cli supports loading and running a packaged model from CLI. With the `DataframeInput` adapter, the CLI command supports reading input Dataframe data from CLI argument or local `csv` or `json` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-05 15:35:08,177] INFO - Getting latest version IrisClassifier:20200905152948_51B712\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "!bentoml run IrisClassifier:latest predict --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-05 15:35:16,530] INFO - Getting latest version IrisClassifier:20200905152948_51B712\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "!bentoml run IrisClassifier:latest predict \\\n",
    "    --input-file=\"./iris_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Options\n",
    "\n",
    "If you are at a small team with limited engineering or DevOps resources, try out automated deployment with BentoML CLI, currently supporting AWS Lambda, AWS SageMaker, and Azure Functions:\n",
    "  - [AWS Lambda Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_lambda.html)\n",
    "  - [AWS SageMaker Deployment Guide](https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html)\n",
    "  - [Azure Functions Deployment Guide](https://docs.bentoml.org/en/latest/deployment/azure_functions.html)\n",
    "\n",
    "If the cloud platform you are working with is not on the list above, try out these step-by-step guide on manually deploying BentoML packaged model to cloud platforms:\n",
    "  - [AWS ECS Deployment](https://docs.bentoml.org/en/latest/deployment/aws_ecs.html)\n",
    "  - [Google Cloud Run Deployment](https://docs.bentoml.org/en/latest/deployment/google_cloud_run.html)\n",
    "  - [Azure container instance Deployment](https://docs.bentoml.org/en/latest/deployment/azure_container_instance.html)\n",
    "  - [Heroku Deployment](https://docs.bentoml.org/en/latest/deployment/heroku.html)\n",
    "\n",
    "Lastly, if you have a DevOps or ML Engineering team who's operating a Kubernetes or OpenShift cluster, use the following guides as references for implementating your deployment strategy:\n",
    "  - [Kubernetes Deployment](https://docs.bentoml.org/en/latest/deployment/kubernetes.html)\n",
    "  - [Knative Deployment](https://docs.bentoml.org/en/latest/deployment/knative.html)\n",
    "  - [Kubeflow Deployment](https://docs.bentoml.org/en/latest/deployment/kubeflow.html)\n",
    "  - [KFServing Deployment](https://docs.bentoml.org/en/latest/deployment/kfserving.html)\n",
    "  - [Clipper.ai Deployment Guide](https://docs.bentoml.org/en/latest/deployment/clipper.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is what it looks like when using BentoML to serve and deploy a model in the cloud. BentoML also supports [many other Machine Learning frameworks](https://docs.bentoml.org/en/latest/examples.html) besides Scikit-learn. The [BentoML core concepts](https://docs.bentoml.org/en/latest/concepts.html) doc is recommended for anyone looking to get a deeper understanding of BentoML.\n",
    "\n",
    "Join the [BentoML Slack](https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg) to follow the latest development updates and roadmap discussions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
