# Copyright 2019 Atalaya Tech, Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This is the default configuration for BentoML. When bentoml is imported, it looks
# for a configuration file at "$BENTOML_HOME/bentoml.cfg".


[core]
debug = false
usage_tracking = true
bentoml_deploy_version = {LAST_PYPI_RELEASE_VERSION}
default_docker_base_image =

[instrument]
default_namespace = BENTOML
prometheus_multiproc_dir = {BENTOML_HOME}/prometheus_multiproc_dir

[logging]
level = INFO
log_format = [%%(asctime)s] %%(levelname)s - %%(message)s
dev_log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s

# the base file directory where bentoml store all its log files
base_log_dir = {BENTOML_HOME}/logs/

log_request_image_files = True

prediction_log_filename = prediction.log
prediction_log_json_format = "%%(service_name)s %%(service_version)s %%(api)s %%(request_id)s %%(task)s %%(result)s %%(asctime)s"

feedback_log_filename = feedback.log
feedback_log_json_format = "%%(service_name)s %%(service_version)s %%(request_id)s %%(asctime)s"

yatai_web_server_log_filename = yatai_web_server.log


[tracing]
# example: http://127.0.0.1:9411/api/v1/spans
zipkin_api_url =

[yatai_service]
# URL of remote YataiService gRPC server endpoint to use
url =
s3_signature_version = s3v4

# YataiService server configs
repository_base_url = {BENTOML_HOME}/repository/
db_url = sqlite:///{BENTOML_HOME}/storage.db
s3_endpoint_url =
default_namespace = dev
client_certificate_file =

[apiserver]
default_port = 5000
enable_metrics = true
enable_feedback = true
default_timeout = 60
default_max_request_size = 20971520
default_image_input_accept_file_extensions = .jpg,.png,.jpeg,.tiff,.webp,.bmp

# Set to a positive integer to take effect, otherwise will fallback to a
# runtime calculated value based on cpu cores
# see `bentoml.server.utils.get_gunicorn_num_of_workers` for details
default_gunicorn_workers_count = -1
batch_request_header = Bentoml-Is-Batch-Request


[marshal_server]
marshal_request_header_flag = BentoML-Is-Merged-Request
default_max_latency = 10000
default_max_batch_size = 2000


[cli]
#
#

[yatai]
bento_uri_default_expiration = 3000

[tensorflow]
#
#

[pytorch]
#
#

