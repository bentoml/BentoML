{% if metadata.ephemeral %}
{# This will be part of ./generated #}
## GENERATED DIRECTORY - DO NOT EDIT

These files are maintained by `manager.py`, which generates Dockerfile from `templates`.
Refers to [main README](../README.md) for more information.

{% else %}
# Unified Model Serving Framework  [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=BentoML:%20The%20Unified%20Model%20Serving%20Framework%20&url=https://github.com/bentoml&via=bentomlai&hashtags=mlops,bentoml)

BentoML is an open platform that simplifies ML model deployment and enables you to serve your models at production scale in minutes

üëâ [Pop into our Slack community!](https://l.linklyhq.com/l/ktOu) We're happy to help with any issue you face or even just to meet you and hear what you're working on :)

[![pypi_status](https://img.shields.io/pypi/v/bentoml.svg)](https://pypi.org/project/BentoML)
[![downloads](https://pepy.tech/badge/bentoml)](https://pepy.tech/project/bentoml)
[![actions_status](https://github.com/bentoml/bentoml/workflows/BentoML-CI/badge.svg)](https://github.com/bentoml/bentoml/actions)
[![documentation_status](https://readthedocs.org/projects/bentoml/badge/?version=latest)](https://docs.bentoml.org/)
[![join_slack](https://badgen.net/badge/Join/BentoML%20Slack/cyan?icon=slack)](https://join.slack.bentoml.org)


## Why BentoML ##

- The easiest way to turn your ML models into production-ready API endpoints.
- High performance model serving, all in Python.
- Standardized model packaging and ML service definition to streamline deployment.
- Support all major machine-learning training [frameworks](https://docs.bentoml.org/en/latest/frameworks/index.html).
- Deploy and operate ML serving workload at scale on Kubernetes via [Yatai](https://github.com/bentoml/yatai).

## Getting Started ##

- [Quickstart guide](https://docs.bentoml.org/en/latest/quickstart.html) will show you a simple example of using BentoML in action. In under 10 minutes, you'll be able to serve your ML model over an HTTP API endpoint, and build a docker image that is ready to be deployed in production.
- [Main concepts](https://docs.bentoml.org/en/latest/concepts/index.html) will give a comprehensive tour of BentoML's components and introduce you to its philosophy. After reading, you will see what drives BentoML's design, and know what `bento` and `runner` stands for.
- [ML Frameworks](https://docs.bentoml.org/en/latest/frameworks/index.html) lays out best practices and example usages by the ML framework used for training models.
- [Advanced Guides](https://docs.bentoml.org/en/latest/guides/index.html) showcases advanced features in BentoML, including GPU support, inference graph, monitoring, and customizing docker environment etc.
- Check out other projects from the [BentoML team](https://github.com/bentoml):
  - [ü¶ÑÔ∏è Yatai](https://github.com/bentoml/yatai): Run BentoML workflow at scale on Kubernetes
  - [üöÄ bentoctl](https://github.com/bentoml/bentoctl): Fast model deployment with BentoML on cloud platforms


## BentoServer base images

There are three type of BentoServer docker base image:

| Image Type | Description                                | Supported OS                                          | Usage                             |
|------------|--------------------------------------------|-------------------------------------------------------|-----------------------------------|
| `runtime`  | contains latest BentoML releases from PyPI | `debian`, `ubi{7,8}`, `amazonlinux2`, `alpine3.14`    | production ready                  |
| `cudnn`    | runtime + support for CUDA-enabled GPU     | `debian`, `ubi{7,8}`                                  | production ready with GPU support |
| `devel`    | nightly build from development branch      | `debian`, `ubi{7,8}`                                  | for development use only          |

* Note: currently there's no nightly devel image with GPU support.

The final docker image tags will have the following format:

```markdown
<release_type>-<python_version>-<distros>-<suffix>
   ‚îÇ             ‚îÇ                ‚îÇ        ‚îÇ
   ‚îÇ             ‚îÇ                ‚îÇ        ‚îî‚îÄ> additional suffix, differentiate runtime and cudnn releases
   ‚îÇ             ‚îÇ                ‚îî‚îÄ> formatted <dist><dist_version>, e.g: ami2, debian, ubi7
   ‚îÇ             ‚îî‚îÄ> Supported Python version: python3.7 | python3.8 | python3.9
   ‚îî‚îÄ>  Release type: devel or official BentoML release (e.g: 1.0.0)
```

Example image tags:
- `bento-server:devel-python3.7-debian`
- `bento-server:1.0.0-python3.8-ubi8-cudnn`
- `bento-server:1.0.0-python3.7-ami2-runtime`

## Latest tags for `{{ metadata.bentoml_package }} {{ metadata.bentoml_release_version }}`

{% set formatted_arch = [] %}
{% for distros, release_context in metadata.release_info.items() %}
    {% set tags_path = release_context['image_tag'] %}
    {% set supported_arch = release_context['supported_arch'] %}
    {% set distro_string = distros.strip('0123456789') %}
    {% set distro_string_len = distro_string | length %}
    {% set distro_version = distros[distro_string_len:] %}
    {% if "ubi" in distro_string %}
        {% set distro_string = distro_string.upper() %}
    {% endif %}
    {% if "alpine" in distros %}
        {% set distro_version = distro_string[-2:] + distro_version %}
        {% set distro_string = 'alpine' %}
    {% endif %}
    {# distro string {{ distro_string }} with version {{ distro_version }} #}
    {% for arch in supported_arch %}
        {% set formatted = "_{}_".format(arch) %}
        {% if formatted not in formatted_arch %}
            {% do formatted_arch.append(formatted) %}
        {% endif %}
    {% endfor %}
    {% print"\n### " %}{% print distro_string %} {% print distro_version %} [{{ formatted_arch | join(", ") }}]{% print "\n\n" %}
    {% for (tag, path) in tags_path %}
        {% if metadata.bentoml_package in tag %}
        {% set tagname = tag.split(':')[1] %}
        {% print "- [`" %}{% print tagname %}{% print "`](https://github.com/bentoml/BentoML/tree/main/docker/" %}{% print path %}{% print ")\n" %}
        {% endif %}
    {% endfor %}
{% endfor %}
{% endif %}
{# We might want to include previous images here as well #}
