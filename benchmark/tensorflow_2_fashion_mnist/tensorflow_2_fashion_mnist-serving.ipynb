{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Basic classification: Classify images of clothing\n",
    "A tensorflow serving style service example using BentoML\n",
    "\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=tensorflow&ea=tensorflow_2_fashion_mnist&dt=tensorflow_2_fashion_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# add venv PATH to shell command PATH\n",
    "import sys, os\n",
    "if sys.base_prefix not in os.environ['PATH']:\n",
    "    os.environ['PATH'] = f\"{sys.base_prefix}/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzLKpmZICaWN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import io\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(_train_images, train_labels), (_test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "train_images = _train_images / 255.0\n",
    "test_images = _test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMnist(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FashionMnist, self).__init__()\n",
    "        self.cnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def image_bytes2tensor(inputs):\n",
    "        inputs = tf.map_fn(lambda i: tf.io.decode_png(i, channels=1), inputs, dtype=tf.uint8)\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "        inputs = (255.0 - inputs) / 255.0\n",
    "        inputs = tf.reshape(inputs, [-1, 28, 28])\n",
    "        return inputs\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=(None,), dtype=tf.string)])\n",
    "    def predict_image(self, inputs):\n",
    "        inputs = self.image_bytes2tensor(inputs)\n",
    "        return self(inputs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.cnn(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick up a test image\n",
    "d_test_img = _test_images[0]\n",
    "print(class_names[test_labels[0]])\n",
    "\n",
    "plt.imshow(255.0 - d_test_img, cmap='gray')\n",
    "plt.imsave(\"test.png\", 255.0 - d_test_img, cmap='gray')\n",
    "\n",
    "# read bytes\n",
    "with open(\"test.png\", \"rb\") as f:\n",
    "    img_bytes = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ODch-OFCaW4"
   },
   "source": [
    "## train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lhan11blCaW7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.5038 - accuracy: 0.8234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b182656a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FashionMnist()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ankle boot', 'Ankle boot', 'Ankle boot']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict_image(tf.constant([img_bytes] * 3))\n",
    "klass = tf.argmax(predict, axis=1)\n",
    "[class_names[k] for k in klass]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFc2HbEVCaXd"
   },
   "source": [
    "And the model predicts a label as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define & save BentoService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tensorflow_fashion_mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tensorflow_fashion_mnist.py\n",
    "\n",
    "import base64\n",
    "import bentoml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from bentoml.artifact import (\n",
    "    TensorflowSavedModelArtifact,\n",
    ")\n",
    "from bentoml.handlers import TensorflowTensorHandler, ClipperStringsHandler\n",
    "\n",
    "\n",
    "FASHION_MNIST_CLASSES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "@bentoml.env(pip_dependencies=['tensorflow', 'numpy', 'pillow'])\n",
    "@bentoml.artifacts([TensorflowSavedModelArtifact('model')])\n",
    "class FashionMnistTensorflow(bentoml.BentoService):\n",
    "\n",
    "    @bentoml.api(TensorflowTensorHandler)\n",
    "    def predict(self, inputs):\n",
    "        outputs = self.artifacts.model.predict_image(inputs)\n",
    "        output_classes = tf.math.argmax(outputs, axis=1)\n",
    "        return [FASHION_MNIST_CLASSES[o] for o in output_classes]\n",
    "\n",
    "    @bentoml.api(ClipperStringsHandler)\n",
    "    def predict_clipper(self, strings):\n",
    "        _bytes = [base64.b64decode(i) for i in strings]\n",
    "        inputs = tf.constant(_bytes, dtype=tf.string)\n",
    "        outputs = self.artifacts.model.predict_image(inputs)\n",
    "        output_classes = tf.math.argmax(outputs, axis=1)\n",
    "        return [FASHION_MNIST_CLASSES[o] for o in output_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-02-20 21:27:55,964] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-02-20 21:27:56,156] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /tmp/bentoml-temp-z44sq20a/FashionMnistTensorflow/artifacts/model_saved_model/assets\n",
      "[2020-02-20 21:28:04,969] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "running sdist\n",
      "running egg_info\n",
      "writing BentoML.egg-info/PKG-INFO\n",
      "writing dependency_links to BentoML.egg-info/dependency_links.txt\n",
      "writing entry points to BentoML.egg-info/entry_points.txt\n",
      "writing requirements to BentoML.egg-info/requires.txt\n",
      "writing top-level names to BentoML.egg-info/top_level.txt\n",
      "reading manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "no previously-included directories found matching 'examples'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'docs'\n",
      "no previously-included directories found matching 'scripts'\n",
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "running check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: check: missing meta-data: if 'author' supplied, 'author_email' must be supplied too\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating BentoML-0.5.2+105.ga67aa5e.dirty\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/BentoML.egg-info\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/bundler\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/clipper\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/configuration\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/aws_lambda\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/sagemaker\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/marshal\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/migrations\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/migrations/versions\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/proto\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/repository\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server/static\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils/validator\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai\n",
      "creating BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai/client\n",
      "copying files to BentoML-0.5.2+105.ga67aa5e.dirty...\n",
      "copying LICENSE -> BentoML-0.5.2+105.ga67aa5e.dirty\n",
      "copying MANIFEST.in -> BentoML-0.5.2+105.ga67aa5e.dirty\n",
      "copying README.md -> BentoML-0.5.2+105.ga67aa5e.dirty\n",
      "copying pyproject.toml -> BentoML-0.5.2+105.ga67aa5e.dirty\n",
      "copying setup.cfg -> BentoML-0.5.2+105.ga67aa5e.dirty\n",
      "copying setup.py -> BentoML-0.5.2+105.ga67aa5e.dirty\n",
      "copying versioneer.py -> BentoML-0.5.2+105.ga67aa5e.dirty\n",
      "copying BentoML.egg-info/PKG-INFO -> BentoML-0.5.2+105.ga67aa5e.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/SOURCES.txt -> BentoML-0.5.2+105.ga67aa5e.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/dependency_links.txt -> BentoML-0.5.2+105.ga67aa5e.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/entry_points.txt -> BentoML-0.5.2+105.ga67aa5e.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/requires.txt -> BentoML-0.5.2+105.ga67aa5e.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/top_level.txt -> BentoML-0.5.2+105.ga67aa5e.dirty/BentoML.egg-info\n",
      "copying bentoml/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml\n",
      "copying bentoml/_version.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml\n",
      "copying bentoml/alembic.ini -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml\n",
      "copying bentoml/db.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml\n",
      "copying bentoml/exceptions.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml\n",
      "copying bentoml/service.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml\n",
      "copying bentoml/service_env.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml\n",
      "copying bentoml/artifact/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/fastai_model_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/h2o_model_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/keras_model_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/lightgbm_model_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/pickle_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/pytorch_model_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/sklearn_model_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/text_file_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/tf_savedmodel_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/xgboost_model_artifact.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/artifact\n",
      "copying bentoml/bundler/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/bundler.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/config.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/loader.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/py_module_utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/templates.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/bundler\n",
      "copying bentoml/cli/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "copying bentoml/cli/aws_lambda.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "copying bentoml/cli/aws_sagemaker.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "copying bentoml/cli/bento.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "copying bentoml/cli/click_utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "copying bentoml/cli/config.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "copying bentoml/cli/deployment.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "copying bentoml/cli/utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/cli\n",
      "copying bentoml/clipper/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/clipper\n",
      "copying bentoml/configuration/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/configparser.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/default_bentoml.cfg -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/configuration\n",
      "copying bentoml/deployment/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/operator.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/store.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/aws_lambda/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/download_extra_resources.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/lambda_app.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/sagemaker/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_nginx.conf -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_serve.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_wsgi.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/handlers/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/base_handlers.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/clipper_handler.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/dataframe_handler.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/fastai_image_handler.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/image_handler.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/json_handler.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/pytorch_tensor_handler.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/tensorflow_tensor_handler.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/handlers\n",
      "copying bentoml/marshal/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/marshal\n",
      "copying bentoml/marshal/marshal.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/marshal\n",
      "copying bentoml/marshal/utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/marshal\n",
      "copying bentoml/migrations/README -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/env.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/script.py.mako -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/versions/a6b00ae45279_add_last_updated_at_for_deployments.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/migrations/versions\n",
      "copying bentoml/proto/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/proto\n",
      "copying bentoml/proto/deployment_pb2.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/proto\n",
      "copying bentoml/proto/repository_pb2.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/proto\n",
      "copying bentoml/proto/status_pb2.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2_grpc.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/proto\n",
      "copying bentoml/repository/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/repository\n",
      "copying bentoml/repository/metadata_store.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/repository\n",
      "copying bentoml/server/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "copying bentoml/server/bento_api_server.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "copying bentoml/server/bento_sagemaker_server.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "copying bentoml/server/gunicorn_config.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "copying bentoml/server/gunicorn_server.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "copying bentoml/server/marshal_server.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "copying bentoml/server/middlewares.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "copying bentoml/server/utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server\n",
      "copying bentoml/server/static/swagger-ui-bundle.js -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server/static\n",
      "copying bentoml/server/static/swagger-ui.css -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/server/static\n",
      "copying bentoml/utils/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "copying bentoml/utils/cloudpickle.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "copying bentoml/utils/hybirdmethod.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "copying bentoml/utils/log.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "copying bentoml/utils/s3.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "copying bentoml/utils/tempdir.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "copying bentoml/utils/trace.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "copying bentoml/utils/usage_stats.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils\n",
      "copying bentoml/utils/validator/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/utils/validator\n",
      "copying bentoml/yatai/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/deployment_utils.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/status.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/yatai_service_impl.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/client/__init__.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/bento_repository_api.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/deployment_api.py -> BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/yatai/client\n",
      "Writing BentoML-0.5.2+105.ga67aa5e.dirty/setup.cfg\n",
      "UPDATING BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/_version.py\n",
      "set BentoML-0.5.2+105.ga67aa5e.dirty/bentoml/_version.py to '0.5.2+105.ga67aa5e.dirty'\n",
      "Creating tar archive\n",
      "removing 'BentoML-0.5.2+105.ga67aa5e.dirty' (and everything under it)\n",
      "[2020-02-20 21:28:05,896] INFO - BentoService bundle 'FashionMnistTensorflow:20200220212756_EBDB17' created at: /tmp/bentoml-temp-z44sq20a\n",
      "[2020-02-20 21:28:05,899] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-02-20 21:28:05,913] WARNING - Saved BentoService bundle version mismatch: loading BentoServie bundle create with BentoML version 0.5.2,  but loading from BentoML version 0.5.2+105.ga67aa5e.dirty\n",
      "[2020-02-20 21:28:05,949] INFO - BentoService bundle 'FashionMnistTensorflow:20200220212756_EBDB17' created at: /home/bentoml/bentoml/repository/FashionMnistTensorflow/20200220212756_EBDB17\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_fashion_mnist import FashionMnistTensorflow\n",
    "\n",
    "bento_svc = FashionMnistTensorflow()\n",
    "bento_svc.pack(\"model\", model)\n",
    "saved_path = bento_svc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_path = f\"{saved_path}/FashionMnistTensorflow/artifacts\"\n",
    "!cp -r {tf_model_path}/model_saved_model {tf_model_path}/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & Run TF-Serving Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/test-model/1/assets\n",
      "2020-02-20 22:06:10.831536: I tensorflow_serving/model_servers/server.cc:85] Building single TensorFlow model file config:  model_name: fashion_model model_base_path: /tmp/test-model/\n",
      "2020-02-20 22:06:10.831673: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "2020-02-20 22:06:10.831685: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: fashion_model\n",
      "2020-02-20 22:06:10.932326: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: fashion_model version: 1}\n",
      "2020-02-20 22:06:10.932384: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: fashion_model version: 1}\n",
      "2020-02-20 22:06:10.932413: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: fashion_model version: 1}\n",
      "2020-02-20 22:06:10.932456: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/test-model/1\n",
      "2020-02-20 22:06:10.937172: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "2020-02-20 22:06:10.942403: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-02-20 22:06:10.981940: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "2020-02-20 22:06:11.006296: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /tmp/test-model/1\n",
      "2020-02-20 22:06:11.013322: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 80859 microseconds.\n",
      "2020-02-20 22:06:11.013802: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /tmp/test-model/1/assets.extra/tf_serving_warmup_requests\n",
      "2020-02-20 22:06:11.014156: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: fashion_model version: 1}\n",
      "2020-02-20 22:06:11.015606: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "2020-02-20 22:06:11.016455: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8890 ...\n",
      "[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n"
     ]
    }
   ],
   "source": [
    "TMP_MODEL_DIR = \"/tmp/test-model/\"\n",
    "TMP_MODEL_SAVE_DIR = \"/tmp/test-model/1\"\n",
    "SERVE_HOST = \"http://localhost:8890\"\n",
    "SERVE_PORT = 8890\n",
    "MODEL_NAME = \"fashion_model\"\n",
    "tf.saved_model.save(model, TMP_MODEL_SAVE_DIR, signatures=model.predict_image)\n",
    "!tensorflow_model_server --rest_api_port={SERVE_PORT} --model_name={MODEL_NAME} --model_base_path={TMP_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run in Docker with 1 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull tensorflow/serving\n",
    "!docker run -t --rm -p {SERVE_PORT}:8501 -v {TMP_MODEL_DIR}:/models/{MODEL_NAME} -e MODEL_NAME={MODEL_NAME} --cpus 1 tensorflow/serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\"query_id\":1,\"output\":\"Ankle boot\",\"default\":false}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "\n",
    "with open(\"test.png\", \"rb\") as f:\n",
    "    img_bytes = f.read()\n",
    "img_b64 = base64.b64encode(img_bytes).decode()\n",
    "\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "data = json.dumps(\n",
    "       {\"instances\": [{\"b64\": img_b64}]}\n",
    ")\n",
    "\n",
    "json_response = requests.post(f\"http://127.0.0.1:{SERVE_PORT}/v1/models/fashion_model:predict\", data=data, headers=headers)\n",
    "print(json_response)\n",
    "print(json_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[\"Ankle boot\", \"Ankle boot\", \"Ankle boot\"]\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "\n",
    "with open(\"test.png\", \"rb\") as f:\n",
    "    img_bytes = f.read()\n",
    "img_b64 = base64.b64encode(img_bytes).decode()\n",
    "\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "data = json.dumps(\n",
    "       {\"instances\": [{\"b64\": img_b64}]}\n",
    ")\n",
    "\n",
    "json_response = requests.post(\n",
    "    f\"http://127.0.0.1:{SERVE_PORT}/v1/models/fashion_model:predict\",\n",
    "    data=data, headers=headers)\n",
    "print(json_response)\n",
    "print(json_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark with locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark_serving.py\n",
    "from locust import HttpLocust, TaskSet, task, constant\n",
    "from functools import lru_cache\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def data_producer():\n",
    "\n",
    "    with open(\"test.png\", \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    img_b64 = base64.b64encode(img_bytes).decode()\n",
    "\n",
    "    def _gen_data(size=3):\n",
    "        headers = {\"content-type\": \"application/json\"}\n",
    "        data = json.dumps(\n",
    "               {\"instances\": [{\"b64\": img_b64}] * size}\n",
    "        )\n",
    "        return headers, data\n",
    "\n",
    "    return _gen_data\n",
    "\n",
    "\n",
    "class WebsiteTasks(TaskSet):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data():\n",
    "        headers, data = data_producer()(3)\n",
    "        return headers, data\n",
    "        \n",
    "    @task\n",
    "    def index(self):\n",
    "        headers, data = self.get_data()\n",
    "        self.client.post(\"/v1/models/fashion_model:predict\", data, headers=headers)\n",
    "\n",
    "class WebsiteUser(HttpLocust):\n",
    "    task_set = WebsiteTasks\n",
    "    wait_time = constant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-02-20 22:06:34,066] beta/INFO/locust.main: Starting web monitor at http://*:8089\n",
      "[2020-02-20 22:06:34,066] beta/INFO/locust.main: Starting Locust 0.14.4\n",
      "[2020-02-20 22:06:52,249] beta/INFO/locust.runners: Hatching and swarming 1000 users at the rate 20 users/s (0 users already running)...\n",
      "[2020-02-20 22:07:49,144] beta/WARNING/root: Loadgen CPU usage above 90%! This may constrain your throughput and may even give inconsistent response time measurements! See https://docs.locust.io/en/stable/running-locust-distributed.html for how to distribute the load over multiple CPU cores or machines\n",
      "[2020-02-20 22:07:51,659] beta/INFO/locust.runners: All locusts hatched: WebsiteUser: 1000 (0 already running)\n"
     ]
    }
   ],
   "source": [
    "!locust -f benchmark_serving.py -H http://127.0.0.1:{SERVE_PORT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bentoml-dev-py36",
   "language": "python",
   "name": "bentoml-dev-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
