{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bozhaoyu/src/bento/examples/deploy-with-sagemaker/../../bentoml/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Import local bentoml repository\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "import bentoml\n",
    "\n",
    "print(bentoml.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Scikit-learn\n",
    "\n",
    "Based on https://github.com/crawles/sentiment_analysis_twitter_model/blob/master/build-sentiment-classifier.ipynb\n",
    "\n",
    "Using dataset from http://help.sentiment140.com/for-students/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  trainingandtestdata.zip\n",
      "  inflating: testdata.manual.2009.06.14.csv  \n",
      "  inflating: training.1600000.processed.noemoticon.csv  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -f ./trainingandtestdata.zip ]; then\n",
    "    wget -q http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "    unzip -n trainingandtestdata.zip\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['polarity', 'tweetid', 'date', 'query_name', 'user', 'text']\n",
    "dftrain = pd.read_csv('training.1600000.processed.noemoticon.csv',\n",
    "                      header = None,\n",
    "                      encoding ='ISO-8859-1')\n",
    "dftest = pd.read_csv('testdata.manual.2009.06.14.csv',\n",
    "                     header = None,\n",
    "                     encoding ='ISO-8859-1')\n",
    "dftrain.columns = columns\n",
    "dftest.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/dev-py3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('count_vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=100,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_lr = Pipeline([\n",
    "                         ('count_vect', CountVectorizer(min_df = 100,\n",
    "                                                        ngram_range = (1,1),\n",
    "                                                        stop_words = 'english')), \n",
    "                         ('lr', LogisticRegression())])\n",
    "sentiment_lr.fit(dftrain.text, dftrain.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83       177\n",
      "           4       0.82      0.86      0.84       182\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       359\n",
      "   macro avg       0.83      0.83      0.83       359\n",
      "weighted avg       0.83      0.83      0.83       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest, ytest = dftest.text[dftest.polarity!=2], dftest.polarity[dftest.polarity!=2]\n",
    "print(classification_report(ytest,sentiment_lr.predict(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_lr.predict([Xtest[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export model for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sentiment_lr_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentiment_lr_model.py\n",
    "import pandas as pd\n",
    "import bentoml\n",
    "from bentoml.artifact import PickleArtifact\n",
    "from bentoml.handlers import DataframeHandler\n",
    "\n",
    "@bentoml.artifacts([PickleArtifact('sentiment_lr')])\n",
    "@bentoml.env(conda_dependencies=[\"scikit-learn\", \"pandas\"])\n",
    "class SentimentLRModel(bentoml.BentoService):\n",
    "    \n",
    "    @bentoml.api(DataframeHandler, typ='series')\n",
    "    def predict(self, series):\n",
    "        \"\"\"\n",
    "        predict expects pandas.Series as input\n",
    "        \"\"\"        \n",
    "        return self.artifacts.sentiment_lr.predict(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/bento/SentimentLRModel/2019_05_20_6e63a7ca\n"
     ]
    }
   ],
   "source": [
    "from sentiment_lr_model import SentimentLRModel\n",
    "\n",
    "# Initialize bentoML model with artifacts\n",
    "\n",
    "bento_model = SentimentLRModel.pack(\n",
    "    sentiment_lr=sentiment_lr\n",
    ")\n",
    "\n",
    "# Save bentoML model to directory\n",
    "saved_path = bento_model.save(\"/tmp/bento\")\n",
    "\n",
    "# print the directory containing exported model archive (prefixed with model name and version)\n",
    "print(saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "# Load exported bentoML model archive from path\n",
    "bento_model = bentoml.load(saved_path)\n",
    "\n",
    "# Call predict on the restored sklearn model\n",
    "bento_model.predict(pd.Series([\"hello\", \"hi\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * For demo purpurse, copy generated model to ./model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "shutil.rmtree('./model', ignore_errors=True)\n",
    "shutil.copytree(saved_path, './model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to AWS SageMaker\n",
    "\n",
    "```bash\n",
    "bentoml deploy ARCHIVE_PATH --platform=PLATFORM --region=REGION --stage=STAGE\n",
    "```\n",
    "### Arguments:\n",
    "* archive_path: The file path or s3 that contains BentoML bundles.\n",
    "\n",
    "### Options:\n",
    "* platform: REQUIRED.  The platform that you want to deploy bentoml bundle to.  For serverless, we support aws-lambda, aws-lambda-py2 and gcp-function.\n",
    "* region: OPTIONAL The cloud provider's region you want to deploy in.\n",
    "* stage: OPTIONAL Stage is a helper value that identify the position in the workflow process. DEFAULT value is dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/11 : FROM continuumio/miniconda3\n",
      " ---> 6b5cf97566c3\n",
      "Step 2/11 : EXPOSE 8080\n",
      " ---> Using cache\n",
      " ---> 2d9744110aa1\n",
      "Step 3/11 : RUN set -x      && apt-get update      && apt-get install --no-install-recommends --no-install-suggests -y libpq-dev build-essential     && apt-get install -y nginx      && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> cfbb3a51fbf0\n",
      "Step 4/11 : RUN conda update conda -y       && conda install pip numpy scipy       && pip install gunicorn six gevent\n",
      " ---> Using cache\n",
      " ---> a18fd0f1cde2\n",
      "Step 5/11 : COPY . /opt/program\n",
      " ---> Using cache\n",
      " ---> e2677518bc4e\n",
      "Step 6/11 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> fc1077b4bb1c\n",
      "Step 7/11 : RUN conda env update -n base -f /opt/program/environment.yml\n",
      " ---> Using cache\n",
      " ---> 6aafc83adc8f\n",
      "Step 8/11 : RUN pip install -r /opt/program/requirements.txt\n",
      " ---> Using cache\n",
      " ---> dd5f555d32ef\n",
      "Step 9/11 : RUN pip install git+git://github.com/bentoml/bentoml.git\n",
      " ---> Using cache\n",
      " ---> 1bef6d9b9c31\n",
      "Step 10/11 : RUN if [ -f /opt/program/setup.sh ]; then /bin/bash -c /opt/program/setup.sh; fi\n",
      " ---> Using cache\n",
      " ---> 203eea256c16\n",
      "Step 11/11 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> b61fc923cb0e\n",
      "Successfully built b61fc923cb0e\n",
      "Successfully tagged sentimentlrmodel-sagemaker:latest\n",
      "BentoML: \u001b[32mDeploy to aws-sagemaker complete!\u001b[0m\n",
      "BentoML: \u001b[32mDeployment archive is saved at /Users/bozhaoyu/.bentoml/deployment-snapshots/aws-sagemaker/SentimentLRModel/2019_05_20_6e63a7ca/2019-05-20T12:57:37.199748\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml deploy ./model --platform aws-sagemaker --region us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check deployment status\n",
    "\n",
    "```\n",
    "bentoml checkt-deployment-status ARCHIVE_PATH --platform=PLATFORM --region=REGION --stage=STAGE\n",
    "```\n",
    "\n",
    "### Arguments:\n",
    "* archive_path: The file path or s3 that contains BentoML bundles.\n",
    "\n",
    "### Options:\n",
    "* platform: REQUIRED.  The platform that you want to deploy bentoml bundle to.  For serverless, we support aws-lambda, aws-lambda-py2 and gcp-function.\n",
    "* region: OPTIONAL The cloud provider's region you want to deploy in.\n",
    "* stage: OPTIONAL Stage is a helper value that identify the position in the workflow process. DEFAULT value is dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml check-deployment-status ./model --platform=aws-sagemaker --region=us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete serverless Deployment\n",
    "\n",
    "\n",
    "```bash\n",
    "bentoml delete-deployment ARCHIVE_PATH --platform=PLATFORM\n",
    "```\n",
    "### Arguments:\n",
    "* archive_path: The file path or s3 that contains BentoML bundles.\n",
    "\n",
    "### Options:\n",
    "* platform: REQUIRED.  The platform that you want to deploy bentoml bundle to.  For serverless, we support aws-lambda, aws-lambda-py2 and gcp-function.\n",
    "* region: OPTIONAL The cloud provider's region you want to deploy in.\n",
    "* stage: OPTIONAL Stage is a helper value that identify the position in the workflow process. DEFAULT value is dev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BentoML: \u001b[32mDelete aws-sagemaker deployment successful\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml delete-deployment ./model --platform aws-sagemaker --region us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
