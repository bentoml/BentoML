

from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Union

import torch
from torch.utils.data.dataset import Dataset

from ...tokenization_utils import PreTrainedTokenizer
from ..processors.squad import SquadFeatures

logger = ...
MODEL_CONFIG_CLASSES = ...
MODEL_TYPES = ...
@dataclass
class SquadDataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """
    model_type: str = ...
    data_dir: str = ...
    max_seq_length: int = ...
    doc_stride: int = ...
    max_query_length: int = ...
    max_answer_length: int = ...
    overwrite_cache: bool = ...
    version_2_with_negative: bool = ...
    null_score_diff_threshold: float = ...
    n_best_size: int = ...
    lang_id: int = ...
    threads: int = ...


class Split(Enum):
    train = ...
    dev = ...


class SquadDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """
    args: SquadDataTrainingArguments
    features: List[SquadFeatures]
    mode: Split
    is_language_sensitive: bool
    def __init__(self, args: SquadDataTrainingArguments, tokenizer: PreTrainedTokenizer, limit_length: Optional[int] = ..., mode: Union[str, Split] = ..., is_language_sensitive: Optional[bool] = ..., cache_dir: Optional[str] = ..., dataset_format: Optional[str] = ...) -> None:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        ...
    


