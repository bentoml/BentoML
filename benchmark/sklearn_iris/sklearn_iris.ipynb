{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# sklearn IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# add venv PATH to shell command PATH\n",
    "import sys, os\n",
    "if sys.base_prefix not in os.environ['PATH']:\n",
    "    os.environ['PATH'] = f\"{sys.base_prefix}/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "NAME = 'sklearn_iris'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  [2.   1.   0.   1.   1.65 0.   1.21 1.   0.   1.   2.   1.   0.   2.\n",
      " 0.   1.88 2.   2.   0.   0.   1.   2.   1.   1.36 1.49 1.88 1.   1.\n",
      " 2.   2.  ]\n",
      "  mse: 0.087503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)\n",
    "\n",
    "    \n",
    "# add parameters for tuning\n",
    "num_estimators = 100\n",
    "\n",
    "# train the model\n",
    "model = RandomForestRegressor(n_estimators=num_estimators)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print('predictions: ', predictions)\n",
    "\n",
    "# log model performance \n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"  mse: %f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sklearn_iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {NAME}.py\n",
    "\n",
    "import bentoml\n",
    "from bentoml.artifact import SklearnModelArtifact\n",
    "from bentoml.handlers import DataframeHandler\n",
    "\n",
    "\n",
    "@bentoml.env()\n",
    "@bentoml.artifacts([SklearnModelArtifact('model')])\n",
    "class BentoSvc(bentoml.BentoService):\n",
    "    @bentoml.api(DataframeHandler)\n",
    "    def predict(self, inputs):\n",
    "        outputs = self.artifacts.model.predict(inputs.to_numpy())\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-03-19 16:37:17,373] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-03-19 16:37:27,241] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "running sdist\n",
      "running egg_info\n",
      "writing BentoML.egg-info/PKG-INFO\n",
      "writing dependency_links to BentoML.egg-info/dependency_links.txt\n",
      "writing entry points to BentoML.egg-info/entry_points.txt\n",
      "writing requirements to BentoML.egg-info/requires.txt\n",
      "writing top-level names to BentoML.egg-info/top_level.txt\n",
      "reading manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "warning: no files found matching '*' under directory 'bentoml/yatai/web/dist'\n",
      "no previously-included directories found matching 'e2e_tests'\n",
      "no previously-included directories found matching 'tests'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating BentoML-0.5.2+137.gcbdaab7\n",
      "creating BentoML-0.5.2+137.gcbdaab7/BentoML.egg-info\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/bundler\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/clipper\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/configuration\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/configuration/__pycache__\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/deployment\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/aws_lambda\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/sagemaker\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/marshal\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/migrations\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/migrations/__pycache__\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/migrations/versions\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/migrations/versions/__pycache__\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/proto\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/repository\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/server/static\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/utils/validator\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/yatai\n",
      "creating BentoML-0.5.2+137.gcbdaab7/bentoml/yatai/client\n",
      "copying files to BentoML-0.5.2+137.gcbdaab7...\n",
      "copying LICENSE -> BentoML-0.5.2+137.gcbdaab7\n",
      "copying MANIFEST.in -> BentoML-0.5.2+137.gcbdaab7\n",
      "copying README.md -> BentoML-0.5.2+137.gcbdaab7\n",
      "copying pyproject.toml -> BentoML-0.5.2+137.gcbdaab7\n",
      "copying setup.cfg -> BentoML-0.5.2+137.gcbdaab7\n",
      "copying setup.py -> BentoML-0.5.2+137.gcbdaab7\n",
      "copying versioneer.py -> BentoML-0.5.2+137.gcbdaab7\n",
      "copying BentoML.egg-info/PKG-INFO -> BentoML-0.5.2+137.gcbdaab7/BentoML.egg-info\n",
      "copying BentoML.egg-info/SOURCES.txt -> BentoML-0.5.2+137.gcbdaab7/BentoML.egg-info\n",
      "copying BentoML.egg-info/dependency_links.txt -> BentoML-0.5.2+137.gcbdaab7/BentoML.egg-info\n",
      "copying BentoML.egg-info/entry_points.txt -> BentoML-0.5.2+137.gcbdaab7/BentoML.egg-info\n",
      "copying BentoML.egg-info/requires.txt -> BentoML-0.5.2+137.gcbdaab7/BentoML.egg-info\n",
      "copying BentoML.egg-info/top_level.txt -> BentoML-0.5.2+137.gcbdaab7/BentoML.egg-info\n",
      "copying bentoml/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml\n",
      "copying bentoml/_version.py -> BentoML-0.5.2+137.gcbdaab7/bentoml\n",
      "copying bentoml/alembic.ini -> BentoML-0.5.2+137.gcbdaab7/bentoml\n",
      "copying bentoml/db.py -> BentoML-0.5.2+137.gcbdaab7/bentoml\n",
      "copying bentoml/exceptions.py -> BentoML-0.5.2+137.gcbdaab7/bentoml\n",
      "copying bentoml/service.py -> BentoML-0.5.2+137.gcbdaab7/bentoml\n",
      "copying bentoml/service_env.py -> BentoML-0.5.2+137.gcbdaab7/bentoml\n",
      "copying bentoml/artifact/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/fastai_model_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/h2o_model_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/keras_model_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/lightgbm_model_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/pickle_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/pytorch_model_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/sklearn_model_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/text_file_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/tf_savedmodel_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/artifact/xgboost_model_artifact.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/artifact\n",
      "copying bentoml/bundler/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/bundler\n",
      "copying bentoml/bundler/bundler.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/bundler\n",
      "copying bentoml/bundler/config.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/bundler\n",
      "copying bentoml/bundler/loader.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/bundler\n",
      "copying bentoml/bundler/py_module_utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/bundler\n",
      "copying bentoml/bundler/templates.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/bundler\n",
      "copying bentoml/bundler/utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/bundler\n",
      "copying bentoml/cli/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/cli/aws_lambda.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/cli/aws_sagemaker.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/cli/bento.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/cli/click_utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/cli/config.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/cli/deployment.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/cli/utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/cli/yatai_service.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/cli\n",
      "copying bentoml/clipper/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/clipper\n",
      "copying bentoml/configuration/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration\n",
      "copying bentoml/configuration/configparser.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration\n",
      "copying bentoml/configuration/default_bentoml.cfg -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-36.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-37.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-38.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-36.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-37.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-38.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/configuration/__pycache__\n",
      "copying bentoml/deployment/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment\n",
      "copying bentoml/deployment/operator.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment\n",
      "copying bentoml/deployment/store.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment\n",
      "copying bentoml/deployment/utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment\n",
      "copying bentoml/deployment/aws_lambda/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/download_extra_resources.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/lambda_app.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/sagemaker/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_nginx.conf -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_serve.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_wsgi.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/deployment/sagemaker\n",
      "copying bentoml/handlers/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/base_handlers.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/clipper_handler.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/dataframe_handler.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/fastai_image_handler.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/image_handler.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/json_handler.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/pytorch_tensor_handler.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/tensorflow_tensor_handler.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/handlers/utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/handlers\n",
      "copying bentoml/marshal/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/marshal\n",
      "copying bentoml/marshal/dispatcher.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/marshal\n",
      "copying bentoml/marshal/marshal.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/marshal\n",
      "copying bentoml/marshal/utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/marshal\n",
      "copying bentoml/migrations/README -> BentoML-0.5.2+137.gcbdaab7/bentoml/migrations\n",
      "copying bentoml/migrations/env.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/migrations\n",
      "copying bentoml/migrations/script.py.mako -> BentoML-0.5.2+137.gcbdaab7/bentoml/migrations\n",
      "copying bentoml/migrations/__pycache__/env.cpython-36.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/migrations/__pycache__\n",
      "copying bentoml/migrations/__pycache__/env.cpython-37.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/migrations/__pycache__\n",
      "copying bentoml/migrations/versions/a6b00ae45279_add_last_updated_at_for_deployments.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/migrations/versions\n",
      "copying bentoml/migrations/versions/__pycache__/a6b00ae45279_add_last_updated_at_for_deployments.cpython-36.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/migrations/versions/__pycache__\n",
      "copying bentoml/migrations/versions/__pycache__/a6b00ae45279_add_last_updated_at_for_deployments.cpython-37.pyc -> BentoML-0.5.2+137.gcbdaab7/bentoml/migrations/versions/__pycache__\n",
      "copying bentoml/proto/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/proto\n",
      "copying bentoml/proto/deployment_pb2.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/proto\n",
      "copying bentoml/proto/repository_pb2.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/proto\n",
      "copying bentoml/proto/status_pb2.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2_grpc.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/proto\n",
      "copying bentoml/repository/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/repository\n",
      "copying bentoml/repository/metadata_store.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/repository\n",
      "copying bentoml/server/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "copying bentoml/server/bento_api_server.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "copying bentoml/server/bento_sagemaker_server.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "copying bentoml/server/gunicorn_config.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "copying bentoml/server/gunicorn_server.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "copying bentoml/server/marshal_server.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "copying bentoml/server/middlewares.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "copying bentoml/server/utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/server\n",
      "copying bentoml/server/static/swagger-ui-bundle.js -> BentoML-0.5.2+137.gcbdaab7/bentoml/server/static\n",
      "copying bentoml/server/static/swagger-ui.css -> BentoML-0.5.2+137.gcbdaab7/bentoml/server/static\n",
      "copying bentoml/utils/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/cloudpickle.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/hybirdmethod.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/log.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/pip_pkg.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/s3.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/tempdir.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/trace.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/usage_stats.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils\n",
      "copying bentoml/utils/validator/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/utils/validator\n",
      "copying bentoml/yatai/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/yatai\n",
      "copying bentoml/yatai/deployment_utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/yatai\n",
      "copying bentoml/yatai/status.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/yatai\n",
      "copying bentoml/yatai/utils.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/yatai\n",
      "copying bentoml/yatai/yatai_service_impl.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/yatai\n",
      "copying bentoml/yatai/client/__init__.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/bento_repository_api.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/deployment_api.py -> BentoML-0.5.2+137.gcbdaab7/bentoml/yatai/client\n",
      "Writing BentoML-0.5.2+137.gcbdaab7/setup.cfg\n",
      "UPDATING BentoML-0.5.2+137.gcbdaab7/bentoml/_version.py\n",
      "set BentoML-0.5.2+137.gcbdaab7/bentoml/_version.py to '0.5.2+137.gcbdaab7'\n",
      "Creating tar archive\n",
      "removing 'BentoML-0.5.2+137.gcbdaab7' (and everything under it)\n",
      "[2020-03-19 16:37:27,992] INFO - BentoService bundle 'BentoSvc:20200319163717_088228' created at: /tmp/bentoml-temp-dc9ta2ub\n",
      "[2020-03-19 16:37:27,993] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-03-19 16:37:28,006] WARNING - Saved BentoService bundle version mismatch: loading BentoServie bundle create with BentoML version 0.5.2,  but loading from BentoML version 0.5.2+138.g30e4b17\n",
      "[2020-03-19 16:37:28,028] INFO - BentoService bundle 'BentoSvc:20200319163717_088228' saved to: /home/bentoml/bentoml/repository/BentoSvc/20200319163717_088228\n"
     ]
    }
   ],
   "source": [
    "from sklearn_iris import BentoSvc\n",
    "\n",
    "bento_svc = BentoSvc()\n",
    "bento_svc.pack(\"model\", model)\n",
    "saved_path = bento_svc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36041\n"
     ]
    }
   ],
   "source": [
    "from bentoml.utils import detect_free_port\n",
    "PORT = detect_free_port()\n",
    "print(PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bentoml serve-gunicorn /home/bentoml/bentoml/repository/BentoSvc/20200319163717_088228 --port 36041 --workers 1 --enable-microbatch\n",
      "[2020-03-19 16:47:07,627] INFO - get_gunicorn_num_of_workers: 3, calculated by cpu count\n",
      "[2020-03-19 16:47:07,632] INFO - Running micro batch service on :36041\n",
      "^C\n",
      "Process Process-1:\n",
      "\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "print(f\"bentoml serve-gunicorn {saved_path} --port {PORT} --workers 1 --enable-microbatch\")\n",
    "!bentoml serve-gunicorn {saved_path} --port {PORT} --workers 1 --enable-microbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & Run Bento Service in Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {saved_path}\n",
    "IMG_NAME = saved_path.split('/')[-1].lower()\n",
    "!docker build -t {IMG_NAME} \\\n",
    "    --build-arg PIP_TRUSTED_HOST=192.168.138.2 \\  # set your prefer PYPI mirror\n",
    "    --build-arg PIP_INDEX_URL=http://192.168.138.2/simple \\\n",
    "    {saved_path}\n",
    "!docker run -itd -p {PORT}:5000 --cpus 1 -e FLAGS=\"--workers 1 --enable-microbatch\" {IMG_NAME}:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[2.0, 1.0, 0.0, 1.0, 1.65, 0.0, 1.21, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.88, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.36, 1.49, 1.88, 1.0, 1.0, 2.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "raw_data = X_test\n",
    "data = pd.DataFrame(raw_data).to_json()\n",
    "\n",
    "json_response = requests.post(f'http://127.0.0.1:{PORT}/predict',\n",
    "                              data=data, headers=headers)\n",
    "print(json_response)\n",
    "print(json_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark with locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_sklearn_iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmark_{NAME}.py\n",
    "from locust import HttpLocust, TaskSet, task, constant\n",
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def data_producer():\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    x = iris.data[:, 2:]\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)\n",
    "\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "    def _gen_data():\n",
    "        raw_data = X_test\n",
    "        data = pd.DataFrame(raw_data).to_json()\n",
    "        return headers, data\n",
    "\n",
    "    return _gen_data\n",
    "\n",
    "\n",
    "class WebsiteTasks(TaskSet):\n",
    "\n",
    "    @task\n",
    "    def index(self):\n",
    "        headers, data = data_producer()()\n",
    "        self.client.post(\"/predict\", data, headers=headers)\n",
    "\n",
    "class WebsiteUser(HttpLocust):\n",
    "    task_set = WebsiteTasks\n",
    "    wait_time = constant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-03-19 16:47:52,290] beta/INFO/locust.main: Starting web monitor at http://*:8089\n",
      "[2020-03-19 16:47:52,290] beta/INFO/locust.main: Starting Locust 0.14.4\n",
      "[2020-03-19 16:48:06,109] beta/INFO/locust.runners: Hatching and swarming 950 users at the rate 100 users/s (0 users already running)...\n",
      "[2020-03-19 16:48:17,325] beta/WARNING/root: Loadgen CPU usage above 90%! This may constrain your throughput and may even give inconsistent response time measurements! See https://docs.locust.io/en/stable/running-locust-distributed.html for how to distribute the load over multiple CPU cores or machines\n",
      "[2020-03-19 16:48:22,522] beta/INFO/locust.runners: All locusts hatched: WebsiteUser: 950 (0 already running)\n",
      "^C\n",
      "[2020-03-19 17:04:38,000] beta/ERROR/stderr: KeyboardInterrupt\n",
      "[2020-03-19 17:04:38,001] beta/ERROR/stderr: 2020-03-19T09:04:37Z\n",
      "[2020-03-19 17:04:38,001] beta/ERROR/stderr: \n",
      "[2020-03-19 17:04:38,001] beta/INFO/locust.main: Shutting down (exit code 0), bye.\n",
      "[2020-03-19 17:04:38,001] beta/INFO/locust.main: Cleaning up runner...\n",
      "[2020-03-19 17:04:38,002] beta/INFO/locust.main: Running teardowns...\n",
      " Name                                                          # reqs      # fails     Avg     Min     Max  |  Median   req/s failures/s\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " POST /predict                                                  75289     0(0.00%)     384      16     809  |     430  604.74    0.00\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      " Aggregated                                                     75289     0(0.00%)     384      16     809  |     430  604.74    0.00\n",
      "\n",
      "Percentage of the requests completed within given times\n",
      " Type                 Name                                                           # reqs    50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100%\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " POST                 /predict                                                        75289    430    520    570    590    650    680    720    740    790    810    810\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " None                 Aggregated                                                      75289    430    520    570    590    650    680    720    740    790    810    810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!locust -f benchmark_{NAME}.py -H http://127.0.0.1:{PORT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bentoml-dev-py36",
   "language": "python",
   "name": "bentoml-dev-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
