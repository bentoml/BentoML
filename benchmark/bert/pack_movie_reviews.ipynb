{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0a4mTk9o1Qg"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "# Predicting Movie Review Sentiment with [kpe/bert-for-tf2](https://github.com/kpe/bert-for-tf2)\n",
    "\n",
    "\n",
    "A modification of https://github.com/kpe/bert-for-tf2/blob/master/examples/gpu_movie_reviews.ipynb,\n",
    "which is a modification of https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb using the Tensorflow 2.0 Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages (4.45.0)\n",
      "Requirement already satisfied: bert-for-tf2 in /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages (0.14.1)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /home/bentoml/.local/lib/python3.6/site-packages (from bert-for-tf2) (0.8.0)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /home/bentoml/.local/lib/python3.6/site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow:  2.1.0\n",
      "Python:  3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29) \n",
      "[GCC 7.3.0]\n",
      "GPU:  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "from tensorflow import keras\n",
    "\n",
    "# tf.config.set_visible_devices([], 'GPU')  # disable GPU\n",
    "print(\"Tensorflow: \", tf.__version__)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"GPU: \", tf.test.is_gpu_available())\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 6  # required by clipper benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom_ff20gyy6"
   },
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in tqdm(os.listdir(directory), desc=os.path.basename(directory)):\n",
    "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True)\n",
    "\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                           \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                          \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaE2G_2DdzVg"
   },
   "source": [
    "Let's use the `MovieReviewData` class below, to prepare/encode \n",
    "the data for feeding into our BERT model, by:\n",
    "  - tokenizing the text\n",
    "  - trim or pad it to a `max_seq_len` length\n",
    "  - append the special tokens `[CLS]` and `[SEP]`\n",
    "  - convert the string tokens to numerical `ID`s using the original model's token encoding from `vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "\n",
    "\n",
    "class MovieReviewData:\n",
    "    DATA_COLUMN = \"sentence\"\n",
    "    LABEL_COLUMN = \"polarity\"\n",
    "\n",
    "    def __init__(self, tokenizer: FullTokenizer, sample_size=None, max_seq_len=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        train, test = download_and_load_datasets()\n",
    "        \n",
    "        train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
    "                          [train, test])\n",
    "                \n",
    "        if sample_size is not None:\n",
    "            train, test = train.head(sample_size), test.head(sample_size)\n",
    "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
    "        \n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
    "                                                       [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                x.append(token_ids)\n",
    "                y.append(int(label))\n",
    "                pbar.update()\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "asset_path = 'asset'\n",
    "bert_model_name = \"uncased_L-12_H-768_A-12\"\n",
    "bert_ckpt_dir    = os.path.join(asset_path, bert_model_name)\n",
    "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -f asset/uncased_L-12_H-768_A-12.zip ]; then\n",
    "    curl -o asset/uncased_L-12_H-768_A-12.zip --create-dirs https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "fi\n",
    "if [ ! -d asset/uncased_L-12_H-768_A-12 ]; then\n",
    "    unzip asset/uncased_L-12_H-768_A-12.zip -d asset/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4xPTleh2X2b"
   },
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Now let's fetch and prepare the data by taking the first `max_seq_len` tokenens after tokenizing with the BERT tokenizer, und use `sample_size` examples for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of about 2500 train and test examples, respectively, and use the first 128 tokens only (transformers memory and computation requirements scale quadraticly with the sequence length - so with a TPU you might use `max_seq_len=512`, but on a GPU this would be too slow, and you will have to use a very small `batch_size`s to fit the model into the GPU memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "kF_3KhGQ0GTc",
    "outputId": "fd993d23-61ce-4aae-c992-5b5591123198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos: 100%|██████████| 12500/12500 [00:00<00:00, 20666.87it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:00<00:00, 20416.22it/s]\n",
      "pos: 100%|██████████| 12500/12500 [00:00<00:00, 19930.50it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:00<00:00, 21163.18it/s]\n",
      "100%|██████████| 2.56k/2.56k [00:02<00:00, 910it/s]  \n",
      "100%|██████████| 2.56k/2.56k [00:02<00:00, 943it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 178\n",
      "CPU times: user 18.4 s, sys: 4.59 s, total: 23 s\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "data = MovieReviewData(tokenizer, \n",
    "                       sample_size=10*128*2, #10*128*2\n",
    "                       max_seq_len=128)\n",
    "\n",
    "print(\"            train_x\", data.train_x.shape)\n",
    "print(\"train_x_token_types\", data.train_x_token_types.shape)\n",
    "print(\"            train_y\", data.train_y.shape)\n",
    "\n",
    "print(\"             test_x\", data.test_x.shape)\n",
    "\n",
    "print(\"        max_seq_len\", data.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "## Adapter BERT\n",
    "\n",
    "If we decide to use [adapter-BERT](https://arxiv.org/abs/1902.00751) we need some helpers for freezing the original BERT layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "def flatten_layers(root_layer):\n",
    "    if isinstance(root_layer, keras.layers.Layer):\n",
    "        yield root_layer\n",
    "    for layer in root_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "\n",
    "def freeze_bert_layers(l_bert):\n",
    "    \"\"\"\n",
    "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
    "    \"\"\"\n",
    "    for layer in flatten_layers(l_bert):\n",
    "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "            layer.trainable = True\n",
    "        elif len(layer._layers) == 0:\n",
    "            layer.trainable = False\n",
    "        l_bert.embeddings_layer.trainable = False\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(\n",
    "                math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "# Creating a model\n",
    "\n",
    "Now let's create a classification model using [adapter-BERT](https//arxiv.org/abs/1902.00751), which is clever way of reducing the trainable parameter count, by freezing the original BERT weights, and adapting them with two FFN bottlenecks (i.e. `adapter_size` bellow) in every BERT layer.\n",
    "\n",
    "**N.B.** The commented out code below show how to feed a `token_type_ids`/`segment_ids` sequence (which is not needed in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, adapter_size=64):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  #adapter_size = 64  # see - arXiv:1902.00751\n",
    "\n",
    "  # create the bert layer\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = adapter_size\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "  input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "  # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
    "  # output         = bert([input_ids, token_type_ids])\n",
    "  output         = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", output.shape)\n",
    "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "  logits = keras.layers.Dropout(0.5)(logits)\n",
    "  logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n",
    "\n",
    "  # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
    "  # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
    "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  # load the pre-trained model weights\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "  # freeze weights if adapter-BERT is used\n",
    "  if adapter_size is not None:\n",
    "      freeze_bert_layers(bert)\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
    "\n",
    "  model.summary()\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "colab_type": "code",
    "id": "bZnmtDc7HlEm",
    "outputId": "fcd96c78-792c-4032-d188-73a9c21ec304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 128, 768)\n",
      "Done loading 196 BERT weights from: asset/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f44990ff9e8> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 109,482,242\n",
      "Trainable params: 109,482,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adapter_size = None # use None to fine-tune all of BERT\n",
    "model = create_model(data.max_seq_len, adapter_size=adapter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZuLOkwonF-9S",
    "outputId": "ce1451d4-310c-41f1-ddd3-a2e0841d8528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2304 samples, validate on 256 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 5.000000000000001e-07.\n",
      "Epoch 1/20\n",
      "2304/2304 [==============================] - 130s 56ms/sample - loss: 0.7038 - acc: 0.5556 - val_loss: 0.6826 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 1.0000000000000002e-06.\n",
      "Epoch 2/20\n",
      "2304/2304 [==============================] - 124s 54ms/sample - loss: 0.6928 - acc: 0.5499 - val_loss: 0.6581 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1.5000000000000002e-06.\n",
      "Epoch 3/20\n",
      "2304/2304 [==============================] - 124s 54ms/sample - loss: 0.6723 - acc: 0.5864 - val_loss: 0.6119 - val_acc: 0.7344\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
      "Epoch 4/20\n",
      "2304/2304 [==============================] - 122s 53ms/sample - loss: 0.5676 - acc: 0.7296 - val_loss: 0.4608 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 2.5000000000000006e-06.\n",
      "Epoch 5/20\n",
      "2304/2304 [==============================] - 124s 54ms/sample - loss: 0.4319 - acc: 0.8837 - val_loss: 0.4156 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 3.0000000000000005e-06.\n",
      "Epoch 6/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3982 - acc: 0.9158 - val_loss: 0.4173 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 3.5000000000000004e-06.\n",
      "Epoch 7/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3786 - acc: 0.9358 - val_loss: 0.4217 - val_acc: 0.8867\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 4.000000000000001e-06.\n",
      "Epoch 8/20\n",
      "2304/2304 [==============================] - 128s 55ms/sample - loss: 0.3640 - acc: 0.9488 - val_loss: 0.4215 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 4.500000000000001e-06.\n",
      "Epoch 9/20\n",
      "2304/2304 [==============================] - 128s 55ms/sample - loss: 0.3548 - acc: 0.9583 - val_loss: 0.4161 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 5.000000000000001e-06.\n",
      "Epoch 10/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3493 - acc: 0.9653 - val_loss: 0.4306 - val_acc: 0.8789\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 5.500000000000001e-06.\n",
      "Epoch 11/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3485 - acc: 0.9631 - val_loss: 0.4167 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 6.000000000000001e-06.\n",
      "Epoch 12/20\n",
      "2304/2304 [==============================] - 126s 55ms/sample - loss: 0.3458 - acc: 0.9674 - val_loss: 0.4385 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.500000000000001e-06.\n",
      "Epoch 13/20\n",
      "2304/2304 [==============================] - 126s 55ms/sample - loss: 0.3438 - acc: 0.9701 - val_loss: 0.4144 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 7.000000000000001e-06.\n",
      "Epoch 14/20\n",
      "2304/2304 [==============================] - 126s 55ms/sample - loss: 0.3440 - acc: 0.9683 - val_loss: 0.4520 - val_acc: 0.8594\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 7.500000000000001e-06.\n",
      "Epoch 15/20\n",
      "2304/2304 [==============================] - 128s 56ms/sample - loss: 0.3446 - acc: 0.9688 - val_loss: 0.4258 - val_acc: 0.8867\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 8.000000000000001e-06.\n",
      "Epoch 16/20\n",
      "2304/2304 [==============================] - 130s 56ms/sample - loss: 0.3451 - acc: 0.9679 - val_loss: 0.4414 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 8.500000000000002e-06.\n",
      "Epoch 17/20\n",
      "2304/2304 [==============================] - 128s 56ms/sample - loss: 0.3388 - acc: 0.9748 - val_loss: 0.4401 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 9.000000000000002e-06.\n",
      "Epoch 18/20\n",
      "2304/2304 [==============================] - 129s 56ms/sample - loss: 0.3370 - acc: 0.9761 - val_loss: 0.4362 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 9.500000000000002e-06.\n",
      "Epoch 19/20\n",
      "2304/2304 [==============================] - 128s 56ms/sample - loss: 0.3394 - acc: 0.9735 - val_loss: 0.4440 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.0000000000000003e-05.\n",
      "Epoch 20/20\n",
      "2304/2304 [==============================] - 127s 55ms/sample - loss: 0.3408 - acc: 0.9727 - val_loss: 0.4424 - val_acc: 0.8711\n",
      "CPU times: user 41min 17s, sys: 14min 34s, total: 55min 52s\n",
      "Wall time: 42min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f43e31a13c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "log_dir = \".log/movie_reviews/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "total_epoch_count = 20\n",
    "# model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,\n",
    "model.fit(x=data.train_x, y=data.train_y,\n",
    "          validation_split=0.1,\n",
    "          batch_size=12,\n",
    "          shuffle=True,\n",
    "          epochs=total_epoch_count,\n",
    "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
    "                                                    end_learn_rate=1e-7,\n",
    "                                                    warmup_epoch_count=20,\n",
    "                                                    total_epoch_count=total_epoch_count),\n",
    "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "                     tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./movie_reviews.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "BSqMu64oHzqy",
    "outputId": "95a8284b-b3b2-4a7d-f335-c4ff65f8f920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 34s 13ms/sample - loss: 0.3419 - acc: 0.9711\n",
      "2560/2560 [==============================] - 33s 13ms/sample - loss: 0.3903 - acc: 0.9230\n",
      "train acc 0.9710938\n",
      " test acc 0.9230469\n",
      "CPU times: user 1min 6s, sys: 194 ms, total: 1min 7s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSKDZEnVabnl"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the trained model, let's load the saved weights in a new model instance, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 128, 768)\n",
      "Done loading 196 BERT weights from: asset/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fb56f0e66a0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 109,482,242\n",
      "Trainable params: 109,482,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(data.max_seq_len, adapter_size=None)\n",
    "model.load_weights(\"./movie_reviews.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "qCpabQ15WS3U",
    "outputId": "220b2b55-dd78-4795-d3b6-d364c8323734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 35s 14ms/sample - loss: 0.3903 - acc: 0.9230\n",
      " test acc 0.9230469\n",
      "CPU times: user 34.6 s, sys: 113 ms, total: 34.7 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# _, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "# print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uzdOFQ5awM1"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "For prediction, we need to prepare the input text the same way as we did for training - tokenize, adding the special `[CLS]` and `[SEP]` token at begin and end of the token sequence, and pad to match the model input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 155 ms, sys: 14.8 ms, total: 170 ms\n",
      "Wall time: 170 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negative', 'negative', 'positive', 'positive']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "CLASSES = [\"negative\",\"positive\"]\n",
    "max_seq_len = 128\n",
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\",\n",
    "]\n",
    "\n",
    "inputs = pd.DataFrame(pred_sentences)\n",
    "\n",
    "pred_tokens    = map(tokenizer.tokenize, inputs.to_numpy()[:, 0].tolist())\n",
    "pred_tokens    = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "pred_token_ids = map(lambda tids: tids + [0] * (max_seq_len-len(tids)), pred_token_ids)\n",
    "pred_token_ids = np.array(list(pred_token_ids))\n",
    "\n",
    "res = model(pred_token_ids).numpy().argmax(axis=-1)\n",
    "[CLASSES[i] for i in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build & Save bentoml service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bentoml_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bentoml_service.py\n",
    "\n",
    "import bentoml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml.artifact import (\n",
    "    TensorflowSavedModelArtifact, PickleArtifact,\n",
    ")\n",
    "from bentoml.handlers import DataframeHandler, ClipperStringsHandler\n",
    "\n",
    "\n",
    "CLASSES = [\"negative\",\"positive\"]\n",
    "max_seq_len = 128\n",
    "\n",
    "try:\n",
    "    tf.config.set_visible_devices([], 'GPU')  # disable GPU, required when served in docker\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "@bentoml.env(pip_dependencies=['tensorflow', 'bert-for-tf2'])\n",
    "@bentoml.artifacts([TensorflowSavedModelArtifact('model'), PickleArtifact('tokenizer')])\n",
    "class Service(bentoml.BentoService):\n",
    "\n",
    "    def tokenize(self, inputs: pd.DataFrame):\n",
    "        tokenizer = self.artifacts.tokenizer\n",
    "        if isinstance(inputs, pd.DataFrame):\n",
    "            inputs = inputs.to_numpy()[:, 0].tolist()\n",
    "        else:\n",
    "            inputs = inputs.tolist()  # for predict_clipper\n",
    "        pred_tokens = map(tokenizer.tokenize, inputs)\n",
    "        pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "        pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "        pred_token_ids = map(lambda tids: tids + [0] * (max_seq_len - len(tids)), pred_token_ids)\n",
    "        pred_token_ids = tf.constant(list(pred_token_ids), dtype=tf.int32)\n",
    "        return pred_token_ids\n",
    "\n",
    "    @bentoml.api(DataframeHandler, mb_max_latency=10000, mb_max_batch_size=1000)\n",
    "    def predict(self, inputs):\n",
    "        model = self.artifacts.model\n",
    "        pred_token_ids = self.tokenize(inputs)\n",
    "        res = model(pred_token_ids).numpy().argmax(axis=-1)\n",
    "        return [CLASSES[i] for i in res]\n",
    "\n",
    "    @bentoml.api(ClipperStringsHandler)\n",
    "    def predict_clipper(self, strings):\n",
    "        model = self.artifacts.model\n",
    "        pred_token_ids = self.tokenize(strings)\n",
    "        res = model(pred_token_ids).numpy().argmax(axis=-1)\n",
    "        return [CLASSES[i] for i in res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-24 10:48:19,051] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-04-24 10:48:19,438] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7fb56dd026a0>, because it is not built.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /tmp/bentoml-temp-jxboq950/Service/artifacts/model_saved_model/assets\n",
      "[2020-04-24 10:49:26,116] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "running sdist\n",
      "running egg_info\n",
      "writing BentoML.egg-info/PKG-INFO\n",
      "writing dependency_links to BentoML.egg-info/dependency_links.txt\n",
      "writing entry points to BentoML.egg-info/entry_points.txt\n",
      "writing requirements to BentoML.egg-info/requires.txt\n",
      "writing top-level names to BentoML.egg-info/top_level.txt\n",
      "reading manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
      "warning: no previously-included files matching '.git' found anywhere in distribution\n",
      "warning: no previously-included files matching '.ipynb_checkpoints' found anywhere in distribution\n",
      "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "warning: no directories found matching 'bentoml/yatai/web/dist'\n",
      "no previously-included directories found matching 'e2e_tests'\n",
      "no previously-included directories found matching 'tests'\n",
      "no previously-included directories found matching 'benchmark'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'BentoML.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/clipper\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/__pycache__\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions/__pycache__\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/repository\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server/static\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils/validator\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "creating BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai/client\n",
      "copying files to BentoML-0.5.2+173.ga4278d4.dirty...\n",
      "copying LICENSE -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying MANIFEST.in -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying README.md -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying pyproject.toml -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying setup.cfg -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying setup.py -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying versioneer.py -> BentoML-0.5.2+173.ga4278d4.dirty\n",
      "copying BentoML.egg-info/PKG-INFO -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/SOURCES.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/dependency_links.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/entry_points.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/requires.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying BentoML.egg-info/top_level.txt -> BentoML-0.5.2+173.ga4278d4.dirty/BentoML.egg-info\n",
      "copying bentoml/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/_version.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/alembic.ini -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/db.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/exceptions.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/service.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/service_env.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml\n",
      "copying bentoml/artifact/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/fastai_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/fasttext_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/h2o_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/keras_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/lightgbm_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/pickle_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/pytorch_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/sklearn_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/text_file_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/tf_savedmodel_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/artifact/xgboost_model_artifact.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/artifact\n",
      "copying bentoml/bundler/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/bundler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/config.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/loader.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/py_module_utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/templates.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/bundler/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/bundler\n",
      "copying bentoml/cli/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/aws_lambda.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/aws_sagemaker.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/bento.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/click_utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/config.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/deployment.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/cli/yatai_service.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/cli\n",
      "copying bentoml/clipper/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/clipper\n",
      "copying bentoml/configuration/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/configparser.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/default_bentoml.cfg -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-36.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-37.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/__init__.cpython-38.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-36.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-37.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/configuration/__pycache__/configparser.cpython-38.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/configuration/__pycache__\n",
      "copying bentoml/deployment/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/operator.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/store.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment\n",
      "copying bentoml/deployment/aws_lambda/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/download_extra_resources.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/lambda_app.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/aws_lambda/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/aws_lambda\n",
      "copying bentoml/deployment/sagemaker/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_nginx.conf -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_serve.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/deployment/sagemaker/sagemaker_wsgi.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/deployment/sagemaker\n",
      "copying bentoml/handlers/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/base_handlers.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/clipper_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/dataframe_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/fastai_image_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/image_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/json_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/pytorch_tensor_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/tensorflow_tensor_handler.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/handlers/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/handlers\n",
      "copying bentoml/marshal/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "copying bentoml/marshal/dispatcher.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "copying bentoml/marshal/marshal.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "copying bentoml/marshal/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/marshal\n",
      "copying bentoml/migrations/README -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/env.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/script.py.mako -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations\n",
      "copying bentoml/migrations/__pycache__/env.cpython-36.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/__pycache__\n",
      "copying bentoml/migrations/__pycache__/env.cpython-37.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/__pycache__\n",
      "copying bentoml/migrations/versions/a6b00ae45279_add_last_updated_at_for_deployments.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions\n",
      "copying bentoml/migrations/versions/__pycache__/a6b00ae45279_add_last_updated_at_for_deployments.cpython-36.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions/__pycache__\n",
      "copying bentoml/migrations/versions/__pycache__/a6b00ae45279_add_last_updated_at_for_deployments.cpython-37.pyc -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/migrations/versions/__pycache__\n",
      "copying bentoml/proto/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/deployment_pb2.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/repository_pb2.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/status_pb2.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/proto/yatai_service_pb2_grpc.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/proto\n",
      "copying bentoml/repository/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/repository\n",
      "copying bentoml/repository/metadata_store.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/repository\n",
      "copying bentoml/server/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/bento_api_server.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/bento_sagemaker_server.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/gunicorn_config.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/gunicorn_server.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/marshal_server.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/middlewares.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server\n",
      "copying bentoml/server/static/swagger-ui-bundle.js -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server/static\n",
      "copying bentoml/server/static/swagger-ui.css -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/server/static\n",
      "copying bentoml/utils/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/benchmark.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/cloudpickle.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/hybridmethod.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/log.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/pip_pkg.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/s3.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/tempdir.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/trace.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/usage_stats.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils\n",
      "copying bentoml/utils/validator/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/utils/validator\n",
      "copying bentoml/yatai/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/deployment_utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/status.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/utils.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/yatai_service_impl.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai\n",
      "copying bentoml/yatai/client/__init__.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/bento_repository_api.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai/client\n",
      "copying bentoml/yatai/client/deployment_api.py -> BentoML-0.5.2+173.ga4278d4.dirty/bentoml/yatai/client\n",
      "Writing BentoML-0.5.2+173.ga4278d4.dirty/setup.cfg\n",
      "UPDATING BentoML-0.5.2+173.ga4278d4.dirty/bentoml/_version.py\n",
      "set BentoML-0.5.2+173.ga4278d4.dirty/bentoml/_version.py to '0.5.2+173.ga4278d4.dirty'\n",
      "Creating tar archive\n",
      "removing 'BentoML-0.5.2+173.ga4278d4.dirty' (and everything under it)\n",
      "[2020-04-24 10:49:27,588] WARNING - BentoML local changes detected - Local BentoML repository including all code changes will be bundled together with the BentoService bundle. When used with docker, the base docker image will be default to same version as last PyPI release at version: 0.5.2. You can also force bentoml to use a specific version for deploying your BentoService bundle, by setting the config 'core/bentoml_deploy_version' to a pinned version or your custom BentoML on github, e.g.:'bentoml_deploy_version = git+https://github.com/{username}/bentoml.git@{branch}'\n",
      "[2020-04-24 10:49:27,603] WARNING - Saved BentoService bundle version mismatch: loading BentoServie bundle create with BentoML version 0.5.2,  but loading from BentoML version 0.5.2+173.ga4278d4.dirty\n",
      "[2020-04-24 10:49:28,021] INFO - BentoService bundle 'Service:20200424104819_1D90A9' saved to: /home/bentoml/bentoml/repository/Service/20200424104819_1D90A9\n"
     ]
    }
   ],
   "source": [
    "from bentoml_service import Service\n",
    "\n",
    "bento_svc = Service()\n",
    "bento_svc.pack(\"model\", model)\n",
    "bento_svc.pack(\"tokenizer\", tokenizer)\n",
    "saved_path = bento_svc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bentoml/bentoml/repository/Service/20200424104819_1D90A9\n"
     ]
    }
   ],
   "source": [
    "print(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Movie Reviews with bert-for-tf2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "bentoml-dev-py36",
   "language": "python",
   "name": "bentoml-dev-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
