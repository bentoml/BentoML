from typing import Dict, List, Optional, Union
from tokenizers import Tokenizer as TokenizerFast
from tokenizers.decoders import Decoder as DecoderFast
from .file_utils import PaddingStrategy, add_end_docstrings
from .tokenization_utils import PreTrainedTokenizer
from .tokenization_utils_base import (
    INIT_TOKENIZER_DOCSTRING,
    PreTrainedTokenizerBase,
    TruncationStrategy,
)

logger = ...
TOKENIZER_FILE = ...
SPECIAL_TOKENS_MAP_FILE = ...
TOKENIZER_CONFIG_FILE = ...
ADDED_TOKENS_FILE = ...
MODEL_TO_TRAINER_MAPPING = ...

@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
    slow_tokenizer_class: PreTrainedTokenizer = ...
    def __init__(self, *args, **kwargs) -> None: ...
    @property
    def is_fast(self) -> bool: ...
    @property
    def vocab_size(self) -> int: ...
    def get_vocab(self) -> Dict[str, int]: ...
    @property
    def vocab(self) -> Dict[str, int]: ...
    def get_added_vocab(self) -> Dict[str, int]: ...
    def __len__(self) -> int: ...
    @property
    def backend_tokenizer(self) -> TokenizerFast: ...
    @property
    def decoder(self) -> DecoderFast: ...
    def convert_tokens_to_ids(
        self, tokens: Union[str, List[str]]
    ) -> Union[int, List[int]]: ...
    def num_special_tokens_to_add(self, pair: bool = ...) -> int: ...
    def convert_ids_to_tokens(
        self, ids: Union[int, List[int]], skip_special_tokens: bool = ...
    ) -> Union[str, List[str]]: ...
    def tokenize(
        self,
        text: str,
        pair: Optional[str] = ...,
        add_special_tokens: bool = ...,
        **kwargs
    ) -> List[str]: ...
    def set_truncation_and_padding(
        self,
        padding_strategy: PaddingStrategy,
        truncation_strategy: TruncationStrategy,
        max_length: int,
        stride: int,
        pad_to_multiple_of: Optional[int],
    ): ...
    def convert_tokens_to_string(self, tokens: List[str]) -> str: ...
    def train_new_from_iterator(
        self,
        text_iterator,
        vocab_size,
        new_special_tokens=...,
        special_tokens_map=...,
        **kwargs
    ): ...
