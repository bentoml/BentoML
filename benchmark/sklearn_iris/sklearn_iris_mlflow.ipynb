{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# sklearn IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# add venv PATH to shell command PATH\n",
    "import sys, os\n",
    "if sys.base_prefix not in os.environ['PATH']:\n",
    "    os.environ['PATH'] = f\"{sys.base_prefix}/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'sklearn_iris_mlflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  [2.   1.   0.   1.   1.58 0.   1.19 1.   0.   1.   2.   1.   0.   2.\n",
      " 0.   1.82 2.   2.   0.   0.   1.   2.   1.   1.25 1.53 1.82 1.   1.\n",
      " 2.   2.  ]\n",
      "  mse: 0.090023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)\n",
    "\n",
    "    \n",
    "# add parameters for tuning\n",
    "num_estimators = 100\n",
    "\n",
    "# train the model\n",
    "rf = RandomForestRegressor(n_estimators=num_estimators)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "print('predictions: ', predictions)\n",
    "\n",
    "# log model performance \n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"  mse: %f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tmpdir = 'mlflow_tmp'\n",
    "data_path = os.path.join(tmpdir, 'skmodel.pkl')\n",
    "with open(data_path, 'wb') as of:\n",
    "    pickle.dump(rf, of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sklearn_iris_mlflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {NAME}.py\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytest\n",
    "import six\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.pyfunc.model\n",
    "from mlflow.models import Model\n",
    "\n",
    "\n",
    "def _load_pyfunc(path):\n",
    "    with open(path, 'rb') as of:\n",
    "        data_model = pickle.load(of)\n",
    "    class Model:\n",
    "        def predict(self, inputs):\n",
    "            inputs = inputs.to_numpy()\n",
    "            outputs = data_model.predict(inputs)\n",
    "            return outputs\n",
    "    return Model()\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tmpdir = 'mlflow_tmp'\n",
    "    data_path = os.path.join(tmpdir, 'skmodel.pkl')\n",
    "    model_path = os.path.join(str(tmpdir), \"model\")\n",
    "\n",
    "    model_config = Model(run_id=\"test\")\n",
    "    mlflow.pyfunc.save_model(path=model_path,\n",
    "                             data_path=data_path,\n",
    "                             loader_module=os.path.basename(__file__)[:-3],\n",
    "                             code_path=[__file__],\n",
    "                             mlflow_model=model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2020/03/19 12:56:47 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n",
      "2020/03/19 12:56:47 INFO mlflow.pyfunc.backend: === Running command 'gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
      "[2020-03-19 12:56:47 +0800] [1555253] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-03-19 12:56:47 +0800] [1555253] [INFO] Listening at: http://127.0.0.1:5000 (1555253)\n",
      "[2020-03-19 12:56:47 +0800] [1555253] [INFO] Using worker: sync\n",
      "[2020-03-19 12:56:47 +0800] [1555267] [INFO] Booting worker with pid: 1555267\n",
      "/opt/anaconda3/envs/bentoml-dev-py36/lib/python3.6/site-packages/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "^C\n",
      "\n",
      "Aborted!\n",
      "[2020-03-19 12:57:39 +0800] [1555253] [INFO] Handling signal: int\n",
      "[2020-03-19 12:57:40 +0800] [1555267] [INFO] Worker exiting (pid: 1555267)\n"
     ]
    }
   ],
   "source": [
    "!rm -r {tmpdir}/model\n",
    "!python {NAME}.py\n",
    "!mlflow models serve -m {tmpdir}/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "raw_data = X_test\n",
    "data = pd.DataFrame(raw_data,\n",
    "                    columns=map(str, range(raw_data.shape[1]))).to_json(orient='split')\n",
    "\n",
    "json_response = requests.post(f'http://127.0.0.1:5000/invocations',\n",
    "                              data=data, headers=headers)\n",
    "print(json_response)\n",
    "print(json_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark with locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting benchmark_sklearn_iris_mlflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmark_{NAME}.py\n",
    "from locust import HttpLocust, TaskSet, task, constant\n",
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def data_producer():\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    x = iris.data[:, 2:]\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)\n",
    "\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "    def _gen_data():\n",
    "        raw_data = X_test\n",
    "        data = pd.DataFrame(raw_data).to_json(orient='split')\n",
    "        return headers, data\n",
    "\n",
    "    return _gen_data\n",
    "\n",
    "\n",
    "class WebsiteTasks(TaskSet):\n",
    "\n",
    "    @task\n",
    "    def index(self):\n",
    "        headers, data = data_producer()()\n",
    "        self.client.post(\"/invocations\", data, headers=headers)\n",
    "\n",
    "class WebsiteUser(HttpLocust):\n",
    "    task_set = WebsiteTasks\n",
    "    wait_time = constant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!locust -f benchmark_{NAME}.py -H http://127.0.0.1:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bentoml-dev-py36",
   "language": "python",
   "name": "bentoml-dev-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
