{% if emphemeral == True %}
<!-- P/S: even this file is autogenerated :happy: -->

## GENERATED DIRECTORY - DO NOT EDIT

These files are maintained by [`Manager`](../manager), which generates Dockerfile from `templates`.
Refers to [main README](../README.md) for more information.
{% else %}
# Unified Model Serving Framework  [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=BentoML:%20The%20Unified%20Model%20Serving%20Framework%20&url=https://github.com/bentoml&via=bentomlai&hashtags=mlops,bentoml)

BentoML is an open platform that simplifies ML model deployment and enables you to serve your models at production scale in minutes

üëâ [Pop into our Slack community!](https://l.linklyhq.com/l/ktOu) We're happy to help with any issue you face or even just to meet you and hear what you're working on :)

[![pypi_status](https://img.shields.io/pypi/v/bentoml.svg)](https://pypi.org/project/BentoML)
[![downloads](https://pepy.tech/badge/bentoml)](https://pepy.tech/project/bentoml)
[![actions_status](https://github.com/bentoml/bentoml/workflows/BentoML-CI/badge.svg)](https://github.com/bentoml/bentoml/actions)
[![documentation_status](https://readthedocs.org/projects/bentoml/badge/?version=latest)](https://docs.bentoml.org/)
[![join_slack](https://badgen.net/badge/Join/BentoML%20Slack/cyan?icon=slack)](https://join.slack.bentoml.org)


## Why BentoML ##

- The easiest way to turn your ML models into production-ready API endpoints.
- High performance model serving, all in Python.
- Standardized model packaging and ML service definition to streamline deployment.
- Support all major machine-learning training [frameworks](https://docs.bentoml.org/en/latest/frameworks/index.html).
- Deploy and operate ML serving workload at scale on Kubernetes via [Yatai](https://github.com/bentoml/yatai).

## Getting Started ##

- [Quickstart guide](https://docs.bentoml.org/en/latest/quickstart.html) will show you a simple example of using BentoML in action. In under 10 minutes, you'll be able to serve your ML model over an HTTP API endpoint, and build a docker image that is ready to be deployed in production.
- [Main concepts](https://docs.bentoml.org/en/latest/concepts/index.html) will give a comprehensive tour of BentoML's components and introduce you to its philosophy. After reading, you will see what drives BentoML's design, and know what `bento` and `runner` stands for.
- [ML Frameworks](https://docs.bentoml.org/en/latest/frameworks/index.html) lays out best practices and example usages by the ML framework used for training models.
- [Advanced Guides](https://docs.bentoml.org/en/latest/guides/index.html) showcases advanced features in BentoML, including GPU support, inference graph, monitoring, and customizing docker environment etc.
- Check out other projects from the [BentoML team](https://github.com/bentoml):
  - [ü¶ÑÔ∏è Yatai](https://github.com/bentoml/yatai): Run BentoML workflow at scale on Kubernetes
  - [üöÄ bentoctl](https://github.com/bentoml/bentoctl): Fast model deployment with BentoML on cloud platforms


## BentoServer base images

There are three type of BentoServer docker base image:

| Image Type | Description                                | Supported OS                                          | Usage                             |
|------------|--------------------------------------------|-------------------------------------------------------|-----------------------------------|
| `runtime`  | contains latest BentoML releases from PyPI | `debian{11,10}`, `ubi8`, `amazonlinux2`, `alpine3.14` | production ready                  |
| `cudnn`    | runtime + support for CUDA-enabled GPU     | `debian{11,10}`, `ubi8`                               | production ready with GPU support |
| `devel`    | nightly build from development branch      | `debian{11,10}`, `ubi8`                               | for development use only          |
| `conda`    | runtime + conda + optional GPU supports    | `debian{11,10}`,                                      | production ready                  |

* Note: currently there's no nightly devel image with GPU support.

The final docker image tags will have the following format:

```markdown
<release_type>-<python_version>-<distros>-<suffix>-<?:conda>
   ‚îÇ             ‚îÇ                ‚îÇ        ‚îÇ
   ‚îÇ             ‚îÇ                ‚îÇ        ‚îî‚îÄ> additional suffix, differentiate runtime and cudnn releases
   ‚îÇ             ‚îÇ                ‚îî‚îÄ> formatted <dist><dist_version>, e.g: ami2, debian, ubi7
   ‚îÇ             ‚îî‚îÄ> Supported Python version: python3.7 | python3.8 | python3.9
   ‚îî‚îÄ>  Release type: devel or official BentoML release (e.g: 1.0.0)
```

Example image tags:
- `bento-server:devel-python3.7-debian`
- `bento-server:1.0.0-python3.8-ubi8-cudnn`
- `bento-server:1.0.0-python3.7-ami2-runtime`

## NOTICE: MISSING PYTHON VERSION ON UBI

Python 3.7 and 3.10 is missing. The reason being RedHat doesn't provide support for these Python version.
If you need to use UBI and Python 3.7 make sure to contact the BentoML team for supports..

## NOTICE: CONDA AVAILABILITY ONLY ON DEBIAN

From 1.0.0a7 onwards, BentoML will only provide conda supports with debian variants only.

We ran into a lot of trouble building BentoML to supports Python from 3.6 to 3.10 with conda environment on other distros than debian. In order to reduce 
complexity we will now only provides conda on Debian-based image. Conda will be available with all of BentoML image type, including `runtime`, `devel`, `cudnn`. 

If you need to use conda on other distros contact the BentoML team for supports.

Example conda tags:
- `bento-server:1.0.0a7-python3.8-debian11-runtime-conda`
- `bento-server:1.0.0a7-python3.8-debian11-cudnn-conda`
- `bento-server:devel-python3.8-debian11-cudnn-conda`

## Latest tags for `{{ bentoml_package }} {{ bentoml_release_version }}`

{% for distros, tag_list in tag_ref.items() %}
    {% set supported_arch = supported[distros] %}

    {%- set distro_string = distros.strip('0123456789') %}
    {% set distro_string_len = distro_string | length %}
    {% set distro_version = distros[distro_string_len:] %}

    {%- if "ubi" in distro_string %}
        {% set distro_string = distro_string.upper() %}
    {% endif %}
    {% if "alpine" in distros %}
        {% set distro_version = distro_string[-2:] + distro_version %}
        {% set distro_string = 'alpine' %}
    {% endif %}

    {%- set formatted_arch = [] %}
    {% for arch in supported_arch %}
        {% set formatted = "_{}_".format(arch) %}
        {% if formatted not in formatted_arch %}
            {% do formatted_arch.append(formatted) %}
        {% endif %}
    {% endfor %}

    {%- set tags_path = tag_ref[distros] %}
    {% print"\n### " %}{% print distro_string %} {% print distro_version %} [ {{ formatted_arch | join(", ") }} ]{% print "\n\n" %}
    {% for (tag, path) in tags_path %}
        {% if bentoml_package in tag %}
        {% set tagname = tag.split(':')[1] %}
        {% print "- [`" %}{% print tagname %}{% print "`](https://github.com/bentoml/BentoML/tree/main/docker/generated/" %}{% print path %}{% print ")\n" %}
        {% endif %}
    {% endfor %}
{% endfor %}
{% endif %}
