import functools
import typing as t

from simple_di import Provide, WrappedCallable
from simple_di import inject as _inject

from ._internal.configuration.containers import BentoMLContainer
from ._internal.models import SAVE_NAMESPACE
from ._internal.runner import Runner
from .exceptions import MissingDependencyException

if t.TYPE_CHECKING:  # pragma: no cover
    # pylint: disable=unused-import
    from _internal.models.store import ModelStore

try:
    ...
except ImportError:  # pragma: no cover
    raise MissingDependencyException("")


inject: t.Callable[[WrappedCallable], WrappedCallable] = functools.partial(
    _inject, squeeze_none=False
)


@inject
def load(
    tag: str,
    model_store: "ModelStore" = Provide[BentoMLContainer.model_store],
):
    """
    Load a model from BentoML local modelstore with given name.

    Args:
        tag (`str`):
            Tag of a saved model in BentoML local modelstore.
        model_store (`~bentoml._internal.models.store.ModelStore`, default to `BentoMLContainer.model_store`):
            BentoML modelstore, provided by DI Container.

    Returns:
        an instance of `xgboost.core.Booster` from BentoML modelstore.

    Examples::
    """  # noqa


@inject
def save(
    name: str,
    model: t.Any,
    *,
    metadata: t.Union[None, t.Dict[str, t.Any]] = None,
    model_store: "ModelStore" = Provide[BentoMLContainer.model_store],
) -> str:
    """
    Save a model instance to BentoML modelstore.

    Args:
        name (`str`):
            Name for given model instance. This should pass Python identifier check.
        model (`xgboost.core.Booster`):
            Instance of model to be saved
        metadata (`t.Optional[t.Dict[str, t.Any]]`, default to `None`):
            Custom metadata for given model.
        model_store (`~bentoml._internal.models.store.ModelStore`, default to `BentoMLContainer.model_store`):
            BentoML modelstore, provided by DI Container.

    Returns:
        tag (`str` with a format `name:version`) where `name` is the defined name user
        set for their models, and version will be generated by BentoML.

    Examples::
    """  # noqa


class _TensorflowRunner(Runner):
    @inject
    def __init__(
        self,
        tag: str,
        resource_quota: t.Dict[str, t.Any],
        batch_options: t.Dict[str, t.Any],
        model_store: "ModelStore" = Provide[BentoMLContainer.model_store],
    ):
        super().__init__(tag, resource_quota, batch_options)

    @property
    def required_models(self) -> t.List[str]:
        ...

    @property
    def num_concurrency_per_replica(self) -> int:
        ...

    @property
    def num_replica(self) -> int:
        ...

    # pylint: disable=arguments-differ,attribute-defined-outside-init
    def _setup(self) -> None:
        ...

    # pylint: disable=arguments-differ,attribute-defined-outside-init
    def _run_batch(self, input_data) -> t.Any:
        ...


@inject
def load_runner(
    tag: str,
    *,
    resource_quota: t.Union[None, t.Dict[str, t.Any]] = None,
    batch_options: t.Union[None, t.Dict[str, t.Any]] = None,
    model_store: "ModelStore" = Provide[BentoMLContainer.model_store],
) -> "_TensorflowRunner":
    """
    Runner represents a unit of serving logic that can be scaled horizontally to
    maximize throughput. `bentoml.tensorflow.load_runner` implements a Runner class that
    wrap around a Tensorflow model, which optimize it for the BentoML runtime.

    Args:
        tag (`str`):
            Model tag to retrieve model from modelstore
        resource_quota (`t.Dict[str, t.Any]`, default to `None`):
            Dictionary to configure resources allocation for runner.
        batch_options (`t.Dict[str, t.Any]`, default to `None`):
            Dictionary to configure batch options for runner in a service context.
        model_store (`~bentoml._internal.models.store.ModelStore`, default to `BentoMLContainer.model_store`):
            BentoML modelstore, provided by DI Container.

    Returns:
        Runner instances for `bentoml.tensorflow` model

    Examples::
    """  # noqa
    return _TensorflowRunner(
        tag=tag,
        resource_quota=resource_quota,
        batch_options=batch_options,
        model_store=model_store,
    )


# import logging
# import os
# import pathlib
# import typing as t
# from distutils.dir_util import copy_tree
#
# import bentoml._internal.constants as _const
#
# from ._internal.models.base import Model
# from ._internal.types import GenericDictType, PathType
# from ._internal.utils import LazyLoader
# from ._internal.utils.tensorflow import (
#     cast_tensor_by_spec,
#     get_arg_names,
#     get_input_signatures,
#     get_restored_functions,
#     pretty_format_restored_model,
# )
#
# _exc = _const.IMPORT_ERROR_MSG.format(
#     fwr="tensorflow",
#     module=__name__,
#     inst="`pip install -U tensorflow`",
# )
#
# if t.TYPE_CHECKING:  # pylint: disable=unused-import # pragma: no cover
#     import tensorflow as tf
#     import tensorflow.python.training.tracking.tracking as tracking
# else:
#     tf = LazyLoader("tf", globals(), "tensorflow", exc_msg=_exc)
#     tracking = LazyLoader(
#         "tf", globals(), "tensorflow.python.training.tracking.tracking", exc_msg=_exc
#     )
#
# TF2 = tf.__version__.startswith("2")
#
# logger = logging.getLogger(__name__)
#
# AUTOTRACKABLE_CALLABLE_WARNING: str = """
# Importing SavedModels from TensorFlow 1.x. `outputs = imported(inputs)`
#  will not be supported by BentoML due to `tensorflow` API.\n
# See https://www.tensorflow.org/api_docs/python/tf/saved_model/load for
#  more details.
# """
#
# TF_FUNCTION_WARNING: str = """
# Due to TensorFlow's internal mechanism, only methods
#  wrapped under `@tf.function` decorator and the Keras default function
#  `__call__(inputs, training=False)` can be restored after a save & load.\n
# You can test the restored model object via `TensorflowModel.load(path)`
# """
#
# KERAS_MODEL_WARNING: str = """
# BentoML detected that {name} is being used to pack a Keras API
#  based model. In order to get optimal serving performance, we recommend
#  to wrap your keras model `call()` methods with `@tf.function` decorator.
# """
#
#
# class _tf_function_wrapper:
#     def __init__(
#         self,
#         origin_func: t.Callable[..., t.Any],
#         arg_names: t.Optional[list] = None,
#         arg_specs: t.Optional[list] = None,
#         kwarg_specs: t.Optional[dict] = None,
#     ) -> None:
#         self.origin_func = origin_func
#         self.arg_names = arg_names
#         self.arg_specs = arg_specs
#         self.kwarg_specs = {k: v for k, v in zip(arg_names or [], arg_specs or [])}
#         self.kwarg_specs.update(kwarg_specs or {})
#
#     def __call__(self, *args, **kwargs):  # type: ignore
#         if self.arg_specs is None and self.kwarg_specs is None:
#             return self.origin_func(*args, **kwargs)
#
#         for k in kwargs:
#             if k not in self.kwarg_specs:
#                 raise TypeError(f"Function got an unexpected keyword argument {k}")
#
#         arg_keys = {k for k, _ in zip(self.arg_names, args)}
#         _ambiguous_keys = arg_keys & set(kwargs)
#         if _ambiguous_keys:
#             raise TypeError(f"got two values for arguments '{_ambiguous_keys}'")
#
#         # INFO:
#         # how signature with kwargs works?
#         # https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/eager/function.py#L1519
#
#         transformed_args = tuple(
#             cast_tensor_by_spec(arg, spec) for arg, spec in zip(args, self.arg_specs)
#         )
#
#         transformed_kwargs = {
#             k: cast_tensor_by_spec(arg, self.kwarg_specs[k])
#             for k, arg in kwargs.items()
#         }
#         return self.origin_func(*transformed_args, **transformed_kwargs)
#
#     def __getattr__(self, k):  # type: ignore
#         return getattr(self.origin_func, k)
#
#     @classmethod
#     def hook_loaded_model(cls, loaded_model) -> None:  # type: ignore # noqa
#         funcs = get_restored_functions(loaded_model)
#         for k, func in funcs.items():
#             arg_names = get_arg_names(func)
#             sigs = get_input_signatures(func)
#             if not sigs:
#                 continue
#             arg_specs, kwarg_specs = sigs[0]
#             setattr(
#                 loaded_model,
#                 k,
#                 cls(
#                     func,
#                     arg_names=arg_names,
#                     arg_specs=arg_specs,
#                     kwarg_specs=kwarg_specs,
#                 ),
#             )
#
#
# _tf_function_wrapper.__doc__ = """
#     TODO:
# """
#
#
# class TensorflowModel(Model):
#     """
#     Artifact class for saving/loading :obj:`tensorflow` model
#     with :obj:`tensorflow.saved_model` format
#
#     Args:
#         model (`Union[tf.keras.Models, tf.Module, PathType, pathlib.PurePath]`):
#             Omit every tensorflow model instance of type :obj:`tf.keras.Models` or
#             :obj:`tf.Module`
#         metadata (`GenericDictType`,  `optional`, default to `None`):
#             Class metadata
#
#
#     Raises:
#         MissingDependencyException:
#             :obj:`tensorflow` is required by TensorflowModel
#
#     Example usage under :code:`train.py`::
#
#         TODO:
#
#     One then can define :code:`bento.py`::
#
#         TODO:
#
#     """
#
#     def __init__(
#         self,
#         model: t.Union["tf.keras.Model", "tf.Module", PathType, pathlib.PurePath],
#         metadata: t.Optional[GenericDictType] = None,
#     ):
#         super(TensorflowModel, self).__init__(model, metadata=metadata)
#
#     @staticmethod
#     def __load_tf_saved_model(  # pylint: disable=unused-private-member
#         path: str,
#     ) -> t.Union["tracking.AutoTrackable", t.Any]:
#         if TF2:
#             return tf.saved_model.load(path)
#         else:
#             loaded = tf.compat.v2.saved_model.load(path)
#             if isinstance(loaded, tracking.AutoTrackable) and not hasattr(
#                 loaded, "__call__"
#             ):
#                 logger.warning(AUTOTRACKABLE_CALLABLE_WARNING)
#             return loaded
#
#     @classmethod
#     def load(cls, path: PathType):  # type: ignore
#         # TODO: type hint returns TF Session or
#         #  Keras model API
#         model = cls.__load_tf_saved_model(str(path))
#         _tf_function_wrapper.hook_loaded_model(model)
#         logger.warning(TF_FUNCTION_WARNING)
#         # pretty format loaded model
#         logger.info(pretty_format_restored_model(model))
#         if hasattr(model, "keras_api"):
#             logger.warning(KERAS_MODEL_WARNING.format(name=cls.__name__))
#         return model
#
#     def save(  # pylint: disable=arguments-differ
#         self,
#         path: PathType,
#         signatures: t.Optional[t.Union[t.Callable[..., t.Any], dict]] = None,
#         options: t.Optional["tf.saved_model.SaveOptions"] = None,
#     ) -> None:  # noqa
#         """
#         Save TensorFlow Trackable object `obj` from [SavedModel format] to path.
#
#         Args:
#             path (`Union[str, bytes, os.PathLike]`):
#                 Path containing a trackable object to export.
#             signatures (`Union[Callable[..., Any], dict]`, `optional`, default to `None`):
#                 `signatures` is one of three types:
#
#                 a `tf.function` with an input signature specified, which will use the default serving signature key
#
#                 a dictionary, which maps signature keys to either :obj`tf.function` instances with input signatures or concrete functions. Keys of such a dictionary may be arbitrary strings, but will typically be from the :obj:`tf.saved_model.signature_constants` module.
#
#                 `f.get_concrete_function` on a `@tf.function` decorated function `f`, in which case f will be used to generate a signature for the SavedModel under the default serving signature key,
#
#                     :code:`tf.function` examples::
#
#                       >>> class Adder(tf.Module):
#                       ...   @tf.function
#                       ...   def add(self, x):
#                       ...     return x + x
#
#                       >>> model = Adder()
#                       >>> tf.saved_model.save(
#                       ...   model, '/tmp/adder',signatures=model.add.get_concrete_function(
#                       ...     tf.TensorSpec([], tf.float32)))
#
#             options (`tf.saved_model.SaveOptions`, `optional`, default to `None`):
#                 :obj:`tf.saved_model.SaveOptions` object that specifies options for saving.
#
#         .. note::
#
#             Refers to `Signatures explanation <https://www.tensorflow.org/api_docs/python/tf/saved_model/save>`_
#             from Tensorflow documentation for more information.
#
#         Raises:
#             ValueError: If `obj` is not trackable.
#         """  # noqa: E501 # pylint: enable=line-too-long
#         if not isinstance(self._model, (str, bytes, pathlib.PurePath, os.PathLike)):
#             if TF2:
#                 tf.saved_model.save(
#                     self._model, str(path), signatures=signatures, options=options
#                 )
#             else:
#                 if options:
#                     logger.warning(
#                         f"Parameter 'options: {str(options)}' is ignored when "
#                         f"using tensorflow {tf.__version__}"
#                     )
#                 tf.saved_model.save(self._model, str(path), signatures=signatures)
#         else:
#             assert os.path.isdir(self._model)
#             copy_tree(str(self._model), str(path))
