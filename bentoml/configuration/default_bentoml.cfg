# Copyright 2019 Atalaya Tech, Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

# http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This is the default configuration for BentoML. When bentoml is imported, it looks
# for a configuration file at "$BENTOML_HOME/bentoml.cfg".


[core]
debug = false
usage_tracking = true
default_repository_base_url = {BENTOML_HOME}/repository/
bentoml_deploy_version = {BENTOML_VERSION}

[deployment]
default_namespace = dev

[db]
url = sqlite:///{BENTOML_HOME}/storage.db

[instrument]
default_namespace = BENTOML
prometheus_multiproc_dir = {BENTOML_HOME}/prometheus_multiproc_dir

[logging]
logging_level = INFO
log_format = [%%(asctime)s] %%(levelname)s - %%(message)s
dev_log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s

# the base file directory where bentoml store all its log files
base_log_dir = {BENTOML_HOME}/logs/

log_request_image_files = True

prediction_log_filename = prediction.log
prediction_log_json_format = "(service_name) (service_version) (api) (request_id) (request) (response) (asctime)"

feedback_log_filename = feedback.log
feedback_log_json_format = "(service_name) (service_version) (request_id) (asctime)"

yatai_web_server_log_filename = yatai_web_server.log


[tracing]
# example: http://127.0.0.1:9411/api/v1/spans
zipkin_api_url =

[yatai_service]
# example for connecting local YataiService grpc server: http://127.0.0.1:50051
url =

[apiserver]
default_port = 5000
enable_metrics = true
enable_feedback = true
default_timeout = 60
default_image_handler_accept_file_extensions = .jpg,.png,.jpeg,.tiff,.webp,.bmp

# Set to a positive integer to take effect, otherwise will fallback to a
# runtime calculated value based on cpu cores
# see `bentoml.server.utils.get_gunicorn_num_of_workers` for details
default_gunicorn_workers_count = -1
batch_request_header = Bentoml-Is-Batch-Request


[marshal_server]
marshal_request_header_flag = BentoML-Is-Merged-Request
default_max_latency = 300
default_max_batch_size = 2000


[cli]
#
#

[yatai]
bento_uri_default_expiration = 3000

[tensorflow]
#
#

[pytorch]
#
#
[aws]
default_region = us-west-2

[google-cloud]
default_region = us-west2

