{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with BentoML\n",
    "\n",
    "BentoML is a python framework for building, shipping and running machine learning services. It provides high-level APIs for defining an ML service and packaging its artifacts, source code, dependencies, and configurations into a production-system-friendly format that is ready for deployment.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=nb&ea=open&el=official-example&dt=bentoml-quick-start-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install BentoML\n",
    "!pip install -I bentoml\n",
    "\n",
    "# Install scikit-learn, we will use a sklean model as an example\n",
    "!pip install pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started with a simple scikit-learn model as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "clf = svm.SVC(gamma='scale')\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ML Service with BentoML\n",
    "\n",
    "To package this model with BentoML, you don't need to change anything in your training code. Following your training workflow, create a new BentoML Service by subclassing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_classifier.py\n",
    "from bentoml import BentoService, api, env, artifacts\n",
    "from bentoml.artifact import PickleArtifact\n",
    "from bentoml.handlers import DataframeHandler\n",
    "\n",
    "# You can also import your own python module here and BentoML will automatically\n",
    "# figure out the dependency chain and package all those python modules\n",
    "\n",
    "@artifacts([PickleArtifact('model')])\n",
    "@env(conda_pip_dependencies=[\"scikit-learn\"])\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(DataframeHandler)\n",
    "    def predict(self, df):\n",
    "        # arbitrary preprocessing or feature fetching code can be placed here \n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@artifacts` decorator here tells BentoML what artifacts are required when \n",
    "packaging this BentoService. Besides `PickleArtifact`, BentoML also provides\n",
    "`TfKerasModelArtifact`, `PytorchModelArtifact`, and `TfSavedModelArtifact` etc.\n",
    "\n",
    "`@env` is designed for specifying the desired system environment in order for this\n",
    "BentoService to load. Other ways you can use this decorator:\n",
    "\n",
    "* If you already have a requirement.txt file listing all python libraries you\n",
    "need:\n",
    "```python\n",
    "@env(requirement_txt='../myproject/requirement.txt')\n",
    "```\n",
    "\n",
    "* Or if you are running this code in a Conda environment that matches the\n",
    "desired production environment:\n",
    "```python\n",
    "@env(with_current_conda_env=True)\n",
    "```\n",
    "\n",
    "Lastly `@api` adds an entry point for accessing this BentoService. Each\n",
    "`api` will be translated into a REST endpoint when [deploying as API\n",
    "server](#serving-via-rest-api), or a CLI command when [running as a CLI\n",
    "tool](#use-as-cli-tool).\n",
    "\n",
    "Each API also requires a `Handler` for defining the expected input format. In\n",
    "this case, `DataframeHandler` will transform either an HTTP request or CLI\n",
    "command arguments into a pandas Dataframe and pass it down to the user defined\n",
    "API function. BentoML also supports `JsonHandler`, `ImageHandler` and\n",
    "`TensorHandler`.\n",
    "\n",
    "\n",
    "## Save BentoML service archive\n",
    "\n",
    "Pack your custom BentoML Service with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) import the custom BentoService defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# 2) `pack` it with required artifacts\n",
    "svc = IrisClassifier.pack(model=clf)\n",
    "\n",
    "# 3) save packed BentoService as archive\n",
    "saved_path = svc.save('/tmp/bentoml_archive')\n",
    "\n",
    "# archive will be saved to:\n",
    "print(saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_That's it._ You've just created your first BentoML servie archive. It's a directory\n",
    "containing all the source code, data and configurations files required to load\n",
    "and run a BentoService. You will also find three 'magic' files generated\n",
    "within the archive directory:\n",
    "\n",
    "* `bentoml.yml` - a YAML file containing all metadata related to this\n",
    "  BentoArchive\n",
    "* `Dockerfile` - for building a Docker Image exposing this BentoService as REST\n",
    "  API endpoint\n",
    "* `setup.py` - the config file that makes a BentoArchive 'pip' installable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purpurse, copy generated model to ./model folder\n",
    "import shutil\n",
    "shutil.rmtree('./model', ignore_errors=True)\n",
    "shutil.copytree(saved_path, './model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving via REST API\n",
    "\n",
    "For exposing your model as a HTTP API endpoint, you can simply use the `bentoml\n",
    "serve` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send prediction request to REST API server\n",
    "\n",
    "*Run the following command in terminal to make a HTTP request to the API server*\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "Note you must ensure the pip and conda dependencies are available in your python\n",
    "environment when using `bentoml serve` command. More commonly we recommend using\n",
    "BentoML API server with Docker:\n",
    "\n",
    "## Run REST API server with Docker\n",
    "\n",
    "You can build a Docker Image for running API server hosting your BentoML archive\n",
    "by using the archive folder as docker build context:\n",
    "\n",
    "__`docker` is note available on Google Colab__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./model && docker build -t iris-classifier ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can `docker push` the image to your choice of registry for deployment,\n",
    "or run it locally for development and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 5000:5000 iris-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BentoService in Python\n",
    "\n",
    "`bentoml.load` is the enssential API for loading a BentoArchive into your\n",
    "python application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "import pandas as pd\n",
    "\n",
    "bento_svc = bentoml.load('./model')\n",
    "\n",
    "# Test loaded bentoml service:\n",
    "bento_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"pip install\" a BentoML archive\n",
    "\n",
    "BentoML also supports distributing a BentoService as PyPI package, with the\n",
    "generated `setup.py` file. A BentoArchive can be installed with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can import your ML service as a regular python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IrisClassifier\n",
    "\n",
    "installed_svc = IrisClassifier.load()\n",
    "installed_svc.predict([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `setup.py` config, a BentoArchive can also be uploaded to pypi.org\n",
    "as a public python package, or to your organization's private PyPI index for all\n",
    "developers in your organization to use:\n",
    "\n",
    "`cd ./model & python setup.py sdist upload`\n",
    "\n",
    "*You will need a \".pypirc\" config file before doing this: https://docs.python.org/2/distutils/packageindex.html*\n",
    "\n",
    "\n",
    "# CLI access\n",
    "\n",
    "`pip install ./model` also installs a CLI tool for accessing the BentoML service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!IrisClassifier info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!IrisClassifier --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!IrisClassifier predict --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!IrisClassifier predict --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML cli also supports reading input data from `csv` or `json` files, in either local machine or remote HTTP/S3 location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing test data to a csv file\n",
    "pd.DataFrame(iris.data).to_csv('iris_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke predict from command lien\n",
    "!IrisClassifier predict --input='./iris_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also use the `bentoml` cli to load and run a BentoML service archive without installing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml info ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml predict ./model --input='[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
